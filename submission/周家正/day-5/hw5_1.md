# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
D * D

#### 1.1.2
B * S * D

#### 1.1.3
B * S * D  

All gather
### 1.2


#### 1.2.1
D * D

#### 1.2.2
B * S * D

#### 1.2.3
B * S * D

All Reduce

## 2 通信分析

### 2.1

#### 2.1.1
不需要

#### 2.1.2
需要，当前层完成反向传递到下一层时，需要做一次All reduce

### 2.2

#### 2.2.1
需要，因为行切分，做完后需要一次All recude，通信量是$2(P-1)*B*S*D$

#### 2.2.2
不需要，只有内存拷贝操作，不需要通信

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
- 如果都使用Row Parallel，那么前向传播时，每个gpu之间多1次all reduce
- 如果都使用Column Parallel，那么在两个线性层之间会多一次all gather，这样会将显存占用扩大，可能会OOM