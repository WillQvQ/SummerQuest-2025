# 语音表示研究

基于论文《Self-supervised Learning with Random-projection Quantizer for Speech
  Recognition》(https://arxiv.org/abs/2202.01855) 的相关研究，按照技术方法进行分类：

## 强化学习相关

### GigaAM: Efficient Self-Supervised Learner for Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2506.01192
- 关键特点: Self-Supervised Learning (SSL) has demonstrated strong performance in speech processing, particularly in automatic speech recognition. In this paper, ...
- 相关技术: Attention, Fine-Tuning

### Tokenizing Electron Cloud in Protein-Ligand Interaction Learning
- ArXiv链接: https://arxiv.org/abs/2505.19014
- 关键特点: The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulati...
- 相关技术: Transformer

### Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration
- ArXiv链接: https://arxiv.org/abs/2505.04457
- 关键特点: Training data cleaning is a new application for generative model-based speech restoration (SR). This paper introduces Miipher-2, an SR model designed ...
- 相关技术: 待分析

### Dolphin: A Large-Scale Automatic Speech Recognition Model for Eastern Languages
- ArXiv链接: https://arxiv.org/abs/2503.20212
- 关键特点: This report introduces Dolphin, a large-scale multilingual automatic speech recognition (ASR) model that extends the Whisper architecture to support a...
- 相关技术: 待分析

### Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation
- ArXiv链接: https://arxiv.org/abs/2503.19611
- 关键特点: Autoregressive (AR) models have demonstrated impressive capabilities in generating high-fidelity music. However, the conventional next-token predictio...
- 相关技术: 待分析

### Keep what you need : extracting efficient subnetworks from large audio representation models
- ArXiv链接: https://arxiv.org/abs/2502.12925
- 关键特点: Recently, research on audio foundation models has witnessed notable advances, as illustrated by the ever improving results on complex downstream tasks...
- 相关技术: 待分析

### BRIDLE: Generalized Self-supervised Learning with Quantization
- ArXiv链接: https://arxiv.org/abs/2502.02118
- 关键特点: Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the...
- 相关技术: 待分析

### Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation
- ArXiv链接: https://arxiv.org/abs/2504.18539
- 关键特点: Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments...
- 相关技术: 待分析

### Towards Early Prediction of Self-Supervised Speech Model Performance
- ArXiv链接: https://arxiv.org/abs/2501.05966
- 关键特点: In Self-Supervised Learning (SSL), pre-training and evaluation are resource intensive. In the speech domain, current indicators of the quality of SSL ...
- 相关技术: Pre-Training

### MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond
- ArXiv链接: https://arxiv.org/abs/2412.11538
- 关键特点: This technical report describes the MERaLiON-SpeechEncoder, a foundation model designed to support a wide range of downstream speech applications. Dev...
- 相关技术: Multimodal

### Identifying Spatio-Temporal Drivers of Extreme Events
- ArXiv链接: https://arxiv.org/abs/2410.24075
- 关键特点: The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine l...
- 相关技术: 待分析

### ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs For Audio, Music, and Speech
- ArXiv链接: https://arxiv.org/abs/2409.15897
- 关键特点: Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have...
- 相关技术: 待分析

### NEST-RQ: Next Token Prediction for Speech Self-Supervised Pre-Training
- ArXiv链接: https://arxiv.org/abs/2409.08680
- 关键特点: Speech self-supervised pre-training can effectively improve the performance of downstream tasks. However, previous self-supervised learning (SSL) meth...
- 相关技术: Pre-Training

### SongCreator: Lyrics-based Universal Song Generation
- ArXiv链接: https://arxiv.org/abs/2409.06029
- 关键特点: Music is an integral part of human culture, embodying human intelligence and creativity, of which songs compose an essential part. While various aspec...
- 相关技术: Attention

### STAB: Speech Tokenizer Assessment Benchmark
- ArXiv链接: https://arxiv.org/abs/2409.02384
- 关键特点: Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use o...
- 相关技术: 待分析

### An Analysis of Linear Complexity Attention Substitutes With Best-RQ
- ArXiv链接: https://arxiv.org/abs/2409.02596
- 关键特点: Self-Supervised Learning (SSL) has proven to be effective in various domains, including speech processing. However, SSL is computationally and memory ...
- 相关技术: Attention

### Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation
- ArXiv链接: https://arxiv.org/abs/2408.16204
- 关键特点: Micro-batch clipping, a gradient clipping method, has recently shown potential in enhancing auto-speech recognition (ASR) model performance. However, ...
- 相关技术: 待分析

### Sharpness-Aware Lookahead for Accelerating Convergence and Improving Generalization
- ArXiv链接: N/A
- 关键特点: Lookahead is a popular stochastic optimizer that can accelerate the training process of deep neural networks. However, the solutions found by Lookahea...
- 相关技术: Neural Network

### ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio
- ArXiv链接: https://arxiv.org/abs/2408.00674
- 关键特点: In the Western music tradition, chords are the main constituent components of harmony, a fundamental dimension of music. Despite its relevance for sev...
- 相关技术: 待分析

### Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2407.04675
- 关键特点: Modern automatic speech recognition (ASR) model is required to accurately transcribe diverse speech signals (from different domains, languages, accent...
- 相关技术: 待分析

### Towards Robust Speech Representation Learning for Thousands of Languages
- ArXiv链接: https://arxiv.org/abs/2407.00837
- 关键特点: Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are stil...
- 相关技术: Pre-Training

### To Distill or Not to Distill? On the Robustness of Robust Knowledge Distillation
- ArXiv链接: https://arxiv.org/abs/2406.04512
- 关键特点: Arabic is known to present unique challenges for Automatic Speech Recognition (ASR). On one hand, its rich linguistic diversity and wide range of dial...
- 相关技术: 待分析

### Anatomy of Industrial Scale Multilingual ASR
- ArXiv链接: https://arxiv.org/abs/2404.09841
- 关键特点: This paper describes AssemblyAI's industrial-scale automatic speech recognition (ASR) system, designed to meet the requirements of large-scale, multil...
- 相关技术: 待分析

### XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception
- ArXiv链接: https://arxiv.org/abs/2403.14402
- 关键特点: Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with...
- 相关技术: Fine-Tuning, Pre-Training

### REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR
- ArXiv链接: https://arxiv.org/abs/2402.03988
- 关键特点: Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription withou...
- 相关技术: Reinforcement Learning

### Retrieval Augmented End-to-End Spoken Dialog Models
- ArXiv链接: https://arxiv.org/abs/2402.01828
- 关键特点: We recently developed a joint speech and language model (SLM [1]) which fuses a pretrained foundational speech model and a large language model (LLM),...
- 相关技术: 待分析

### USM-Lite: Quantization and Sparsity Aware Fine-Tuning for Speech Recognition with Universal Speech Models
- ArXiv链接: https://arxiv.org/abs/2312.08553
- 关键特点: End-to-end automatic speech recognition (ASR) models have seen revolutionary quality gains with the recent development of large-scale universal speech...
- 相关技术: Fine-Tuning

### XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale
- ArXiv链接: https://arxiv.org/abs/2111.09296
- 关键特点: This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B pa...
- 相关技术: 待分析

### MLS: A Large-Scale Multilingual Dataset for Speech Research
- ArXiv链接: https://arxiv.org/abs/2012.03411
- 关键特点: This paper introduces Multilingual LibriSpeech (MLS) dataset, a large multilingual corpus suitable for speech research. The dataset is derived from re...
- 相关技术: 待分析

### FastEmit: Low-Latency Streaming ASR with Sequence-Level Emission Regularization
- ArXiv链接: https://arxiv.org/abs/2010.11148
- 关键特点: Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible. However, emitting fast without...
- 相关技术: Transformer

### Conformer: Convolution-augmented Transformer for Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2005.08100
- 关键特点: Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperformi...
- 相关技术: Transformer, Neural Network

### Lingvo: a Modular and Scalable Framework for Sequence-to-Sequence Modeling
- ArXiv链接: https://arxiv.org/abs/1902.08295
- 关键特点: Lingvo is a Tensorflow framework offering a complete solution for collaborative deep learning research, with a particular focus towards sequence-to-se...
- 相关技术: 待分析

## Transformer架构相关

### DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation
- ArXiv链接: https://arxiv.org/abs/2505.19774
- 关键特点: Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most r...
- 相关技术: Attention

### MKConvLFormer: A streaming speech recognition algorithm for power grid semantic environments
- ArXiv链接: N/A
- 关键特点: The power grid dispatching system is a critical component in the development of the power industry and the technological innovation of smart grids. Ho...
- 相关技术: Transformer, Attention, Neural Network

### Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets
- ArXiv链接: https://arxiv.org/abs/2503.22513
- 关键特点: Self-supervised learning has emerged as a powerful approach for leveraging large-scale unlabeled data to improve model performance in various domains....
- 相关技术: Transformer, Fine-Tuning, Pre-Training

### DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models
- ArXiv链接: https://arxiv.org/abs/2410.24177
- 关键特点: Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and sp...
- 相关技术: Attention

### Recent Advances in Speech Language Models: A Survey
- ArXiv链接: https://arxiv.org/abs/2410.03751
- 关键特点: Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natur...
- 相关技术: Attention

### Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper
- ArXiv链接: https://arxiv.org/abs/2409.13499
- 关键特点: The training of automatic speech recognition (ASR) with little to no supervised data remains an open question. In this work, we demonstrate that strea...
- 相关技术: Transformer, Fine-Tuning, Pre-Training

### An End-to-End Approach for Chord-Conditioned Song Generation
- ArXiv链接: https://arxiv.org/abs/2409.06307
- 关键特点: The Song Generation task aims to synthesize music composed of vocals and accompaniment from given lyrics. While the existing method, Jukebox, has expl...
- 相关技术: Attention

### Revisiting Convolution-free Transformer for Speech Recognition
- ArXiv链接: N/A
- 关键特点: Convolution augmented Transformer architectures have dominated the field of automatic speech recognition by showing better WER results when the models...
- 相关技术: Transformer, Pre-Training

### NEST: Self-supervised Fast Conformer as All-purpose Seasoning to Speech Processing Tasks
- ArXiv链接: https://arxiv.org/abs/2408.13106
- 关键特点: Self-supervised learning has been proved to benefit a wide range of speech processing tasks, such as speech recognition/translation, speaker verificat...
- 相关技术: Transformer

### Linear-Complexity Self-Supervised Learning for Speech Processing
- ArXiv链接: https://arxiv.org/abs/2407.13377
- 关键特点: Self-supervised learning (SSL) models usually require weeks of pre-training with dozens of high-end GPUs. These models typically have a multi-headed s...
- 相关技术: Attention, Pre-Training

### XLSR-Transducer: Streaming ASR for Self-Supervised Pretrained Models
- ArXiv链接: https://arxiv.org/abs/2407.04439
- 关键特点: Self-supervised pretrained models exhibit competitive performance in automatic speech recognition on finetuning, even with limited in-domain supervise...
- 相关技术: Transformer, Attention

### Pseudo-Label Based Supervised Contrastive Loss for Robust Speech Representations
- ArXiv链接: N/A
- 关键特点: The self supervised learning (SSL) of speech, with discrete tokenization (pseudo-labels), while illustrating performance improvements in low-resource ...
- 相关技术: Transformer

### BEiT: BERT Pre-Training of Image Transformers
- ArXiv链接: https://arxiv.org/abs/2106.08254
- 关键特点: We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Follow...
- 相关技术: Transformer, Pre-Training

### 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding
- ArXiv链接: N/A
- 关键特点: 暂无摘要信息
- 相关技术: Transformer, Pre-Training

### Attention is All you Need
- ArXiv链接: https://arxiv.org/abs/1706.03762
- 关键特点: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The bes...
- 相关技术: Transformer, Attention, Neural Network

### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
- ArXiv链接: https://arxiv.org/abs/1810.04805
- 关键特点: We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent ...
- 相关技术: Transformer, Pre-Training

## 推理优化相关

### Make Some Noise: Towards LLM audio reasoning and generation using sound tokens
- ArXiv链接: https://arxiv.org/abs/2503.22275
- 关键特点: Integrating audio comprehension and generation into large language models (LLMs) remains challenging due to the continuous nature of audio and the res...
- 相关技术: Reasoning, Multimodal

### Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens
- ArXiv链接: https://arxiv.org/abs/2410.03298
- 关键特点: Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules...
- 相关技术: 待分析

### Zero-shot Cross-lingual Voice Transfer for TTS
- ArXiv链接: https://arxiv.org/abs/2409.13910
- 关键特点: In this paper, we introduce a zero-shot Voice Transfer (VT) module that can be seamlessly integrated into a multi-lingual Text-to-speech (TTS) system ...
- 相关技术: 待分析

### TransVIP: Speech to Speech Translation System with Voice and Isochrony Preservation
- ArXiv链接: https://arxiv.org/abs/2405.17809
- 关键特点: There is a rising interest and trend in research towards directly translating speech from one language to another, known as end-to-end speech-to-speec...
- 相关技术: 待分析

### Bigger is not Always Better: The Effect of Context Size on Speech Pre-Training
- ArXiv链接: https://arxiv.org/abs/2312.01515
- 关键特点: It has been generally assumed in the automatic speech recognition (ASR) literature that it is better for models to have access to wider context window...
- 相关技术: Pre-Training

## 多模态相关

### MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2502.10447
- 关键特点: Audio-visual speech recognition (AVSR) has become critical for enhancing speech recognition in noisy environments by integrating both auditory and vis...
- 相关技术: 待分析

### A Survey of Recent Advances and Challenges in Deep Audio-Visual Correlation Learning
- ArXiv链接: https://arxiv.org/abs/2412.00049
- 关键特点: Audio-visual correlation learning aims to capture and understand natural phenomena between audio and visual data. The rapid growth of Deep Learning pr...
- 相关技术: 待分析

### Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation
- ArXiv链接: https://arxiv.org/abs/2410.20336
- 关键特点: Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. Howeve...
- 相关技术: Multimodal, Fine-Tuning

### VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing
- ArXiv链接: https://arxiv.org/abs/2408.05758
- 关键特点: Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice ...
- 相关技术: Multimodal, Fine-Tuning, Pre-Training

### SpeechVerse: A Large-scale Generalizable Audio Language Model
- ArXiv链接: https://arxiv.org/abs/2405.08295
- 关键特点: Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instruction...
- 相关技术: Multimodal

### SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models
- ArXiv链接: https://arxiv.org/abs/2405.08317
- 关键特点: Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lat...
- 相关技术: Multimodal

### Self-supervised Pre-training of Text Recognizers
- ArXiv链接: https://arxiv.org/abs/2405.00420
- 关键特点: In this paper, we investigate self-supervised pre-training methods for document text recognition. Nowadays, large unlabeled datasets can be collected ...
- 相关技术: Pre-Training

### Conformer is All You Need for Visual Speech Recognition
- ArXiv链接: N/A
- 关键特点: Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temp...
- 相关技术: 待分析

### Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data
- ArXiv链接: https://arxiv.org/abs/2402.18932
- 关键特点: Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper prop...
- 相关技术: 待分析

### Investigating Zero-Shot Generalizability on Mandarin-English Code-Switched ASR And Speech-to-Text Translation of Recent Foundation Models with Self-Supervision and Weak Supervision
- ArXiv链接: https://arxiv.org/abs/2401.00273
- 关键特点: This work evaluated several cutting-edge large-scale foundation models based on self-supervision or weak supervision, including SeamlessM4T, SeamlessM...
- 相关技术: Pre-Training

### Audio-visual fine-tuning of audio-only ASR models
- ArXiv链接: https://arxiv.org/abs/2312.09369
- 关键特点: Audio-visual automatic speech recognition (AV-ASR) models are very effective at reducing word error rates on noisy speech, but require large amounts o...
- 相关技术: Fine-Tuning, Pre-Training

### Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach
- ArXiv链接: https://arxiv.org/abs/2312.03406
- 关键特点: Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. V...
- 相关技术: 待分析

### Joint Unsupervised and Supervised Training for Multilingual ASR
- ArXiv链接: https://arxiv.org/abs/2111.08137
- 关键特点: Self-supervised training has shown promising gains in pretraining models and facilitating the downstream finetuning for speech recognition, like multi...
- 相关技术: 待分析

### Masked Autoencoders Are Scalable Vision Learners
- ArXiv链接: https://arxiv.org/abs/2111.06377
- 关键特点: This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random ...
- 相关技术: 待分析

### Libri-Light: A Benchmark for ASR with Limited or No Supervision
- ArXiv链接: https://arxiv.org/abs/1912.07875
- 关键特点: We introduce a new collection of spoken English audio suitable for training speech recognition systems under limited or no supervision. It is derived ...
- 相关技术: 待分析

### Effectiveness of self-supervised pre-training for speech recognition
- ArXiv链接: https://arxiv.org/abs/1911.03912
- 关键特点: We compare self-supervised representation learning algorithms which either explicitly quantize the audio data or learn representations without quantiz...
- 相关技术: Fine-Tuning, Pre-Training

## 知识图谱相关

### Bridging the Gap between Continuous and Informative Discrete Representations by Random Product Quantization
- ArXiv链接: https://arxiv.org/abs/2504.04721
- 关键特点: Self-supervised learning has become a core technique in speech processing, but the high dimensionality of its representations makes discretization ess...
- 相关技术: 待分析

### MuCodec: Ultra Low-Bitrate Music Codec
- ArXiv链接: https://arxiv.org/abs/2409.13216
- 关键特点: Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and gen...
- 相关技术: 待分析

### Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech
- ArXiv链接: https://arxiv.org/abs/2402.16321
- 关键特点: Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current model...
- 相关技术: 待分析

### LUPET: Incorporating Hierarchical Information Path into Multilingual ASR
- ArXiv链接: https://arxiv.org/abs/2401.03689
- 关键特点: Toward high-performance multilingual automatic speech recognition (ASR), various types of linguistic information and model design have demonstrated th...
- 相关技术: 待分析

### Towards Matching Phones and Speech Representations
- ArXiv链接: https://arxiv.org/abs/2310.17558
- 关键特点: Learning phone types from phone instances has been a long-standing problem, while still being open. In this work, we revisit this problem in the conte...
- 相关技术: 待分析

### REPRESENTATION
- ArXiv链接: N/A
- 关键特点: Chapter 6 presents the motif of representation: the idea that the eucharistic bread and wine are symbols (or signs, figures, antitypes, etc.) of the b...
- 相关技术: 待分析

## 其他方法

### Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis
- ArXiv链接: https://arxiv.org/abs/2505.21138
- 关键特点: Large-scale training corpora have significantly improved the performance of ASR models. Unfortunately, due to the relative scarcity of data, Chinese a...
- 相关技术: Pre-Training

### Layer-wise Investigation of Large-Scale Self-Supervised Music Representation Models
- ArXiv链接: https://arxiv.org/abs/2505.16306
- 关键特点: Recently, pre-trained models for music information retrieval based on self-supervised learning (SSL) are becoming popular, showing success in various ...
- 相关技术: 待分析

### LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors
- ArXiv链接: https://arxiv.org/abs/2505.11352
- 关键特点: Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a ra...
- 相关技术: Fine-Tuning

### UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation
- ArXiv链接: https://arxiv.org/abs/2503.00733
- 关键特点: Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applica...
- 相关技术: Pre-Training

### Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models
- ArXiv链接: https://arxiv.org/abs/2502.05837
- 关键特点: Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impa...
- 相关技术: 待分析

### An overview of high-resource automatic speech recognition methods and their empirical evaluation in low-resource environments
- ArXiv链接: N/A
- 关键特点: 暂无摘要信息
- 相关技术: 待分析

### Optimized Self-supervised Training with BEST-RQ for Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2501.16131
- 关键特点: Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Tr...
- 相关技术: Fine-Tuning, Pre-Training

### DQ-Data2vec: Decoupling Quantization for Multilingual Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2501.13497
- 关键特点: Data2vec is a self-supervised learning (SSL) approach that employs a teacher-student architecture for contextual representation learning via masked pr...
- 相关技术: 待分析

### Zero-resource Speech Translation and Recognition with LLMs
- ArXiv链接: https://arxiv.org/abs/2412.18566
- 关键特点: Despite recent advancements in speech processing, zero-resource speech translation (ST) and automatic speech recognition (ASR) remain challenging prob...
- 相关技术: 待分析

### Employing Deep Learning to Create Speech Recognition Systems for Accented English
- ArXiv链接: N/A
- 关键特点: This research proposes a novel approach for developing an accented English speech recognition system using a deep learning model optimized by the Afri...
- 相关技术: 待分析

### Formal Verification of an Enhanced Deep Learning Model: Unveiling Computational Effectiveness in Speech Recognition
- ArXiv链接: N/A
- 关键特点: —Automatic speech recognition (ASR) assumes a crucial function in various domains, improving search engines, aiding healthcare with medical reporting ...
- 相关技术: Neural Network

### Restructuring Vector Quantization with the Rotation Trick
- ArXiv链接: https://arxiv.org/abs/2410.06424
- 关键特点: Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with min...
- 相关技术: 待分析

### Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech
- ArXiv链接: https://arxiv.org/abs/2410.01162
- 关键特点: This work studies the capabilities of a large language model (LLM) to understand paralinguistic aspects of speech without fine-tuning its weights. We ...
- 相关技术: Fine-Tuning

### MT2KD: Towards A General-Purpose Encoder for Speech, Speaker, and Audio Events
- ArXiv链接: https://arxiv.org/abs/2409.17010
- 关键特点: With the advances in deep learning, the performance of end-to-end (E2E) single-task models for speech and audio processing has been constantly improvi...
- 相关技术: Fine-Tuning

### Textless NLP -- Zero Resource Challenge with Low Resource Compute
- ArXiv链接: https://arxiv.org/abs/2409.19015
- 关键特点: This work addresses the persistent challenges of substantial training time and GPU resource requirements even when training lightweight encoder-vocode...
- 相关技术: 待分析

### Training Large ASR Encoders With Differential Privacy
- ArXiv链接: https://arxiv.org/abs/2409.13953
- 关键特点: Self-supervised learning (SSL) methods for large speech models have proven to be highly effective at ASR. With the interest in public deployment of la...
- 相关技术: Fine-Tuning, Pre-Training

### M-BEST-RQ: A Multi-Channel Speech Foundation Model for Smart Glasses
- ArXiv链接: https://arxiv.org/abs/2409.11494
- 关键特点: The growing popularity of multi-channel wearable devices, such as smart glasses, has led to a surge of applications such as targeted speech recognitio...
- 相关技术: 待分析

### Speaker-IPL: Unsupervised Learning of Speaker Characteristics with i-Vector based Pseudo-Labels
- ArXiv链接: https://arxiv.org/abs/2409.10791
- 关键特点: Iterative self-training, or iterative pseudo-labeling (IPL) -- using an improved model from the current iteration to provide pseudo-labels for the nex...
- 相关技术: 待分析

### Exploring Prediction Targets in Masked Pre-Training for Speech Foundation Models
- ArXiv链接: https://arxiv.org/abs/2409.10788
- 关键特点: Speech foundation models, such as HuBERT and its variants, are pre-trained on large amounts of unlabeled speech data and then used for a range of down...
- 相关技术: Pre-Training

### Comparative Analysis of Pretrained Audio Representations in Music Recommender Systems
- ArXiv链接: https://arxiv.org/abs/2409.08987
- 关键特点: Over the years, Music Information Retrieval (MIR) has proposed various models pretrained on large amounts of music data. Transfer learning showcases t...
- 相关技术: Neural Network

### Speaker Change Detection with Weighted-sum Knowledge Distillation based on Self-supervised Pre-trained Models
- ArXiv链接: N/A
- 关键特点: Speaker Change Detection (SCD) is an essential problem in speech processing and has various applications in many fields. The self-supervised models ha...
- 相关技术: Fine-Tuning, Pre-Training

### SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks
- ArXiv链接: https://arxiv.org/abs/2408.13040
- 关键特点: Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to a...
- 相关技术: Fine-Tuning

### Parameter-Efficient Transfer Learning under Federated Learning for Automatic Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2408.11873
- 关键特点: This work explores the challenge of enhancing Automatic Speech Recognition (ASR) model performance across various user-specific domains while preservi...
- 相关技术: 待分析

### Leave No Knowledge Behind During Knowledge Distillation: Towards Practical and Effective Knowledge Distillation For Code-Switching ASR Using Realistic Data
- ArXiv链接: https://arxiv.org/abs/2407.10603
- 关键特点: Recent advances in automatic speech recognition (ASR) often rely on large speech foundation models for generating high-quality transcriptions. However...
- 相关技术: 待分析

### MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model
- ArXiv链接: https://arxiv.org/abs/2406.09869
- 关键特点: Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast conv...
- 相关技术: 待分析

### ASTRA: Aligning Speech and Text Representations for Asr without Sampling
- ArXiv链接: https://arxiv.org/abs/2406.06664
- 关键特点: This paper introduces ASTRA, a novel method for improving Automatic Speech Recognition (ASR) through text injection.Unlike prevailing techniques, ASTR...
- 相关技术: 待分析

### USM RNN-T model weights binarization
- ArXiv链接: https://arxiv.org/abs/2406.02887
- 关键特点: Large-scale universal speech models (USM) are already used in production. However, as the model size grows, the serving cost grows too. Serving cost o...
- 相关技术: Neural Network

### Open Implementation and Study of Best-RQ for Speech Processing
- ArXiv链接: https://arxiv.org/abs/2405.04296
- 关键特点: Self-Supervised Learning (SSL) has proven to be useful in various speech tasks. However, these methods are generally very demanding in terms of data, ...
- 相关技术: Pre-Training

### A Comparison of Parameter-Efficient ASR Domain Adaptation Methods for Universal Speech and Language Models
- ArXiv链接: N/A
- 关键特点: A recent paradigm shift in artificial intelligence has seen the rise of foundation models, such as the large language models and the universal speech ...
- 相关技术: 待分析

### Task Vector Algebra for ASR Models
- ArXiv链接: N/A
- 关键特点: Vector representations of text and speech signals such as word2vec and wav2vec are used commonly in automatic speech recognition (ASR) and spoken lang...
- 相关技术: 待分析

### Noise Masking Attacks and Defenses for Pretrained Speech Models
- ArXiv链接: https://arxiv.org/abs/2404.02052
- 关键特点: Speech models are often trained on sensitive data in order to improve model performance, leading to potential privacy leakage. Our work considers nois...
- 相关技术: 待分析

### Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
- ArXiv链接: https://arxiv.org/abs/2403.19709
- 关键特点: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task par...
- 相关技术: Fine-Tuning

### HeAR - Health Acoustic Representations
- ArXiv链接: https://arxiv.org/abs/2403.02522
- 关键特点: Health acoustic sounds such as coughs and breaths are known to contain useful health signals with significant potential for monitoring health and dise...
- 相关技术: 待分析

### Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?
- ArXiv链接: https://arxiv.org/abs/2402.00340
- 关键特点: Self-supervised features are typically used in place of filter-bank features in speaker verification models. However, these models were originally des...
- 相关技术: 待分析

### Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective
- ArXiv链接: https://arxiv.org/abs/2401.08833
- 关键特点: Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for...
- 相关技术: 待分析

### Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization
- ArXiv链接: https://arxiv.org/abs/2401.06980
- 关键特点: In this paper, we present a novel bilevel optimization-based training approach to training acoustic models for automatic speech recognition (ASR) task...
- 相关技术: Fine-Tuning, Pre-Training

### Revisiting the Entropy Semiring for Neural Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2312.10087
- 关键特点: In streaming settings, speech recognition models have to map sub-sequences of speech to text before the full audio stream becomes available. However, ...
- 相关技术: 待分析

### Leveraging Cross Lingual Speech Representations To Build ASR For Under-resourced Languages
- ArXiv链接: N/A
- 关键特点: Automatic speech recognition (ASR) technology can help document and preserve under-resourced tribal languages by converting spoken words into text. Bu...
- 相关技术: Pre-Training

### A Foundation Model for Music Informatics
- ArXiv链接: https://arxiv.org/abs/2311.03318
- 关键特点: This paper investigates foundation models tailored for music informatics, a domain currently challenged by the scarcity of labeled data and generaliza...
- 相关技术: 待分析

### BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2109.13226
- 关键特点: We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled dataset...
- 相关技术: Fine-Tuning, Pre-Training

### w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training
- ArXiv链接: https://arxiv.org/abs/2108.06209
- 关键特点: Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM f...
- 相关技术: Pre-Training

### HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units
- ArXiv链接: https://arxiv.org/abs/2106.07447
- 关键特点: Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each inpu...
- 相关技术: Fine-Tuning, Pre-Training

### Scaling End-to-End Models for Large-Scale Multilingual ASR
- ArXiv链接: https://arxiv.org/abs/2104.14830
- 关键特点: Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing w...
- 相关技术: 待分析

### Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2010.10504
- 关键特点: We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on Libr...
- 相关技术: Pre-Training

### Unsupervised Cross-lingual Representation Learning for Speech Recognition
- ArXiv链接: https://arxiv.org/abs/2006.13979
- 关键特点: This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple l...
- 相关技术: 待分析

### wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations
- ArXiv链接: https://arxiv.org/abs/2006.11477
- 关键特点: We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform...
- 相关技术: Fine-Tuning, Pre-Training

### A Streaming On-Device End-To-End Model Surpassing Server-Side Conventional Model Quality and Latency
- ArXiv链接: https://arxiv.org/abs/2003.12710
- 关键特点: Thus far, end-to-end (E2E) models have not been shown to outperform state-of-the-art conventional models with respect to both quality, i.e., word erro...
- 相关技术: Neural Network

### vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations
- ArXiv链接: https://arxiv.org/abs/1910.05453
- 关键特点: We propose vq-wav2vec to learn discrete representations of audio segments through a wav2vec-style self-supervised context prediction task. The algorit...
- 相关技术: Pre-Training

### Speech Recognition
- ArXiv链接: N/A
- 关键特点: 暂无摘要信息
- 相关技术: 待分析

### Feature Learning with Raw-Waveform CLDNNs for Voice Activity Detection
- ArXiv链接: N/A
- 关键特点: Voice Activity Detection (VAD) is an important preprocessing step in any state-of-the-art speech recognition system. Choosing the right set of feature...
- 相关技术: Neural Network

### Adam: A Method for Stochastic Optimization
- ArXiv链接: https://arxiv.org/abs/1412.6980
- 关键特点: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-or...
- 相关技术: 待分析

### Sequence Transduction with Recurrent Neural Networks
- ArXiv链接: https://arxiv.org/abs/1211.3711
- 关键特点: Many machine learning tasks can be expressed as the transformation---or \emph{transduction}---of input sequences into output sequences: speech recogni...
- 相关技术: Neural Network

### Japanese and Korean voice search
- ArXiv链接: N/A
- 关键特点: 暂无摘要信息
- 相关技术: 待分析

### Understanding the difficulty of training deep feedforward neural networks
- ArXiv链接: N/A
- 关键特点: 暂无摘要信息
- 相关技术: Neural Network

---

**统计总结:**
- 强化学习相关: 32篇论文
- Transformer架构相关: 16篇论文
- 推理优化相关: 5篇论文
- 多模态相关: 16篇论文
- 知识图谱相关: 6篇论文
- 其他方法: 54篇论文
- 引用该论文的文章总数: 0
- 该论文引用的文章总数: 0

**主要趋势:**
1. 强化学习相关在该领域占重要地位，共有32篇相关论文
1. 其他方法在该领域占重要地位，共有54篇相关论文
2. 该研究方向活跃度较高，共收集到129篇相关论文
3. 建议重点关注引用数量较多的核心论文进行深入研究
