# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
[d, d]即[1024, 1024]

#### 1.1.2
[B, S, d]即[8, 128, 1024]

#### 1.1.3
[B, S, d]即[8, 128, 1024]
不需要得到完整的Y,这样可以直接传递给Linear2

### 1.2


#### 1.2.1
[d, d]即[1024, 1024]

#### 1.2.2
[B, S, d]即[8, 128, 1024]

#### 1.2.3
[B, s, d]即[8, 128, 1024]
通过AllReduce操作进行求和,最后结构为[B, S, d]即[8, 128, 1024]

## 2 通信分析

### 2.1

#### 2.1.1
不需要通信

#### 2.1.2
需要AllReduce通信
反向传播时求梯度需要对每个rank上的X的梯度求和

### 2.2

#### 2.2.1
需要AllReduce通信,对每个分块的矩阵进行求和

#### 2.2.2
需要把 $$\frac{\partial L}{\partial Y}$$  拷贝到四块 GPU 上，就能各自独立做梯度计算 $$\frac{\partial Y_i}{\partial W_i}$$ 这里只涉及到内存的复制，而不涉及到设备之间的通信。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
- **如果两层都使用 Row Parallel:**
  1.  **Linear1 (Row)**: 前向传播需要一次Scatter按照最后一个维度进行四等分。Linear1完之后还需要一次 AllReduce 来得到中间结果 Y，而不是直接传递给Linear2
  2.  **额外通信**: 一次Scatter和一次 AllReduce
- **如果两层都使用 Column Parallel:**
  1.  **Linear2 (Column)**: 从 Linear1 接收到的是分片的 `Y_i`。但是由于使用了列分割,所以导致无法进行矩阵乘法,需要先整合Linear传来的矩阵。
  2.  **额外通信**:一次AllGather 