# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
每个 rank 上的 $W_1^{(i)} \in \mathbb{R}^{d \times (4d/P)} = \mathbb{R}^{1024 \times 4096/4} = \mathbb{R}^{1024 \times 1024}$

#### 1.1.2
$X \in \mathbb{R}^{B \times S \times d} = \mathbb{R}^{8 \times 128 \times 1024}$

#### 1.1.3
本地输出：$Y_i \in \mathbb{R}^{B \times S \times (4d/P)} = \mathbb{R}^{8 \times 128 \times 1024}$
最终完整输出 $Y$ 由所有 $Y_i$ 沿最后一维 concatenate 得到
$\Rightarrow Y \in \mathbb{R}^{8 \times 128 \times 4096}$

### 1.2


#### 1.2.1
$W_2^{(i)} \in \mathbb{R}^{(4d/P) \times d} = \mathbb{R}^{1024 \times 1024}$

#### 1.2.2
$Y_i \in \mathbb{R}^{8 \times 128 \times 1024}$

#### 1.2.3
本地输出：$Z_i \in \mathbb{R}^{8 \times 128 \times d} = \mathbb{R}^{8 \times 128 \times 1024}$
最终输出 $Z$ 由所有 $Z_i$ reduce sum得到
$\Rightarrow Z = \sum_{i=1}^P Z_i \in \mathbb{R}^{8 \times 128 \times 1024}$

## 2 通信分析

### 2.1

#### 2.1.1
不需要通信

#### 2.1.2
需要通信。因为每个 rank 计算出的 $\partial L / \partial X_i$ 是不完整的
需要 all reduce sum 汇聚所有 rank 的梯度
### 2.2

#### 2.2.1
需要通信。因为 Linear2 的输出 $Z_i$ 要 reduce sum 得到完整输出 $Z$
通信操作：all-reduce(sum)
通信量：每个 rank 发送/接收 $\mathbb{R}^{8 \times 128 \times 1024} = 1M$ 元素，总通信量为 $P \times B \times S \times d$

#### 2.2.2
不需要通信。因为 Row Parallel 中每个 rank 本地拥有的 $W_2^{(i)}$ 与 $Z_i$ 可独立计算对应的 $\partial L / \partial Y_i$
各部分拼接成完整 $\partial L / \partial Y$

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
- 会产生额外通信：
Linear1 的输出 $Y$ 在 Row Parallel 中是按列切分的，需要 all-gather 合并后才能输入下一层导致在层间引入额外的 all-gather 通信
- 会出现输入shape不匹配问题：
Linear1 的输出是按列切分的 $Y_i \in \mathbb{R}^{8 \times 128 \times 1024}$Column Parallel 的 Linear2 要求完整的输入 $Y$，而不是片段。因此需要 all-gather 恢复完整 $Y$，否则 Linear2 无法计算。导致效率低，通信开销增大。