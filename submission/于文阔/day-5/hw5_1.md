# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
$[d, d]$ = $[1024, 1024]$

#### 1.1.2
$[B, S, d]$ = $[8, 128, 1024]$

#### 1.1.3
$[B, S, d]$ = $[8, 128, 1024]$ , 对 $Y_i$ 做 $AllGather$.

### 1.2


#### 1.2.1
$[d, d]$ = $[1024, 1024]$

#### 1.2.2
$[B, S, d]$ = $[8, 128, 1024]$

#### 1.2.3
$[B, S, d]$ = $[8, 128, 1024]$ , 对 $Z_i$ 做 $AllReduce$.

## 2 通信分析

### 2.1

#### 2.1.1
不需要

#### 2.1.2
需要，因为要把每个 GPU 中的 $\frac{\partial L}{\partial X}|i$ 相加。

### 2.2

#### 2.2.1
需要，$AllReduce$, $2 \times 3 \times B \times S \times d = 6291456$

#### 2.2.2
不需要，只需要让 GPU 各自独立做梯度计算就可以了。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
### 都使用 Row Parallel :
Forward: 会增加一次 ALLReduce

Backward: 会增加两次 AllGather 并减少一次 AllReduce。

### 都使用 Column Parallel :

Forward: 会增加二次 AllGather，并减少一次ALLReduce

Backward: 会增加一次 ALLReduce

同时第一层输出的完整矩阵需要存储在所有设备上，增加了显存占用