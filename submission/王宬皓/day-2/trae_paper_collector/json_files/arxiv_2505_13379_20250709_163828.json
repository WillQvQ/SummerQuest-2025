{
  "arxiv_id": "2505.13379",
  "arxiv_url": "https://arxiv.org/abs/2505.13379",
  "timestamp": "2025-07-09T16:38:28.852860",
  "papers": {
    "main_paper": {
      "title": "Thinkless: LLM Learns When to Think",
      "abstract": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
      "url": "http://arxiv.org/abs/2505.13379v2",
      "authors": [
        "Gongfan Fang",
        "Xinyin Ma",
        "Xinchao Wang"
      ],
      "published": "2025-05-19T17:24:16Z",
      "arxiv_id": "2505.13379v2"
    },
    "citing_papers": [
      {
        "paperId": "4e49bc6dd4a215aaea3a7c4d7fdd157adbccb49d",
        "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task",
        "abstract": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately predict the table names and column names required for the SQL query based on the given question. However, current fine-tuning approaches for schema linking models employ a rote-learning paradigm, excessively optimizing for ground truth schema linking outcomes while compromising reasoning ability. This limitation arises because of the difficulty in acquiring a high-quality reasoning sample for downstream tasks. To address this, we propose Schema-R1, a reasoning schema linking model trained using reinforcement learning. Specifically, Schema-R1 consists of three key steps: constructing small batches of high-quality reasoning samples, supervised fine-tuning for cold-start initialization, and rule-based reinforcement learning training. The final results demonstrate that our method effectively enhances the reasoning ability of the schema linking model, achieving a 10\\% improvement in filter accuracy compared to the existing method. Our code is available at https://github.com/hongWin/Schema-R1/.",
        "externalIds": {
          "ArXiv": "2506.11986",
          "CorpusId": 279391488
        },
        "ArXiv": "2506.11986"
      },
      {
        "paperId": "c64a10ec08a1ae26e043ac5b5f0d91d7d617369b",
        "title": "Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models",
        "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like\"Wait\"or\"Let me rethink\"can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to\"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance-creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from\"more thinking\"are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models.",
        "externalIds": {
          "ArXiv": "2506.04210",
          "DBLP": "journals/corr/abs-2506-04210",
          "DOI": "10.48550/arXiv.2506.04210",
          "CorpusId": 279155405
        },
        "ArXiv": "2506.04210"
      },
      {
        "paperId": "6b3ce0d47482e3080c19af5c5ccfe9915e5a8f8e",
        "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
        "abstract": "Large Reasoning Models (LRMs) excel at complex tasks using Chain-of-Thought (CoT) reasoning. However, their tendency to overthinking leads to unnecessarily lengthy reasoning chains, dramatically increasing inference costs. To mitigate this issue, we introduce VeriThinker, a novel approach for CoT compression. Unlike conventional methods that fine-tune LRMs directly on the original reasoning task using synthetic concise CoT data, we innovatively fine-tune the model solely through an auxiliary verification task. By training LRMs to accurately verify the correctness of CoT solutions, the LRMs inherently become more discerning about the necessity of subsequent self-reflection steps, thereby effectively suppressing overthinking. Extensive experiments validate that VeriThinker substantially reduces reasoning chain lengths while maintaining or even slightly improving accuracy. When applied to DeepSeek-R1-Distill-Qwen-7B, our approach reduces reasoning tokens on MATH500 from 3790 to 2125 while improving accuracy by 0.8% (94.0% to 94.8%), and on AIME25, tokens decrease from 14321 to 10287 with a 2.1% accuracy gain (38.7% to 40.8%). Additionally, our experiments demonstrate that VeriThinker can also be zero-shot generalized to speculative reasoning. Code is available at https://github.com/czg1225/VeriThinker",
        "externalIds": {
          "ArXiv": "2505.17941",
          "DBLP": "journals/corr/abs-2505-17941",
          "DOI": "10.48550/arXiv.2505.17941",
          "CorpusId": 278886376
        },
        "ArXiv": "2505.17941"
      }
    ],
    "referenced_papers": [
      {
        "paperId": "a98d89f8398e3d63721e7cb8ee96325f89833d23",
        "title": "Efficient Reasoning Models: A Survey",
        "abstract": "Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this\"slow-thinking\"paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.",
        "externalIds": {
          "ArXiv": "2504.10903",
          "DBLP": "journals/corr/abs-2504-10903",
          "DOI": "10.48550/arXiv.2504.10903",
          "CorpusId": 277786677
        },
        "ArXiv": "2504.10903"
      },
      {
        "paperId": "de23d38bc2604dcf334dcc46aff217eb6bcd1fe1",
        "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
        "abstract": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.",
        "externalIds": {
          "ArXiv": "2503.20783",
          "DBLP": "journals/corr/abs-2503-20783",
          "DOI": "10.48550/arXiv.2503.20783",
          "CorpusId": 277322777
        },
        "ArXiv": "2503.20783"
      },
      {
        "paperId": "fb970ce4383a78ad52c641bc38815d78ad737aad",
        "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process.",
        "externalIds": {
          "ArXiv": "2503.19470",
          "DBLP": "journals/corr/abs-2503-19470",
          "DOI": "10.48550/arXiv.2503.19470",
          "CorpusId": 277313597
        },
        "ArXiv": "2503.19470"
      },
      {
        "paperId": "16db56b5a57f2e675e5c4f60ca0fbd5764915a5e",
        "title": "SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild",
        "abstract": "DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the\"aha moment\"). Notably, we observe the\"aha moment\"for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.",
        "externalIds": {
          "ArXiv": "2503.18892",
          "DBLP": "journals/corr/abs-2503-18892",
          "DOI": "10.48550/arXiv.2503.18892",
          "CorpusId": 277940848
        },
        "ArXiv": "2503.18892"
      },
      {
        "paperId": "90f2a86c37db66fcfbd2d2ec8df2ec136caf7df6",
        "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
        "abstract": "Recent advances in large language models (LLMs) have enabled strong reasoning capabilities through Chain-of-Thought (CoT) prompting, which elicits step-by-step problem solving, but often at the cost of excessive verbosity in intermediate outputs, leading to increased computational overhead. We propose Sketch-of-Thought (SoT), a prompting framework that integrates cognitively inspired reasoning paradigms with linguistic constraints to reduce token usage while preserving reasoning accuracy. SoT is designed as a flexible, modular approach and is instantiated with three paradigms--Conceptual Chaining, Chunked Symbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and selected dynamically at test-time by a lightweight routing model. Across 15 reasoning datasets spanning multiple domains, languages, and modalities, SoT achieves token reductions of up to 78% with minimal accuracy loss. In tasks such as mathematical and multi-hop reasoning, it even improves accuracy while shortening outputs.",
        "externalIds": {
          "ArXiv": "2503.05179",
          "DBLP": "journals/corr/abs-2503-05179",
          "DOI": "10.48550/arXiv.2503.05179",
          "CorpusId": 276885298
        },
        "ArXiv": "2503.05179"
      },
      {
        "paperId": "0681d80cd58b6135534cf279a69f5f999f47ebca",
        "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
        "abstract": "Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1",
        "externalIds": {
          "ArXiv": "2503.04697",
          "DBLP": "journals/corr/abs-2503-04697",
          "DOI": "10.48550/arXiv.2503.04697",
          "CorpusId": 276813519
        },
        "ArXiv": "2503.04697"
      },
      {
        "paperId": "56a9aa8825b6c6441b0d22b9bc04852e1af80852",
        "title": "Chain of Draft: Thinking Faster by Writing Less",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT) prompting, which emphasizes verbose, step-by-step reasoning. However, humans typically employ a more efficient strategy: drafting concise intermediate thoughts that capture only essential information. In this work, we propose Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes, where LLMs generate minimalistic yet informative intermediate reasoning outputs while solving tasks. By reducing verbosity and focusing on critical insights, CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of the tokens, significantly reducing cost and latency across various reasoning tasks. Our code and data are available at https://github.com/sileix/chain-of-draft.",
        "externalIds": {
          "ArXiv": "2502.18600",
          "DBLP": "journals/corr/abs-2502-18600",
          "DOI": "10.48550/arXiv.2502.18600",
          "CorpusId": 276618268
        },
        "ArXiv": "2502.18600"
      },
      {
        "paperId": "b39e8ff1babf6b5276c0af68ab766c46487e1df7",
        "title": "Reasoning with Latent Thoughts: On the Power of Looped Transformers",
        "abstract": "Large language models have shown remarkable reasoning abilities and scaling laws suggest that large parameter count, especially along the depth axis, is the primary driver. In this work, we make a stronger claim -- many reasoning problems require a large depth but not necessarily many parameters. This unlocks a novel application of looped models for reasoning. Firstly, we show that for many synthetic reasoning problems like addition, $p$-hop induction, and math problems, a $k$-layer transformer looped $L$ times nearly matches the performance of a $kL$-layer non-looped model, and is significantly better than a $k$-layer model. This is further corroborated by theoretical results showing that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved effectively using looped models with nearly optimal depth. Perhaps surprisingly, these benefits also translate to practical settings of language modeling -- on many downstream reasoning tasks, a language model with $k$-layers looped $L$ times can be competitive to, if not better than, a $kL$-layer language model. In fact, our empirical analysis reveals an intriguing phenomenon: looped and non-looped models exhibit scaling behavior that depends on their effective depth, akin to the inference-time scaling of chain-of-thought (CoT) reasoning. We further elucidate the connection to CoT reasoning by proving that looped models implicitly generate latent thoughts and can simulate $T$ steps of CoT with $T$ loops. Inspired by these findings, we also present an interesting dichotomy between reasoning and memorization, and design a looping-based regularization that is effective on both fronts.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2502-17416",
          "ArXiv": "2502.17416",
          "DOI": "10.48550/arXiv.2502.17416",
          "CorpusId": 276575815
        },
        "ArXiv": "2502.17416"
      },
      {
        "paperId": "88e00d651413264261a1c3100710d491502930a7",
        "title": "Towards Reasoning Ability of Small Language Models",
        "abstract": "Reasoning has long been viewed as an emergent property of large language models (LLMs), appearing at or above a certain scale ($\\sim$100B parameters). However, recent studies challenge this assumption, showing that small language models (SLMs) can also achieve competitive reasoning performance. SLMs are increasingly favored for their efficiency and deployability. However, there is a lack of systematic study on the reasoning abilities of diverse SLMs, including those trained from scratch or derived from LLMs through quantization, pruning, and distillation. This raises a critical question: Can SLMs achieve reasoning abilities comparable to LLMs? In this work, we systematically survey, benchmark, and analyze 72 SLMs from six model families across 14 reasoning benchmarks. For reliable evaluation, we examine four evaluation methods and compare four LLM judges against human evaluations on 800 data points. We repeat all experiments three times to ensure a robust performance assessment. Additionally, we analyze the impact of different prompting strategies in small models. Beyond accuracy, we also evaluate model robustness under adversarial conditions and intermediate reasoning steps. Our findings challenge the assumption that scaling is the only way to achieve strong reasoning. Instead, we foresee a future where SLMs with strong reasoning capabilities can be developed through structured training or post-training compression. They can serve as efficient alternatives to LLMs for reasoning-intensive tasks.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2502-11569",
          "ArXiv": "2502.11569",
          "DOI": "10.48550/arXiv.2502.11569",
          "CorpusId": 276409399
        },
        "ArXiv": "2502.11569"
      },
      {
        "paperId": "181b4a18a17105c6c270ec533b96b519c9e28c42",
        "title": "Small Models Struggle to Learn from Strong Reasoners",
        "abstract": "Large language models (LLMs) excel in complex reasoning tasks, and distilling their reasoning capabilities into smaller models has shown promise. However, we uncover an interesting phenomenon, which we term the Small Model Learnability Gap: small models ($\\leq$3B parameters) do not consistently benefit from long chain-of-thought (CoT) reasoning or distillation from larger models. Instead, they perform better when fine-tuned on shorter, simpler reasoning chains that better align with their intrinsic learning capacity. To address this, we propose Mix Distillation, a simple yet effective strategy that balances reasoning complexity by combining long and short CoT examples or reasoning from both larger and smaller models. Our experiments demonstrate that Mix Distillation significantly improves small model reasoning performance compared to training on either data alone. These findings highlight the limitations of direct strong model distillation and underscore the importance of adapting reasoning complexity for effective reasoning capability transfer.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2502-12143",
          "ArXiv": "2502.12143",
          "DOI": "10.48550/arXiv.2502.12143",
          "CorpusId": 276422318
        },
        "ArXiv": "2502.12143"
      },
      {
        "paperId": "2039018e618ccc34744addf94c2ab8ad141d7f24",
        "title": "CoT-Valve: Length-Compressible Chain-of-Thought Tuning",
        "abstract": "Chain-of-Thought significantly enhances a model's reasoning capability, but it also comes with a considerable increase in inference costs due to long chains. With the observation that the reasoning path can be easily compressed under easy tasks but struggle on hard tasks, we explore the feasibility of elastically controlling the length of reasoning paths with only one model, thereby reducing the inference overhead of reasoning models dynamically based on task difficulty. We introduce a new tuning and inference strategy named CoT-Valve, designed to allow models to generate reasoning chains of varying lengths. To achieve this, we propose to identify a direction in the parameter space that, when manipulated, can effectively control the length of generated CoT. Moreover, we show that this property is valuable for compressing the reasoning chain. We construct datasets with chains from long to short for the same questions and explore two enhanced strategies for CoT-Valve: (1) a precise length-compressible CoT tuning method, and (2) a progressive chain length compression approach. Our experiments show that CoT-Valve successfully enables controllability and compressibility of the chain and shows better performance than the prompt-based control. We applied this method to QwQ-32B-Preview, reducing reasoning chains on GSM8K from 741 to 225 tokens with a minor performance drop (95.07% to 94.92%) and on AIME from 6827 to 4629 tokens, with only one additional incorrect answer.",
        "externalIds": {
          "ArXiv": "2502.09601",
          "DBLP": "journals/corr/abs-2502-09601",
          "DOI": "10.48550/arXiv.2502.09601",
          "CorpusId": 276317564
        },
        "ArXiv": "2502.09601"
      },
      {
        "paperId": "ce31151064003530fd21ad5395bb48eff50dd63e",
        "title": "The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks",
        "abstract": "Large Reasoning Models (LRMs) represent a breakthrough in AI problem-solving capabilities, but their effectiveness in interactive environments can be limited. This paper introduces and analyzes overthinking in LRMs. A phenomenon where models favor extended internal reasoning chains over environmental interaction. Through experiments on software engineering tasks using SWE Bench Verified, we observe three recurring patterns: Analysis Paralysis, Rogue Actions, and Premature Disengagement. We propose a framework to study these behaviors, which correlates with human expert assessments, and analyze 4018 trajectories. We observe that higher overthinking scores correlate with decreased performance, with reasoning models exhibiting stronger tendencies toward overthinking compared to non-reasoning models. Our analysis reveals that simple efforts to mitigate overthinking in agentic environments, such as selecting the solution with the lower overthinking score, can improve model performance by almost 30% while reducing computational costs by 43%. These results suggest that mitigating overthinking has strong practical implications. We suggest that by leveraging native function-calling capabilities and selective reinforcement learning overthinking tendencies could be mitigated. We also open-source our evaluation framework and dataset to facilitate research in this direction at https://github.com/AlexCuadron/Overthinking.",
        "externalIds": {
          "ArXiv": "2502.08235",
          "DBLP": "journals/corr/abs-2502-08235",
          "DOI": "10.48550/arXiv.2502.08235",
          "CorpusId": 276287600
        },
        "ArXiv": "2502.08235"
      },
      {
        "paperId": "34a98d89b38d10aad635a8bf713058c8d0c06c3d",
        "title": "Agentic Reasoning: Reasoning LLMs with Tools for the Deep Research",
        "abstract": "We introduce Agentic Reasoning, a framework that enhances large language model (LLM) reasoning by integrating external tool-using agents. Unlike conventional LLM-based reasoning approaches, which rely solely on internal inference, Agentic Reasoning dynamically engages web search, code execution, and structured reasoning-context memory to solve complex problems requiring deep research and multi-step logical deduction. Our framework introduces the Mind Map agent, which constructs a structured knowledge graph to track logical relationships, improving deductive reasoning. Additionally, the integration of web-search and coding agents enables real-time retrieval and computational analysis, enhancing reasoning accuracy and decision-making. Evaluations on PhD-level scientific reasoning (GPQA) and domain-specific deep research tasks demonstrate that our approach significantly outperforms existing models, including leading retrieval-augmented generation (RAG) systems and closed-source LLMs. Moreover, our results indicate that agentic reasoning improves expert-level knowledge synthesis, test-time scalability, and structured problem-solving. The code is at: https://github.com/theworldofagents/Agentic-Reasoning.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2502-04644",
          "ArXiv": "2502.04644",
          "DOI": "10.48550/arXiv.2502.04644",
          "CorpusId": 276235557
        },
        "ArXiv": "2502.04644"
      },
      {
        "paperId": "c784ce3b0c90a11f87d1164d16d4ddb041aa9d7f",
        "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
        "abstract": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. The code is available at https://github.com/BaohaoLiao/RSD.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2501-19324",
          "ArXiv": "2501.19324",
          "DOI": "10.48550/arXiv.2501.19324",
          "CorpusId": 276079559
        },
        "ArXiv": "2501.19324"
      },
      {
        "paperId": "34471a2fa18ea22efad5287cf4aeb18542c98a9b",
        "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
        "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2501-12948",
          "ArXiv": "2501.12948",
          "DOI": "10.48550/arXiv.2501.12948",
          "CorpusId": 275789950
        },
        "ArXiv": "2501.12948"
      },
      {
        "paperId": "668075792a7ab40457d92e09da28d35c879271c3",
        "title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs",
        "abstract": "Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).",
        "externalIds": {
          "ArXiv": "2501.12599",
          "DBLP": "journals/corr/abs-2501-12599",
          "DOI": "10.48550/arXiv.2501.12599",
          "CorpusId": 275789974
        },
        "ArXiv": "2501.12599"
      },
      {
        "paperId": "09cedd18546c334891df025aa3a4e21658affa23",
        "title": "O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning",
        "abstract": "Recently, long-thought reasoning LLMs, such as OpenAI's O1, adopt extended reasoning processes similar to how humans ponder over complex problems. This reasoning paradigm significantly enhances the model's problem-solving abilities and has achieved promising results. However, long-thought reasoning process leads to a substantial increase in inference time. A pressing challenge is reducing the inference overhead of long-thought LLMs while ensuring accuracy. In this paper, we experimentally demonstrate that long-thought reasoning models struggle to effectively allocate token budgets based on problem difficulty and reasoning redundancies. To address this, we propose Length-Harmonizing Fine-Tuning (O1-Pruner), aiming at minimizing reasoning overhead while maintaining accuracy. This effective fine-tuning method first estimates the LLM's baseline performance through pre-sampling and then uses RL-style fine-tuning to encourage the model to generate shorter reasoning processes under accuracy constraints. This allows the model to achieve efficient reasoning with lower redundancy while maintaining accuracy. Experiments on various mathematical reasoning benchmarks show that O1-Pruner not only significantly reduces inference overhead but also achieves higher accuracy, providing a novel and promising solution to this challenge. Our code is coming soon at https://github.com/StarDewXXX/O1-Pruner",
        "externalIds": {
          "ArXiv": "2501.12570",
          "DBLP": "journals/corr/abs-2501-12570",
          "DOI": "10.48550/arXiv.2501.12570",
          "CorpusId": 275790112
        },
        "ArXiv": "2501.12570"
      },
      {
        "paperId": "84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c",
        "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
        "abstract": "The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
        "externalIds": {
          "ArXiv": "2412.21187",
          "DBLP": "journals/corr/abs-2412-21187",
          "DOI": "10.48550/arXiv.2412.21187",
          "CorpusId": 275133600
        },
        "ArXiv": "2412.21187"
      },
      {
        "paperId": "3bbf541c2a3ce0ffaed3703cd486f0e1b2febb27",
        "title": "Token-Budget-Aware LLM Reasoning",
        "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE",
        "externalIds": {
          "DBLP": "journals/corr/abs-2412-18547",
          "ArXiv": "2412.18547",
          "DOI": "10.48550/arXiv.2412.18547",
          "CorpusId": 274992044
        },
        "ArXiv": "2412.18547"
      },
      {
        "paperId": "88aa6b1f37d1fd8e0a40499ce9bb87873f03aaa8",
        "title": "Qwen2.5 Technical Report",
        "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
        "externalIds": {
          "ArXiv": "2412.15115",
          "DBLP": "journals/corr/abs-2412-15115",
          "DOI": "10.48550/arXiv.2412.15115",
          "CorpusId": 274859421
        },
        "ArXiv": "2412.15115"
      },
      {
        "paperId": "2cd94b619034ae0226f4391e3ff628fe9ce780a1",
        "title": "C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness",
        "abstract": "Generating Chain-of-Thought (CoT) before deriving the answer can effectively improve the reasoning capabilities of large language models (LLMs) and significantly improve the accuracy of the generated answer. However, in most cases, the length of the generated CoT is much longer than the desired final answer, which results in additional decoding costs. Furthermore, existing research has discovered that shortening the reasoning steps in CoT, even while preserving the key information, diminishes LLMs' abilities. These phenomena make it difficult to use LLMs and CoT in many real-world applications that only require the final answer and are sensitive to latency, such as search and recommendation. To reduce the costs of model decoding and shorten the length of the generated CoT, this paper presents Conditioned Compressed Chain-of-Thought (C3oT), a CoT compression framework that involves a compressor to compress an original longer CoT into a shorter CoT while maintaining key information and interpretability, a conditioned training method to train LLMs with both longer CoT and shorter CoT simultaneously to learn the corresponding relationships between them, and a conditioned inference method to gain the reasoning ability learned from longer CoT by generating shorter CoT. We conduct experiments over four datasets from arithmetic and commonsense scenarios, showing that the proposed method is capable of compressing the length of generated CoT by up to more than 50% without compromising its effectiveness.",
        "externalIds": {
          "DBLP": "conf/aaai/KangSCZ25",
          "ArXiv": "2412.11664",
          "DOI": "10.48550/arXiv.2412.11664",
          "CorpusId": 274776959
        },
        "ArXiv": "2412.11664"
      },
      {
        "paperId": "07e61eaafcbc78061fb87c2b893db9fc26c30eb9",
        "title": "Improving Mathematical Reasoning Capabilities of Small Language Models via Feedback-Driven Distillation",
        "abstract": "Large Language Models (LLMs) demonstrate exceptional reasoning capabilities, often achieving state-of-the-art performance in various tasks. However, their substantial computational and memory demands, due to billions of parameters, hinder deployment in resource-constrained environments. A promising solution is knowledge distillation, where LLMs transfer reasoning capabilities to Small Language Models (SLMs, $\\le$ 1B parameters), enabling wider deployment on low-resource devices. Existing methods primarily focus on generating high-quality reasoning rationales for distillation datasets but often neglect the critical role of data quantity and quality. To address these challenges, we propose a Feedback-Driven Distillation (FDD) framework to enhance SLMs' mathematical reasoning capabilities. In the initialization stage, a distillation dataset is constructed by prompting LLMs to pair mathematical problems with corresponding reasoning rationales. We classify problems into easy and hard categories based on SLM performance. For easy problems, LLMs generate more complex variations, while for hard problems, new questions of similar complexity are synthesized. In addition, we propose a multi-round distillation paradigm to iteratively enrich the distillation datasets, thereby progressively improving the mathematical reasoning abilities of SLMs. Experimental results demonstrate that our method can make SLMs achieve SOTA mathematical reasoning performance.",
        "externalIds": {
          "ArXiv": "2411.14698",
          "DBLP": "journals/corr/abs-2411-14698",
          "DOI": "10.48550/arXiv.2411.14698",
          "CorpusId": 274192661
        },
        "ArXiv": "2411.14698"
      },
      {
        "paperId": "f2d0f3d47ae850f49a58f4977393bd0025af4bec",
        "title": "HybridFlow: A Flexible and Efficient RLHF Framework",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) is widely used in Large Language Model (LLM) alignment. Traditional RL can be modeled as a dataflow, where each node represents computation of a neural network (NN) and each edge denotes data dependencies between the NNs. RLHF complicates the dataflow by expanding each node into a distributed LLM training or generation program, and each edge into a many-to-many multicast. Traditional RL frameworks execute the dataflow using a single controller to instruct both intra-node computation and inter-node communication, which can be inefficient in RLHF due to large control dispatch overhead for distributed intra-node computation. Existing RLHF systems adopt a multi-controller paradigm, which can be inflexible due to nesting distributed computation and data communication. We propose HybridFlow, which combines single-controller and multi-controller paradigms in a hybrid manner to enable flexible representation and efficient execution of the RLHF data flow. We carefully design a set of hierarchical APIs that decouple and encapsulate computation and data dependencies in the complex RLHF dataflow, allowing efficient operation orchestration to implement RLHF algorithms and flexible mapping of the computation onto various devices. We further design a 3D-HybridEngine for efficient actor model resharding between training and generation phases, with zero memory redundancy and significantly reduced communication overhead. Our experimental results demonstrate 1.53x~20.57× throughput improvement when running various RLHF algorithms using HybridFlow, as compared with state-of-the-art baselines. HybridFlow source code is available at https://github.com/volcengine/verl",
        "externalIds": {
          "DBLP": "conf/eurosys/ShengZYWZZPL025",
          "ArXiv": "2409.19256",
          "DOI": "10.1145/3689031.3696075",
          "CorpusId": 272987758
        },
        "ArXiv": "2409.19256"
      },
      {
        "paperId": "f15a1a87c345a0b0b2b989064ede053d1411bb03",
        "title": "Distilling System 2 into System 1",
        "abstract": "Large language models (LLMs) can spend extra compute during inference to generate intermediate thoughts, which helps to produce better final responses. Since Chain-of-Thought (Wei et al., 2022), many such System 2 techniques have been proposed such as Rephrase and Respond (Deng et al., 2023a), System 2 Attention (Weston and Sukhbaatar, 2023) and Branch-Solve-Merge (Saha et al., 2023). In this work we investigate self-supervised methods to ``compile'' (distill) higher quality outputs from System 2 techniques back into LLM generations without intermediate reasoning token sequences, as this reasoning has been distilled into System 1. We show that several such techniques can be successfully distilled, resulting in improved results compared to the original System 1 performance, and with less inference cost than System 2. We posit that such System 2 distillation will be an important feature of future continually learning AI systems, enabling them to focus System 2 capabilities on the reasoning tasks that they cannot yet do well.",
        "externalIds": {
          "ArXiv": "2407.06023",
          "DBLP": "journals/corr/abs-2407-06023",
          "DOI": "10.48550/arXiv.2407.06023",
          "CorpusId": 271050364
        },
        "ArXiv": "2407.06023"
      },
      {
        "paperId": "9b3239cff17327960804098e33e1ca903e7b9e85",
        "title": "RouteLLM: Learning to Route LLMs with Preference Data",
        "abstract": "Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs.",
        "externalIds": {
          "ArXiv": "2406.18665",
          "DBLP": "journals/corr/abs-2406-18665",
          "DOI": "10.48550/arXiv.2406.18665",
          "CorpusId": 270764307
        },
        "ArXiv": "2406.18665"
      },
      {
        "paperId": "d688af24de21449ef5fe5288746a51569942eb32",
        "title": "Distilling Reasoning Ability from Large Language Models with Adaptive Thinking",
        "abstract": "Chain of thought finetuning (cot-finetuning) aims to endow small language models (SLM) with reasoning ability to improve their performance towards specific tasks by allowing them to imitate the reasoning procedure of large language models (LLM) beyond simply predicting the answers. Most existing cot-finetuning methods adopt a pre-thinking mechanism, allowing the SLM to generate a rationale before providing an answer. This mechanism enables SLM to analyze and think about complex questions, but it also makes answer correctness highly sensitive to minor errors in rationale. Therefore, we propose a robust post-thinking mechanism to generate answers before rationale. Thanks to this answer-first setting, 1) the answer can escape from the adverse effects caused by minor errors in the rationale; 2) the rationale serves as an error amplifier to the answer, which makes the SLM focus on learning hard samples; 3) the inferring efficiency can also benefit from the setting since users can stop the generation right after answers are outputted when inference is conducted. However, although the post-thinking mechanism brings many advantages and improves the overall performance of SLM on specific tasks, it may lose the ability to think about the questions and decompose complex questions into simple sub-questions compared to pre-thinking mechanism. Therefore, a plug-and-play adaptive-thinking mechanism is proposed with the aid of the soft prompt tuning to integrate the merits of the pre-thinking mechanism and post-thinking mechanism, in which a perception module is introduced to adaptively prompt SLM answer or think first based on perceiving the complexity of the questions. Extensive experiments are conducted across 12 reasoning tasks and 2 representative language models to demonstrate the effectiveness of the proposed mechanism.",
        "externalIds": {
          "ArXiv": "2404.09170",
          "CorpusId": 269149491
        },
        "ArXiv": "2404.09170"
      },
      {
        "paperId": "35b142ea69598e6241f0011312128031df55895c",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "abstract": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
        "externalIds": {
          "ArXiv": "2402.03300",
          "DBLP": "journals/corr/abs-2402-03300",
          "DOI": "10.48550/arXiv.2402.03300",
          "CorpusId": 267412607
        },
        "ArXiv": "2402.03300"
      },
      {
        "paperId": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
        "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
        "abstract": "While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generality of reasoning tasks, the model generated by it exceeds the comprehensive performance of two individually distilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved remarkable improvements of (84.5%) and (85.5%), respectively, outperforming GPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2312-10730",
          "ArXiv": "2312.10730",
          "DOI": "10.48550/arXiv.2312.10730",
          "CorpusId": 266359672
        },
        "ArXiv": "2312.10730"
      },
      {
        "paperId": "be8db99310602d66bba64bcf41a572c45816fbfc",
        "title": "Let's Verify Step by Step",
        "abstract": "In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.",
        "externalIds": {
          "ArXiv": "2305.20050",
          "DBLP": "conf/iclr/LightmanKBEBLLS24",
          "DOI": "10.48550/arXiv.2305.20050",
          "CorpusId": 258987659
        },
        "ArXiv": "2305.20050"
      },
      {
        "paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
        "title": "Teaching Small Language Models to Reason",
        "abstract": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2212-08410",
          "ACL": "2023.acl-short.151",
          "ArXiv": "2212.08410",
          "DOI": "10.48550/arXiv.2212.08410",
          "CorpusId": 254823156
        },
        "ArXiv": "2212.08410"
      },
      {
        "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
        "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
        "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
        "externalIds": {
          "ArXiv": "2201.11903",
          "DBLP": "journals/corr/abs-2201-11903",
          "CorpusId": 246411621
        },
        "ArXiv": "2201.11903"
      },
      {
        "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
        "title": "Training Verifiers to Solve Math Word Problems",
        "abstract": "State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",
        "externalIds": {
          "ArXiv": "2110.14168",
          "DBLP": "journals/corr/abs-2110-14168",
          "CorpusId": 239998651
        },
        "ArXiv": "2110.14168"
      },
      {
        "paperId": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
        "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
        "abstract": "Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",
        "externalIds": {
          "DBLP": "conf/nips/HendrycksBKABTS21",
          "ArXiv": "2103.03874",
          "CorpusId": 232134851
        },
        "ArXiv": "2103.03874"
      },
      {
        "paperId": "8323c591e119eb09b28b29fd6c7bc76bd889df7a",
        "title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
        "abstract": "Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",
        "externalIds": {
          "MAG": "2973727699",
          "ArXiv": "1909.08053",
          "DBLP": "journals/corr/abs-1909-08053",
          "CorpusId": 202660670
        },
        "ArXiv": "1909.08053"
      },
      {
        "paperId": "0c908739fbff75f03469d13d4a1a07de3414ee19",
        "title": "Distilling the Knowledge in a Neural Network",
        "abstract": "A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.",
        "externalIds": {
          "ArXiv": "1503.02531",
          "MAG": "1821462560",
          "DBLP": "journals/corr/HintonVD15",
          "CorpusId": 7200347
        },
        "ArXiv": "1503.02531"
      },
      {
        "paperId": "bf82b02460c2cc442617a0920c4d0b07d468ed1f",
        "title": "DNA Bench: When Silence is Smarter - Benchmarking Over-Reasoning in Reasoning LLMs",
        "abstract": "Test-time scaling has significantly improved large language model (LLM) performance, enabling deeper reasoning to solve complex problems. However, this increased reasoning capability also leads to excessive token generation and unnecessary problem-solving attempts. We introduce “Don’t Answer Bench (DNA Bench)”, a new benchmark designed to evaluate LLMs’ ability to robustly understand the tricky reasoning triggers and avoiding unnecessary generation. DNA Bench consists of 150 adversarially designed prompts that are easy for humans to understand and respond to, but surprisingly not for many of the recent prominent LLMs. DNA Bench tests models’ abilities across different capabilities, such as instruction adherence, hallucination avoidance, redundancy filtering, and unanswerable question recognition. We evaluate reasoning LLMs (RLMs), including DeepSeek-R1, OpenAI O3-mini, Claude-3.7-sonnet and compare them against a powerful non-reasoning model, e.g., GPT-4o. Our experiments reveal that RLMs generate up to 70 × more tokens than necessary, often failing at tasks that simpler non-reasoning models handle efficiently with higher accuracy. Our findings underscore the need for more effective training and inference strategies in RLMs.",
        "externalIds": {
          "DBLP": "journals/corr/abs-2503-15793",
          "DOI": "10.48550/arXiv.2503.15793",
          "CorpusId": 277785746
        },
        "ArXiv": null
      },
      {
        "paperId": "985298bd0b6f4c84ab34bb15cfed0b092f43b83a",
        "title": "OpenAI o1 System Card",
        "abstract": null,
        "externalIds": {
          "CorpusId": 272648256
        },
        "ArXiv": null
      }
    ]
  },
  "summary": {
    "total_citing": 3,
    "total_referenced": 37
  }
}