{
  "arxiv_id": "2210.03094",
  "arxiv_url": "https://arxiv.org/abs/2210.03094",
  "timestamp": "2025-07-09T15:41:36.788077",
  "papers": {
    "main_paper": {
      "title": "VIMA: General Robot Manipulation with Multimodal Prompts",
      "abstract": "Prompt-based learning has emerged as a successful paradigm in natural\nlanguage processing, where a single general-purpose language model can be\ninstructed to perform any task specified by input prompts. Yet task\nspecification in robotics comes in various forms, such as imitating one-shot\ndemonstrations, following language instructions, and reaching visual goals.\nThey are often considered different tasks and tackled by specialized models. We\nshow that a wide spectrum of robot manipulation tasks can be expressed with\nmultimodal prompts, interleaving textual and visual tokens. Accordingly, we\ndevelop a new simulation benchmark that consists of thousands of\nprocedurally-generated tabletop tasks with multimodal prompts, 600K+ expert\ntrajectories for imitation learning, and a four-level evaluation protocol for\nsystematic generalization. We design a transformer-based robot agent, VIMA,\nthat processes these prompts and outputs motor actions autoregressively. VIMA\nfeatures a recipe that achieves strong model scalability and data efficiency.\nIt outperforms alternative designs in the hardest zero-shot generalization\nsetting by up to $2.9\\times$ task success rate given the same training data.\nWith $10\\times$ less training data, VIMA still performs $2.7\\times$ better than\nthe best competing variant. Code and video demos are available at\nhttps://vimalabs.github.io/",
      "url": "http://arxiv.org/abs/2210.03094v2",
      "authors": [
        "Yunfan Jiang",
        "Agrim Gupta",
        "Zichen Zhang",
        "Guanzhi Wang",
        "Yongqiang Dou",
        "Yanjun Chen",
        "Li Fei-Fei",
        "Anima Anandkumar",
        "Yuke Zhu",
        "Linxi Fan"
      ],
      "published": "2022-10-06T17:50:11Z",
      "arxiv_id": "2210.03094v2"
    },
    "citing_papers": [],
    "referenced_papers": []
  },
  "summary": {
    "total_citing": 0,
    "total_referenced": 0
  }
}