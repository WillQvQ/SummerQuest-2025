# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1

- 权重矩阵被按列切分为P份，1024 x 1024

#### 1.1.2

- 输入张量不需要切分，直接为8 x 128 x 1024

#### 1.1.3

- 每个rank按列进行切分，得到8 x 128 x 1024

- 完整的Y通过拼接不同的Yi，得到8 x 128 x 4096

### 1.2


#### 1.2.1

- 权重矩阵被按行切分为P份，1024 x 1024

#### 1.2.2

- 每个rank在最后一个纬度上进行切分，得到8 x 128 x 1024

#### 1.2.3
- 8 x 128 x 1024

- 完整的Z：将所有 rank 上的本地输出的Zi求和，8 x 128 x 1024


## 2 通信分析

### 2.1

#### 2.1.1
- 需要通信。通信操作是 All-gather。对于Linear1，所有rank需要将本地计算的Yi拼接。

- 通信量：每个rank需要接受其他（P-1）个矩阵，3 x 8 x 128 x 1024 = 3145728 个浮点数

#### 2.1.2
- 需要通信。反向传播中需要计算损失 L 对输入 X 的梯度，需要对每个 rank 局部梯度进行 All-reduce 求和。

### 2.2

#### 2.2.1
- 需要通信。通信操作是 All-reduce。对于Linear2，为了得到完整的Z，需要All-reduce求和

- 通信量：每个rank需要接受其他（P-1）个矩阵，3 x 8 x 128 x 1024 = 3145728 个浮点数

#### 2.2.2
- 不需要通信。由于Z在求和后是完整的，W2i在本地就有，所以不需要通信

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
1. 两层如果都使用 Row Parallel
    - 额外通信发生在 Linear1 和 Linear2 之间的连接处，需要All-reduce -> Scatter先聚合成一个完整的Y再分发

2. 两层如果都使用 Column Parallel
    - linear2的输入是需要是完整的,因此需要额外的操作，要先把linear1的输出进行 all-gather再输入到 linear2
