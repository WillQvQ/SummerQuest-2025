# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
$R^{1024 * 1024}$

#### 1.1.2
$R^{8 * 128 * 1024}$

#### 1.1.3
$Y_i \in R^{8 * 128 * 1024}$
应该将 $Y_0, Y_1, Y_2, Y_3$ 在最后一个维度上拼接。

### 1.2


#### 1.2.1
$R^{1024 * 1024}$

#### 1.2.2
$R^{8 * 128 * 1024}$

#### 1.2.3
$Z_i \in R^{8 * 128 * 1024}$
直接 AllReduce 即可。

## 2 通信分析

### 2.1

#### 2.1.1
需要通信。
AllGather。
假设一个参数 4 字节，每个节点需要向所有其他节点发送自己的所有数据，并从其他节点接收等大小的数据，所以通行量为 $2 * 3 * 8 * 128 * 1024 * 4 \text{BYTE} = 24\text{MB}$

#### 2.1.2
不需要。
因为由于 column paralle 的输出也被切分了，所以局部节点 $X_i$ 可以直接用局部 $L$ 对 $Y_i$ 的导数和 $W_i$ 求得自己的梯度。

### 2.2


#### 2.2.1
需要通信。
AllReduce。
通行量上和 AllGather 相同，每个节点的通信行为类似，所以为 $24\text{MB}$


#### 2.2.2
不需要通信。
与 2.1.2 类似，但是这里局部节点使用全量的 $L$ 对 $Y$ 的偏导和 $W_i$ 来计算自己的导数。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
如果都用 Row Parallel，会直接导致 Linear 1 的 output 和 Linear 2 不对齐，所以 Linear 1 的 output 必须 AllGather 后再切块才能送入 Linear 2。

如果都使用 Column Parallel，由于 Linear 2 需要完整的输入，而 Linear 1 的输出被 Column Parallel 切开了，所以理论上无法运行。