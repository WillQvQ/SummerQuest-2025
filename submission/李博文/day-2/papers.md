# Visual Instruction Tuning 相关研究分类

基于论文《Visual Instruction Tuning》(https://arxiv.org/abs/2304.08485) 的相关研究，按照研究领域和技术路线进行分类：

## 主论文信息

- **标题**: Visual Instruction Tuning
- **作者**: Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee
- **ArXiv ID**: 2304.08485v2
- **URL**: http://arxiv.org/abs/2304.08485v2

## 相关论文统计

- **引用该论文的文章**: 32 篇
- **该论文引用的文章**: 25 篇
- **总计相关论文**: 57 篇

## 安全与监控

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### EgoPrivacy: What Your First-Person Camera Says About You?

- **作者**: Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos
- **ArXiv 链接**: https://arxiv.org/abs/2506.12258
- **摘要**: While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera weare...

### Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

- **作者**: Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng yu, Zhongyu Wei, Jiaqi Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.09040
- **摘要**: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in th...

### A Simple Framework for Open-Vocabulary Segmentation and Detection

- **作者**: Hao Zhang, Feng Li, Xueyan Zou, Siyi Liu, Chun-yue Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2303.08131
- **摘要**: We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotatio...

### ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models

- **作者**: Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, J. Aneja, Jianwei Yang, Ping Jin, Yong Jae Lee, Houdong Hu, Zicheng Liu, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2204.08790
- **摘要**: Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate stron...

### Grounded Language-Image Pre-training

- **作者**: Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2112.03857
- **摘要**: This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase gro...

### Microsoft COCO: Common Objects in Context

- **作者**: Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays, P. Perona, Deva Ramanan, Piotr Dollár, C. L. Zitnick
- **ArXiv 链接**: https://arxiv.org/abs/1405.0312
- **摘要**: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understandi...

## 强化学习方法

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning

- **作者**: Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
- **ArXiv 链接**: https://arxiv.org/abs/2506.17221
- **摘要**: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation s...

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Visual-Instructed Degradation Diffusion for All-in-One Image Restoration

- **作者**: Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu
- **ArXiv 链接**: https://arxiv.org/abs/2506.16960
- **摘要**: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown ...

### MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models

- **作者**: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi
- **ArXiv 链接**: https://arxiv.org/abs/2506.16157
- **摘要**: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tas...

### AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models

- **作者**: Yuan Zhang, Chun-Kai Fan, Tao Huang, Ming Lu, Sicheng Yu, Junwen Pan, Kuan Cheng, Qi She, Shanghang Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16112
- **摘要**: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuris...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### Dense360: Dense Understanding from Omnidirectional Panoramas

- **作者**: Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi
- **ArXiv 链接**: https://arxiv.org/abs/2506.14471
- **摘要**: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capab...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### MambaMia: A State-Space-Model-Based Compression for Efficient Video Understanding in Large Multimodal Models

- **作者**: Geewook Kim, Minjoon Seo
- **ArXiv 链接**: https://arxiv.org/abs/2506.13564
- **摘要**: We propose an efficient framework to compress multiple video-frame features before feeding them into large multimodal models, thereby mitigating the severe token explosion arising from long or dense v...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding

- **作者**: Chenlu Zhan, Gaoang Wang, Hongwei Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13629
- **摘要**: Semantic querying in complex 3D scenes through free-form language presents a significant challenge. Existing 3D scene understanding methods use large-scale training data and CLIP to align text queries...

### FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

- **作者**: Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13066
- **摘要**: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### ROSA: Harnessing Robot States for Vision-Language and Action Alignment

- **作者**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun
- **ArXiv 链接**: https://arxiv.org/abs/2506.13679
- **摘要**: Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A ...

### Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models

- **作者**: Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, Wentao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.12776
- **摘要**: Vision-Language Models (VLMs) face significant challenges when dealing with the diverse resolutions and aspect ratios of real-world images, as most existing models rely on fixed, low-resolution inputs...

### How Visual Representations Map to Language Feature Space in Multimodal LLMs

- **作者**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
- **ArXiv 链接**: https://arxiv.org/abs/2506.11976
- **摘要**: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly underst...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### EgoPrivacy: What Your First-Person Camera Says About You?

- **作者**: Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos
- **ArXiv 链接**: https://arxiv.org/abs/2506.12258
- **摘要**: While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera weare...

### Vision Generalist Model: A Survey

- **作者**: Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu
- **ArXiv 链接**: https://arxiv.org/abs/2506.09954
- **摘要**: Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process vario...

### ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

- **作者**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, K. Lin, Linjie Li, Furong Huang, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.10128
- **摘要**: Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation...

### Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

- **作者**: Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng yu, Zhongyu Wei, Jiaqi Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.09040
- **摘要**: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in th...

### Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs

- **作者**: Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov
- **ArXiv 链接**: https://arxiv.org/abs/2506.09047
- **摘要**: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task o...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

### A Simple Framework for Open-Vocabulary Segmentation and Detection

- **作者**: Hao Zhang, Feng Li, Xueyan Zou, Siyi Liu, Chun-yue Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2303.08131
- **摘要**: We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotatio...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### Training language models to follow instructions with human feedback

- **作者**: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, P. Welinder, P. Christiano, Jan Leike, Ryan J. Lowe
- **ArXiv 链接**: https://arxiv.org/abs/2203.02155
- **摘要**: Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpf...

### A General Language Assistant as a Laboratory for Alignment

- **作者**: Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, T. Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan
- **ArXiv 链接**: https://arxiv.org/abs/2112.00861
- **摘要**: Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, hone...

### Habitat 2.0: Training Home Assistants to Rearrange their Habitat

- **作者**: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Z. Kira, V. Koltun, Jitendra Malik, M. Savva, Dhruv Batra
- **ArXiv 链接**: https://arxiv.org/abs/2106.14405
- **摘要**: We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all leve...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 机器人学与自动驾驶

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning

- **作者**: Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
- **ArXiv 链接**: https://arxiv.org/abs/2506.17221
- **摘要**: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation s...

### Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation

- **作者**: Nick Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim
- **ArXiv 链接**: https://arxiv.org/abs/2506.14015
- **摘要**: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair styl...

### ROSA: Harnessing Robot States for Vision-Language and Action Alignment

- **作者**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun
- **ArXiv 链接**: https://arxiv.org/abs/2506.13679
- **摘要**: Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A ...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### Habitat 2.0: Training Home Assistants to Rearrange their Habitat

- **作者**: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Z. Kira, V. Koltun, Jitendra Malik, M. Savva, Dhruv Batra
- **ArXiv 链接**: https://arxiv.org/abs/2106.14405
- **摘要**: We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all leve...

### Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training

- **作者**: Weituo Hao, Chunyuan Li, Xiujun Li, L. Carin, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2002.10638
- **摘要**: Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a n...

## 视觉编码器与大语言模型融合

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning

- **作者**: Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
- **ArXiv 链接**: https://arxiv.org/abs/2506.17221
- **摘要**: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation s...

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Visual-Instructed Degradation Diffusion for All-in-One Image Restoration

- **作者**: Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu
- **ArXiv 链接**: https://arxiv.org/abs/2506.16960
- **摘要**: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown ...

### AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models

- **作者**: Yuan Zhang, Chun-Kai Fan, Tao Huang, Ming Lu, Sicheng Yu, Junwen Pan, Kuan Cheng, Qi She, Shanghang Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16112
- **摘要**: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuris...

### video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models

- **作者**: Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.15220
- **摘要**: Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an adva...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### DreamLight: Towards Harmonious and Consistent Image Relighting

- **作者**: Yong Liu, Wen‐Jun Xiao, Qianqian Wang, Junlin Chen, Shiyin Wang, Yitong Wang, Xinglong Wu, Yansong Tang
- **ArXiv 链接**: https://arxiv.org/abs/2506.14549
- **摘要**: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of ligh...

### Dense360: Dense Understanding from Omnidirectional Panoramas

- **作者**: Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi
- **ArXiv 链接**: https://arxiv.org/abs/2506.14471
- **摘要**: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capab...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems

- **作者**: Tuan Nguyen, Long-Vu Hoang, H. Tran
- **ArXiv 链接**: https://arxiv.org/abs/2506.13596
- **摘要**: This paper presents our system for the MLC-SLM Challenge 2025, focusing on multilingual speech recognition and language modeling with large language models (LLMs). Our approach combines a fine-tuned W...

### FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

- **作者**: Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13066
- **摘要**: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets...

### HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment

- **作者**: Numair Nadeem, Saeed Anwar, M. Asad, Abdul Bais
- **ArXiv 链接**: https://arxiv.org/abs/2506.13925
- **摘要**: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification bet...

### Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation

- **作者**: Nick Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim
- **ArXiv 链接**: https://arxiv.org/abs/2506.14015
- **摘要**: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair styl...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### ROSA: Harnessing Robot States for Vision-Language and Action Alignment

- **作者**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun
- **ArXiv 链接**: https://arxiv.org/abs/2506.13679
- **摘要**: Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A ...

### Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models

- **作者**: Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, Wentao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.12776
- **摘要**: Vision-Language Models (VLMs) face significant challenges when dealing with the diverse resolutions and aspect ratios of real-world images, as most existing models rely on fixed, low-resolution inputs...

### How Visual Representations Map to Language Feature Space in Multimodal LLMs

- **作者**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
- **ArXiv 链接**: https://arxiv.org/abs/2506.11976
- **摘要**: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly underst...

### Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

- **作者**: Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan
- **ArXiv 链接**: https://arxiv.org/abs/2506.11515
- **摘要**: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, i...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

- **作者**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, K. Lin, Linjie Li, Furong Huang, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.10128
- **摘要**: Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation...

### Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

- **作者**: Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng yu, Zhongyu Wei, Jiaqi Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.09040
- **摘要**: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in th...

### Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs

- **作者**: Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov
- **ArXiv 链接**: https://arxiv.org/abs/2506.09047
- **摘要**: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task o...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

### LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

- **作者**: Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Y. Qiao
- **ArXiv 链接**: https://arxiv.org/abs/2303.16199
- **摘要**: We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M l...

### MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

- **作者**: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, E. Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2303.11381
- **摘要**: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of ad...

### ViperGPT: Visual Inference via Python Execution for Reasoning

- **作者**: D'idac Sur'is, Sachit Menon, Carl Vondrick
- **ArXiv 链接**: https://arxiv.org/abs/2303.08128
- **摘要**: Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, ...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### LLaMA: Open and Efficient Foundation Language Models

- **作者**: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, M. Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
- **ArXiv 链接**: https://arxiv.org/abs/2302.13971
- **摘要**: We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art mod...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

- **作者**: Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi
- **ArXiv 链接**: https://arxiv.org/abs/2301.12597
- **摘要**: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training stra...

### OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization

- **作者**: S. Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke S. Zettlemoyer, Veselin Stoyanov
- **ArXiv 链接**: https://arxiv.org/abs/2212.12017
- **摘要**: Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization...

### Visual Programming: Compositional visual reasoning without training

- **作者**: Tanmay Gupta, Aniruddha Kembhavi
- **ArXiv 链接**: https://arxiv.org/abs/2211.11559
- **摘要**: We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it...

### OPT: Open Pre-trained Transformer Language Models

- **作者**: Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer
- **ArXiv 链接**: https://arxiv.org/abs/2205.01068
- **摘要**: Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these mode...

### Visual Prompt Tuning

- **作者**: Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, S. Lim
- **ArXiv 链接**: https://arxiv.org/abs/2203.12119
- **摘要**: The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, ie, full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effe...

### Training language models to follow instructions with human feedback

- **作者**: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, P. Welinder, P. Christiano, Jan Leike, Ryan J. Lowe
- **ArXiv 链接**: https://arxiv.org/abs/2203.02155
- **摘要**: Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpf...

### A General Language Assistant as a Laboratory for Alignment

- **作者**: Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, T. Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova Dassarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, John Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Jared Kaplan
- **ArXiv 链接**: https://arxiv.org/abs/2112.00861
- **摘要**: Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, hone...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 对比学习方法

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### Visual-Instructed Degradation Diffusion for All-in-One Image Restoration

- **作者**: Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu
- **ArXiv 链接**: https://arxiv.org/abs/2506.16960
- **摘要**: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown ...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### Dense360: Dense Understanding from Omnidirectional Panoramas

- **作者**: Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi
- **ArXiv 链接**: https://arxiv.org/abs/2506.14471
- **摘要**: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capab...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding

- **作者**: Chenlu Zhan, Gaoang Wang, Hongwei Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13629
- **摘要**: Semantic querying in complex 3D scenes through free-form language presents a significant challenge. Existing 3D scene understanding methods use large-scale training data and CLIP to align text queries...

### FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

- **作者**: Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13066
- **摘要**: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets...

### HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment

- **作者**: Numair Nadeem, Saeed Anwar, M. Asad, Abdul Bais
- **ArXiv 链接**: https://arxiv.org/abs/2506.13925
- **摘要**: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification bet...

### Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation

- **作者**: Nick Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim
- **ArXiv 链接**: https://arxiv.org/abs/2506.14015
- **摘要**: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair styl...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### Vision Generalist Model: A Survey

- **作者**: Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu
- **ArXiv 链接**: https://arxiv.org/abs/2506.09954
- **摘要**: Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process vario...

### Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs

- **作者**: Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov
- **ArXiv 链接**: https://arxiv.org/abs/2506.09047
- **摘要**: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task o...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### LLaMA: Open and Efficient Foundation Language Models

- **作者**: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, M. Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
- **ArXiv 链接**: https://arxiv.org/abs/2302.13971
- **摘要**: We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art mod...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization

- **作者**: S. Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke S. Zettlemoyer, Veselin Stoyanov
- **ArXiv 链接**: https://arxiv.org/abs/2212.12017
- **摘要**: Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization...

### ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models

- **作者**: Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, J. Aneja, Jianwei Yang, Ping Jin, Yong Jae Lee, Houdong Hu, Zicheng Liu, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2204.08790
- **摘要**: Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate stron...

### Grounded Language-Image Pre-training

- **作者**: Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2112.03857
- **摘要**: This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase gro...

### Habitat 2.0: Training Home Assistants to Rearrange their Habitat

- **作者**: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Z. Kira, V. Koltun, Jitendra Malik, M. Savva, Dhruv Batra
- **ArXiv 链接**: https://arxiv.org/abs/2106.14405
- **摘要**: We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all leve...

### Learning Transferable Visual Models From Natural Language Supervision

- **作者**: Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, I. Sutskever
- **ArXiv 链接**: https://arxiv.org/abs/2103.00020
- **摘要**: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since addition...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

## 指令微调与多模态大模型

### MambaSlip: A Novel Multimodal Large Language Model for Real-Time Robotic Slip Detection

- **作者**: Shaohua Zhang, Haoze Li, BingYi Mao, Fengda Zhao, Wenbai Chen, Guowei Gao, Peiliang Wu
- **ArXiv ID**: N/A
- **摘要**: The current robotic sliding detection tasks lack an effective contextual reasoning mechanism, which leads to inaccurate decision-making in unknown environments. To address this issue, we propose Mamba...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning

- **作者**: Zhangyang Qi, Zhixiong Zhang, Yizhou Yu, Jiaqi Wang, Hengshuang Zhao
- **ArXiv 链接**: https://arxiv.org/abs/2506.17221
- **摘要**: Vision-Language Navigation (VLN) is a core challenge in embodied AI, requiring agents to navigate real-world environments using natural language instructions. Current language model-based navigation s...

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Visual-Instructed Degradation Diffusion for All-in-One Image Restoration

- **作者**: Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu
- **ArXiv 链接**: https://arxiv.org/abs/2506.16960
- **摘要**: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown ...

### MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models

- **作者**: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi
- **ArXiv 链接**: https://arxiv.org/abs/2506.16157
- **摘要**: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tas...

### AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models

- **作者**: Yuan Zhang, Chun-Kai Fan, Tao Huang, Ming Lu, Sicheng Yu, Junwen Pan, Kuan Cheng, Qi She, Shanghang Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16112
- **摘要**: Inspired by text prompts in large language models (LLMs), visual prompts have been explored to enhance the reasoning capabilities of large vision-language models (LVLMs). Current methods design heuris...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### Dense360: Dense Understanding from Omnidirectional Panoramas

- **作者**: Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi
- **ArXiv 链接**: https://arxiv.org/abs/2506.14471
- **摘要**: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capab...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### MambaMia: A State-Space-Model-Based Compression for Efficient Video Understanding in Large Multimodal Models

- **作者**: Geewook Kim, Minjoon Seo
- **ArXiv 链接**: https://arxiv.org/abs/2506.13564
- **摘要**: We propose an efficient framework to compress multiple video-frame features before feeding them into large multimodal models, thereby mitigating the severe token explosion arising from long or dense v...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

- **作者**: Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13066
- **摘要**: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets...

### HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment

- **作者**: Numair Nadeem, Saeed Anwar, M. Asad, Abdul Bais
- **ArXiv 链接**: https://arxiv.org/abs/2506.13925
- **摘要**: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification bet...

### Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation

- **作者**: Nick Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim
- **ArXiv 链接**: https://arxiv.org/abs/2506.14015
- **摘要**: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair styl...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### ROSA: Harnessing Robot States for Vision-Language and Action Alignment

- **作者**: Yuqing Wen, Kefan Gu, Haoxuan Liu, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiaoyan Sun
- **ArXiv 链接**: https://arxiv.org/abs/2506.13679
- **摘要**: Vision-Language-Action (VLA) models have recently made significant advance in multi-task, end-to-end robotic control, due to the strong generalization capabilities of Vision-Language Models (VLMs). A ...

### Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models

- **作者**: Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, Wentao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.12776
- **摘要**: Vision-Language Models (VLMs) face significant challenges when dealing with the diverse resolutions and aspect ratios of real-world images, as most existing models rely on fixed, low-resolution inputs...

### How Visual Representations Map to Language Feature Space in Multimodal LLMs

- **作者**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
- **ArXiv 链接**: https://arxiv.org/abs/2506.11976
- **摘要**: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly underst...

### Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

- **作者**: Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan
- **ArXiv 链接**: https://arxiv.org/abs/2506.11515
- **摘要**: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, i...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

- **作者**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, K. Lin, Linjie Li, Furong Huang, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.10128
- **摘要**: Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation...

### Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

- **作者**: Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng yu, Zhongyu Wei, Jiaqi Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.09040
- **摘要**: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in th...

### Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs

- **作者**: Yaniv Nikankin, Dana Arad, Yossi Gandelsman, Yonatan Belinkov
- **ArXiv 链接**: https://arxiv.org/abs/2506.09047
- **摘要**: Vision-Language models (VLMs) show impressive abilities to answer questions on visual inputs (e.g., counting objects in an image), yet demonstrate higher accuracies when performing an analogous task o...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

### LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

- **作者**: Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Y. Qiao
- **ArXiv 链接**: https://arxiv.org/abs/2303.16199
- **摘要**: We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M l...

### MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

- **作者**: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, E. Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2303.11381
- **摘要**: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of ad...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

- **作者**: Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi
- **ArXiv 链接**: https://arxiv.org/abs/2301.12597
- **摘要**: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training stra...

### Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training

- **作者**: Weituo Hao, Chunyuan Li, Xiujun Li, L. Carin, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2002.10638
- **摘要**: Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a n...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 生成式方法

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models

- **作者**: Xingbai Chen, Tingchao Fu, Renyang Liu, Wei Zhou, Chao Yi
- **ArXiv 链接**: https://arxiv.org/abs/2506.16157
- **摘要**: Referring Expression Segmentation (RES) enables precise object segmentation in images based on natural language descriptions, offering high flexibility and broad applicability in real-world vision tas...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### DreamLight: Towards Harmonious and Consistent Image Relighting

- **作者**: Yong Liu, Wen‐Jun Xiao, Qianqian Wang, Junlin Chen, Shiyin Wang, Yitong Wang, Xinglong Wu, Yansong Tang
- **ArXiv 链接**: https://arxiv.org/abs/2506.14549
- **摘要**: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of ligh...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding

- **作者**: Chenlu Zhan, Gaoang Wang, Hongwei Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13629
- **摘要**: Semantic querying in complex 3D scenes through free-form language presents a significant challenge. Existing 3D scene understanding methods use large-scale training data and CLIP to align text queries...

### FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design

- **作者**: Kai Lan, Jiayong Zhu, Jiangtong Li, Dawei Cheng, Guang Chen, Changjun Jiang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13066
- **摘要**: Large Multimodal Models (LMMs) demonstrate significant cross-modal reasoning capabilities. However, financial applications face challenges due to the lack of high-quality multimodal reasoning datasets...

### Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation

- **作者**: Nick Huang, Akin Caliskan, Berkay Kicanaoglu, James Tompkin, Hyeongwoo Kim
- **ArXiv 链接**: https://arxiv.org/abs/2506.14015
- **摘要**: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair styl...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

- **作者**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, K. Lin, Linjie Li, Furong Huang, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.10128
- **摘要**: Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation...

### Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better

- **作者**: Dianyi Wang, Wei Song, Yikun Wang, Siyuan Wang, Kaicheng yu, Zhongyu Wei, Jiaqi Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.09040
- **摘要**: Typical large vision-language models (LVLMs) apply autoregressive supervision solely to textual sequences, without fully incorporating the visual modality into the learning process. This results in th...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### ViperGPT: Visual Inference via Python Execution for Reasoning

- **作者**: D'idac Sur'is, Sachit Menon, Carl Vondrick
- **ArXiv 链接**: https://arxiv.org/abs/2303.08128
- **摘要**: Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, ...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

- **作者**: Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi
- **ArXiv 链接**: https://arxiv.org/abs/2301.12597
- **摘要**: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training stra...

### Training language models to follow instructions with human feedback

- **作者**: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, P. Welinder, P. Christiano, Jan Leike, Ryan J. Lowe
- **ArXiv 链接**: https://arxiv.org/abs/2203.02155
- **摘要**: Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpf...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 综述类论文

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Visual-Instructed Degradation Diffusion for All-in-One Image Restoration

- **作者**: Wenyang Luo, Haina Qin, Zewen Chen, Libin Wang, Dandan Zheng, Yuming Li, Yufan Liu, Bing Li, Weiming Hu
- **ArXiv 链接**: https://arxiv.org/abs/2506.16960
- **摘要**: Image restoration tasks like deblurring, denoising, and dehazing usually need distinct models for each degradation type, restricting their generalization in real-world scenarios with mixed or unknown ...

### video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models

- **作者**: Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.15220
- **摘要**: Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an adva...

### Dense360: Dense Understanding from Omnidirectional Panoramas

- **作者**: Yikang Zhou, Tao Zhang, Dizhe Zhang, Shunping Ji, Xiangtai Li, Lu Qi
- **ArXiv 链接**: https://arxiv.org/abs/2506.14471
- **摘要**: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capab...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### MambaMia: A State-Space-Model-Based Compression for Efficient Video Understanding in Large Multimodal Models

- **作者**: Geewook Kim, Minjoon Seo
- **ArXiv 链接**: https://arxiv.org/abs/2506.13564
- **摘要**: We propose an efficient framework to compress multiple video-frame features before feeding them into large multimodal models, thereby mitigating the severe token explosion arising from long or dense v...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding

- **作者**: Chenlu Zhan, Gaoang Wang, Hongwei Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13629
- **摘要**: Semantic querying in complex 3D scenes through free-form language presents a significant challenge. Existing 3D scene understanding methods use large-scale training data and CLIP to align text queries...

### HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment

- **作者**: Numair Nadeem, Saeed Anwar, M. Asad, Abdul Bais
- **ArXiv 链接**: https://arxiv.org/abs/2506.13925
- **摘要**: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification bet...

### A Comprehensive Survey on Continual Learning in Generative Models

- **作者**: Haiyang Guo, Fanhu Zeng, Fei Zhu, Jiayi Wang, Xukai Wang, Jingang Zhou, Hongbo Zhao, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu
- **ArXiv 链接**: https://arxiv.org/abs/2506.13045
- **摘要**: The rapid advancement of generative models has enabled modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, th...

### Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models

- **作者**: Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, Wentao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.12776
- **摘要**: Vision-Language Models (VLMs) face significant challenges when dealing with the diverse resolutions and aspect ratios of real-world images, as most existing models rely on fixed, low-resolution inputs...

### How Visual Representations Map to Language Feature Space in Multimodal LLMs

- **作者**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
- **ArXiv 链接**: https://arxiv.org/abs/2506.11976
- **摘要**: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly underst...

### Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs

- **作者**: Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan
- **ArXiv 链接**: https://arxiv.org/abs/2506.11515
- **摘要**: Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, i...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### EgoPrivacy: What Your First-Person Camera Says About You?

- **作者**: Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos
- **ArXiv 链接**: https://arxiv.org/abs/2506.12258
- **摘要**: While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera weare...

### Vision Generalist Model: A Survey

- **作者**: Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu
- **ArXiv 链接**: https://arxiv.org/abs/2506.09954
- **摘要**: Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process vario...

### ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs

- **作者**: Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, K. Lin, Linjie Li, Furong Huang, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.10128
- **摘要**: Reinforcement learning (RL) has shown great effectiveness for fine-tuning large language models (LLMs) using tasks that are challenging yet easily verifiable, such as math reasoning or code generation...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

### MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

- **作者**: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, E. Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2303.11381
- **摘要**: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of ad...

### A Simple Framework for Open-Vocabulary Segmentation and Detection

- **作者**: Hao Zhang, Feng Li, Xueyan Zou, Siyi Liu, Chun-yue Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2303.08131
- **摘要**: We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotatio...

### ViperGPT: Visual Inference via Python Execution for Reasoning

- **作者**: D'idac Sur'is, Sachit Menon, Carl Vondrick
- **ArXiv 链接**: https://arxiv.org/abs/2303.08128
- **摘要**: Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, ...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### LLaMA: Open and Efficient Foundation Language Models

- **作者**: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, M. Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
- **ArXiv 链接**: https://arxiv.org/abs/2302.13971
- **摘要**: We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art mod...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

- **作者**: Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi
- **ArXiv 链接**: https://arxiv.org/abs/2301.12597
- **摘要**: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training stra...

### Visual Programming: Compositional visual reasoning without training

- **作者**: Tanmay Gupta, Aniruddha Kembhavi
- **ArXiv 链接**: https://arxiv.org/abs/2211.11559
- **摘要**: We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it...

### Habitat 2.0: Training Home Assistants to Rearrange their Habitat

- **作者**: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Z. Kira, V. Koltun, Jitendra Malik, M. Savva, Dhruv Batra
- **ArXiv 链接**: https://arxiv.org/abs/2106.14405
- **摘要**: We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all leve...

### Learning Transferable Visual Models From Natural Language Supervision

- **作者**: Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, I. Sutskever
- **ArXiv 链接**: https://arxiv.org/abs/2103.00020
- **摘要**: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since addition...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

### Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training

- **作者**: Weituo Hao, Chunyuan Li, Xiujun Li, L. Carin, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2002.10638
- **摘要**: Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a n...

### Microsoft COCO: Common Objects in Context

- **作者**: Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays, P. Perona, Deva Ramanan, Piotr Dollár, C. L. Zitnick
- **ArXiv 链接**: https://arxiv.org/abs/1405.0312
- **摘要**: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understandi...

## 视觉理解与推理

### Improved IEC performance via emotional stimuli-aware captioning

- **作者**: Zibo Zhou, Zhengjun Zhai, Xin Gao, Jiaqi Zhu
- **ArXiv ID**: N/A
- **摘要**: Image emotion classification (IEC), a crucial task in computer vision, aims to infer the emotional state of subjects in images. Existing techniques have focused on the use of semantic information to s...

### FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation

- **作者**: Fan Yang, Yousong Zhu, Xin Li, Yufei Zhan, Hongyin Zhao, Shurong Zheng, Yaowei Wang, Ming Tang, Jinqiao Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16806
- **摘要**: Recent Large Vision Language Models (LVLMs) demonstrate promising capabilities in unifying visual understanding and generative modeling, enabling both accurate content understanding and flexible editi...

### FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding

- **作者**: Chenlu Zhan, Gaoang Wang, Hongwei Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13629
- **摘要**: Semantic querying in complex 3D scenes through free-form language presents a significant challenge. Existing 3D scene understanding methods use large-scale training data and CLIP to align text queries...

### Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models

- **作者**: Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, Wentao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.12776
- **摘要**: Vision-Language Models (VLMs) face significant challenges when dealing with the diverse resolutions and aspect ratios of real-world images, as most existing models rely on fixed, low-resolution inputs...

### How Visual Representations Map to Language Feature Space in Multimodal LLMs

- **作者**: Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda
- **ArXiv 链接**: https://arxiv.org/abs/2506.11976
- **摘要**: Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly underst...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

- **作者**: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, E. Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2303.11381
- **摘要**: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of ad...

### Visual Programming: Compositional visual reasoning without training

- **作者**: Tanmay Gupta, Aniruddha Kembhavi
- **ArXiv 链接**: https://arxiv.org/abs/2211.11559
- **摘要**: We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it...

### ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models

- **作者**: Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, J. Aneja, Jianwei Yang, Ping Jin, Yong Jae Lee, Houdong Hu, Zicheng Liu, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2204.08790
- **摘要**: Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate stron...

### Grounded Language-Image Pre-training

- **作者**: Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2112.03857
- **摘要**: This paper presents a grounded language-image pretraining (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and phrase gro...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

### Microsoft COCO: Common Objects in Context

- **作者**: Tsung-Yi Lin, M. Maire, Serge J. Belongie, James Hays, P. Perona, Deva Ramanan, Piotr Dollár, C. L. Zitnick
- **ArXiv 链接**: https://arxiv.org/abs/1405.0312
- **摘要**: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understandi...

## 检索式方法

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models

- **作者**: Ling Li, Yao Zhou, Yuxuan Liang, F. Tsung, Jiaheng Wei
- **ArXiv 链接**: https://arxiv.org/abs/2506.14674
- **摘要**: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vi...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding

- **作者**: Zhucun Xue, Jiang-She Zhang, Xurong Xie, Yuxuan Cai, Yong Liu, Xiangtai Li, Dacheng Tao
- **ArXiv 链接**: https://arxiv.org/abs/2506.13589
- **摘要**: Multimodal Large Language Models (MLLMs) struggle with long videos due to fixed context windows and weak long-term dependency modeling. Existing Retrieval-Augmented Generation (RAG) methods for videos...

### Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis

- **作者**: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, J. Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz
- **ArXiv 链接**: https://arxiv.org/abs/2506.11526
- **摘要**: For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key app...

### EgoPrivacy: What Your First-Person Camera Says About You?

- **作者**: Yijiang Li, Genpei Zhang, Jiacheng Cheng, Yi Li, Xiaojun Shan, Dashan Gao, Jiancheng Lyu, Yuan Li, Ning Bi, Nuno Vasconcelos
- **ArXiv 链接**: https://arxiv.org/abs/2506.12258
- **摘要**: While the rapid proliferation of wearable cameras has raised significant concerns about egocentric video privacy, prior work has largely overlooked the unique privacy threats posed to the camera weare...

### Vision Generalist Model: A Survey

- **作者**: Ziyi Wang, Yongming Rao, Shuofeng Sun, Xinrun Liu, Yi Wei, Xumin Yu, Zuyan Liu, Yanbo Wang, Hongmin Liu, Jie Zhou, Jiwen Lu
- **ArXiv 链接**: https://arxiv.org/abs/2506.09954
- **摘要**: Recently, we have witnessed the great success of the generalist model in natural language processing. The generalist model is a general framework trained with massive data and is able to process vario...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### Multimodal Foundation Models: From Specialists to General-Purpose Assistants

- **作者**: Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2309.10020
- **摘要**: This paper presents a comprehensive survey of the taxonomy and evolution of multimodal foundation models that demonstrate vision and vision-language capabilities, focusing on the transition from speci...

### A Simple Framework for Open-Vocabulary Segmentation and Detection

- **作者**: Hao Zhang, Feng Li, Xueyan Zou, Siyi Liu, Chun-yue Li, Jianfeng Gao, Jianwei Yang, Lei Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2303.08131
- **摘要**: We present OpenSeeD, a simple Open-vocabulary Segmentation and Detection framework that jointly learns from different segmentation and detection datasets. To bridge the gap of vocabulary and annotatio...

### LLaMA: Open and Efficient Foundation Language Models

- **作者**: Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, M. Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample
- **ArXiv 链接**: https://arxiv.org/abs/2302.13971
- **摘要**: We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art mod...

### OPT: Open Pre-trained Transformer Language Models

- **作者**: Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer
- **ArXiv 链接**: https://arxiv.org/abs/2205.01068
- **摘要**: Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these mode...

### Habitat 2.0: Training Home Assistants to Rearrange their Habitat

- **作者**: Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Z. Kira, V. Koltun, Jitendra Malik, M. Savva, Dhruv Batra
- **ArXiv 链接**: https://arxiv.org/abs/2106.14405
- **摘要**: We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all leve...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 多模态对话与问答

### Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **作者**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **ArXiv 链接**: https://arxiv.org/abs/2506.16962
- **摘要**: Multimodal large language models (MLLMs) have begun to demonstrate robust reasoning capabilities on general tasks, yet their application in the medical domain remains in its early stages. Constructing...

### Improved Baselines with Visual Instruction Tuning

- **作者**: Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee
- **ArXiv 链接**: https://arxiv.org/abs/2310.03744
- **摘要**: Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in...

### LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention

- **作者**: Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, Y. Qiao
- **ArXiv 链接**: https://arxiv.org/abs/2303.16199
- **摘要**: We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M l...

### MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action

- **作者**: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, E. Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang
- **ArXiv 链接**: https://arxiv.org/abs/2303.11381
- **摘要**: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of ad...

### PaLM-E: An Embodied Multimodal Language Model

- **作者**: Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Q. Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, P. Sermanet, Daniel Duckworth, S. Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, Peter R. Florence
- **ArXiv 链接**: https://arxiv.org/abs/2303.03378
- **摘要**: Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied...

### Multimodal Chain-of-Thought Reasoning in Language Models

- **作者**: Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, G. Karypis, Alexander J. Smola
- **ArXiv 链接**: https://arxiv.org/abs/2302.00923
- **摘要**: Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer t...

### BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

- **作者**: Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi
- **ArXiv 链接**: https://arxiv.org/abs/2301.12597
- **摘要**: The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training stra...

### OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization

- **作者**: S. Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke S. Zettlemoyer, Veselin Stoyanov
- **ArXiv 链接**: https://arxiv.org/abs/2212.12017
- **摘要**: Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization...

### Visual Programming: Compositional visual reasoning without training

- **作者**: Tanmay Gupta, Aniruddha Kembhavi
- **ArXiv 链接**: https://arxiv.org/abs/2211.11559
- **摘要**: We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it...

### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

- **作者**: Soravit Changpinyo, P. Sharma, Nan Ding, Radu Soricut
- **ArXiv 链接**: https://arxiv.org/abs/2102.08981
- **摘要**: The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pretraining. However, these datasets ar...

### Grounding Language Models to Images for Multimodal Generation

- **作者**: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
- **ArXiv ID**: N/A
- **摘要**: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method levera...

## 多模态指令数据生成与利用

### video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models

- **作者**: Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang
- **ArXiv 链接**: https://arxiv.org/abs/2506.15220
- **摘要**: Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an adva...

### GenRecal: Generation after Recalibration from Large to Small Vision-Language Models

- **作者**: Byung-Kwan Lee, Ryo Hachiuma, Yonghyun Ro, Yu-Chiang Frank Wang, Yueh-Hua Wu
- **ArXiv 链接**: https://arxiv.org/abs/2506.15681
- **摘要**: Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models i...

### Evolution of ReID: From Early Methods to LLM Integration

- **作者**: Amran Bhuiyan, Mizanur Rahman, Md Tahmid Rahman Laskar, Aijun An, Jimmy X. Huang
- **ArXiv 链接**: https://arxiv.org/abs/2506.13039
- **摘要**: Person re-identification (ReID) has evolved from handcrafted feature-based methods to deep learning approaches and, more recently, to models incorporating large language models (LLMs). Early methods s...

### Instruction Tuning with GPT-4

- **作者**: Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
- **ArXiv 链接**: https://arxiv.org/abs/2304.03277
- **摘要**: Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and ...

---

**统计总结:**

- 安全与监控: 8 篇论文
- 强化学习方法: 33 篇论文
- 机器人学与自动驾驶: 10 篇论文
- 视觉编码器与大语言模型融合: 43 篇论文
- 对比学习方法: 28 篇论文
- 指令微调与多模态大模型: 35 篇论文
- 生成式方法: 20 篇论文
- 综述类论文: 35 篇论文
- 视觉理解与推理: 12 篇论文
- 检索式方法: 14 篇论文
- 多模态对话与问答: 11 篇论文
- 多模态指令数据生成与利用: 4 篇论文

**主要趋势:**

1. 多模态大模型和指令微调是主要研究方向
2. 视觉编码器与大语言模型融合技术发展迅速
3. 应用领域广泛，涵盖机器人学、安全监控、教育等多个领域
4. 技术路线多样化，包括生成式、强化学习、监督学习等方法
