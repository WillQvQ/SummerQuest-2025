# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
[1024, 1024]

#### 1.1.2
[8, 128, 1024]

#### 1.1.3
[8, 128, 1024]

每个Y_i的shape是[8, 128, 1024]
通过在dim=2上拼接4个Y_i得到完整的Y， 维度是[8, 128, 4096]。
### 1.2


#### 1.2.1
[1024, 1024]

#### 1.2.2
[8, 128, 1024]

#### 1.2.3
[8, 128, 1024]
对4个Z_i求和， 完整Z维度是[8, 128, 1024]。

## 2 通信分析

### 2.1

#### 2.1.1
前向传播不需要通信。没有显式all gather通信，通信量为0。



#### 2.1.2
需要通信。

因为前向传播时，输出是通过拼接得到的，所以反向传播时梯度需要使用all-reduce对每个rank上的X梯度求和，通信量为 $8*128*1024*4$ 

### 2.2

#### 2.2.1
需要通信。操作是all-reduce（对每个rank求和）。通信量是 $8*128*1024*4$

#### 2.2.2
不需要通信。因为前向传播已经进行了all-reduce，反向传播时梯度可以直接分发到各个rank。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？

两层都使用row parallel, 额外通信有：Linear1的输出需要进行all-reduce通信；Linear1输出到Linear2需要重新切片分发，产生 all-to-all 通信。


两层都使用column parallel 通信，会带来以下问题：Linear1输出被拼接，而Linear2输入又被切片。这破坏了column parallel设计的初衷。