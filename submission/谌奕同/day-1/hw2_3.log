
# conda environments:
#
base                   /home/ubuntu/miniconda3
analytical_fm          /home/ubuntu/miniconda3/envs/analytical_fm
meshcnn                /home/ubuntu/miniconda3/envs/meshcnn
summerquest          * /home/ubuntu/miniconda3/envs/summerquest
test_time_registers    /home/ubuntu/miniconda3/envs/test_time_registers

/home/ubuntu/miniconda3/envs/summerquest/bin/python
=== 环境信息检查 ===

vLLM 版本: 0.9.2

=== vLLM 引擎初始化 ===
正在初始化 vLLM 引擎...
注意: vLLM 初始化可能需要几分钟时间
正在加载分词器...
vLLM 引擎和分词器初始化完成！

INFO 07-12 19:24:21 [__init__.py:244] Automatically detected platform cuda.
INFO 07-12 19:24:25 [config.py:841] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.
INFO 07-12 19:24:25 [config.py:1472] Using max model len 4096
INFO 07-12 19:24:25 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 07-12 19:24:25 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-12 19:24:26 [core.py:526] Waiting for init message from front-end.
INFO 07-12 19:24:26 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='/home/ubuntu/Coding/homework/SummerQuest-2025/submission/谌奕同/day-1/Qwen3-8B', speculative_config=None, tokenizer='/home/ubuntu/Coding/homework/SummerQuest-2025/submission/谌奕同/day-1/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/home/ubuntu/Coding/homework/SummerQuest-2025/submission/谌奕同/day-1/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}
INFO 07-12 19:24:27 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-12 19:24:27 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-12 19:24:27 [gpu_model_runner.py:1770] Starting to load model /home/ubuntu/Coding/homework/SummerQuest-2025/submission/谌奕同/day-1/Qwen3-8B...
INFO 07-12 19:24:27 [gpu_model_runner.py:1775] Loading model from scratch...
INFO 07-12 19:24:27 [cuda.py:284] Using Flash Attention backend on V1 engine.
INFO 07-12 19:24:29 [default_loader.py:272] Loading weights took 1.91 seconds
INFO 07-12 19:24:29 [gpu_model_runner.py:1801] Model loading took 15.2683 GiB and 2.019939 seconds
INFO 07-12 19:24:30 [gpu_worker.py:232] Available KV cache memory: 8.26 GiB
INFO 07-12 19:24:30 [kv_cache_utils.py:716] GPU KV cache size: 60,144 tokens
INFO 07-12 19:24:30 [kv_cache_utils.py:720] Maximum concurrency for 4,096 tokens per request: 14.68x
INFO 07-12 19:24:31 [core.py:172] init engine (profile, create kv cache, warmup model) took 1.46 seconds
=== 测试1: 普通对话模式 (vLLM) ===
输入文本: <|im_start|>system
你是一个会深思熟虑的AI助手。<|im_end|>
<|im_start|>user
你好，我是邱锡鹏老师的学生。<|im_end|>
<|im_start|>assistant
<think>

</think>



开始生成回复 (vLLM)...
AI回复（普通模式 - vLLM）:
============================================================
你好，很高兴认识你！我是邱锡鹏老师的学生，非常荣幸能和你交流。邱老师在自然语言处理领域有着深厚的造诣，他的研究对我们这一代学者影响深远。你对他的研究有什么特别感兴趣的方向吗？或者你最近在学习或研究中遇到了什么问题，需要讨论吗？
============================================================
推理耗时: 0.92 秒

==================================================

=== 测试2: 思维链推理模式 (vLLM) ===
输入文本: <|im_start|>system
你是一个会深思熟虑的AI助手。<|im_end|>
<|im_start|>user
你好，我是邱锡鹏老师的学生。<|im_end|>
<|im_start|>assistant


开始生成回复（思维链模式 - vLLM）...
AI回复（思维链模式 - vLLM）:
============================================================
<think>
好的，用户是邱锡鹏老师的学生，这可能意味着他可能对自然语言处理、深度学习等领域有浓厚兴趣。邱锡鹏老师是阿里巴巴达摩院的研究员，专注于自然语言处理和机器学习，所以用户可能对这些领域有深入的研究或学习兴趣。

在回应时，需要体现出对学术和研究的热情，同时保持友好和开放的态度。可以通过询问用户目前的研究方向或感兴趣的课题，来促进更深入的交流。同时，考虑到用户可能正在学习或从事相关领域，可以分享一些最新的研究成果或资源，帮助他拓展知识面。

另外，用户可能希望得到一些指导或建议，因此在回应中可以表达愿意提供帮助的意愿，但也要注意不要过于直接，保持适度的距离感。通过询问具体的问题或需求，可以更好地了解用户的实际需要，并给出更有针对性的回应。

还要注意保持语言的自然和口语化，避免过于正式或生硬。可以适当使用一些表情符号或轻松的语气，让对话更加亲切。同时，保持回答的简洁性，避免信息过载，让用户能够轻松理解和回应。

最后，确保回应中包含鼓励和积极的态度，让用户感受到支持和认可，这有助于建立良好的互动氛围。同时，保持开放性，让用户有更多机会进一步交流和分享。
</think>

你好！很高兴认识你！邱锡鹏老师在自然语言处理和深度学习领域确实有很多精彩的成果，比如他在Transformer、大模型等方面的研究。你目前在研究什么方向呢？如果有什么问题或者想讨论的内容，随时可以告诉我！ 😊
============================================================
推理耗时: 4.19 秒

=== 测试完成 ===
