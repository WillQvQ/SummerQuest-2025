# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
shape = [d, d] = [1024,1024]

#### 1.1.2
shape = [B, S, d] = [8,128,1024]

#### 1.1.3
shape = [B, S, d] = [8,128,1024]

通过进行All-Gather操作来得到完整的Y。(实际上不需要进行这一步)
### 1.2


#### 1.2.1
shape = [d, d] = [1024,1024]

#### 1.2.2
shape = [B, S, d] = [8,128,1024]

#### 1.2.3
shape = [B, S, d] = [8,128,1024]

通过进行All-Reduce操作来得到完整的Z。

## 2 通信分析

### 2.1

#### 2.1.1
不需要通信，计算得到的Yi直接用于下一步。

#### 2.1.2
需要通信。

原因: X的总梯度是所有路径回传的梯度之和。反向传播计算∂L/∂X的时候，需要将∂L/∂Xi 加起来，才能得出最终X的梯度。这就需要进行一次All-Reduce操作。

### 2.2

#### 2.2.1
需要通信。

进行All-Reduce操作。

通信量是2 × 8 × 128 × 1024

#### 2.2.2
不需要通信。此时每个rank都有完整的Z，计算输入的梯度时 ∂L/∂Y_i = (∂L/∂Z) * (∂Z/∂Y_i)，可以各自独立做梯度计算。

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
两层都用Row Parallel，会使第一层计算GELU时，额外多加一次All-Reduce，之后在第二层输入又要进行Scatter。

两层都使用 Column Parallel，第二层的权重会变成[4d,d/4]，无法与Y_i相乘。如果想要计算，需要完整的Y，这需要一次 All-Gather。