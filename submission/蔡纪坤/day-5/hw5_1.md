# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
TODO
dxd=1024x1024
#### 1.1.2
TODO
BxSxd=8x128x1024
#### 1.1.3
TODO
Yi:8x128x1024
将所有P=4个rank的Yi在最后一维拼接得到Y:8x128x4096
### 1.2


#### 1.2.1
TODO
dxd=1024x1024
#### 1.2.2
TODO
8x128x1024
#### 1.2.3
TODO
Zi:8x128x1024
通过All-Reduce汇总所有Zi得到Z
## 2 通信分析

### 2.1

#### 2.1.1
TODO
不需要通信，通信量为0
#### 2.1.2
TODO
需要通信，为All-Reduce
反向传播中，∂L/∂X=∂L/∂Y⋅W1T。W1是列分割的,W1T是行分割的，每个rank仅持有W1T的一个行块W1(i)T。因此，每个rank计算的∂L/∂Xi=∂L/∂Y⋅W1(i)T是∂L/∂X的部分梯度，需通过All-Reduce汇总所有rank的∂L/∂Xi得到完整梯度。
### 2.2

#### 2.2.1
TODO
需要通信，为All-Reduce
通信量为(P-1)xBxSxd=3x8x128x1024
#### 2.2.2
TODO
不需要通信
反向传播中，∂L/∂Y=∂L/∂Z⋅W2T。由于W2是行分割的，W2T是列分割的，每个rank仅持有W2T的一个列块W2(i)T。而∂L/∂Z是前向All-Reduce后的完整张量，因此每个rank计算的∂L/∂Yi=∂L/∂Z⋅W2(i)T是∂L/∂Y的 “列分块”，直接拼接即可得到完整梯度，无需通信。
# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
TODO
两层都使用Row Parallel会产生额外的前向通信
Linear1用Row Parallel，前向需对输入X分割后计算，再通过All-Reduce汇总结果
Linear2用Row Parallel时前向也需All-Reduce

使用Column Parallel会使反向传播通信量剧增
Linear1和Linear2均要反向All-Reduce汇总梯度