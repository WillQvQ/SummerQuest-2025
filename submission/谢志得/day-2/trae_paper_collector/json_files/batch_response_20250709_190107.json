{
  "timestamp": "2025-07-09T19:01:07.136052",
  "request_paper_ids": [
    "412f63efec05fb86bb5320ccf73018e4ad6d2591",
    "5ce560024ae6a64f46ec82172557876251146faa",
    "70d0678bc39671d72d2545e472ed991b8cef5554",
    "4b407a3f30a08b75df24707f3b8440cfe0d8f7f0",
    "f9762ec9856e6210e0bfba77e8fd2487eb24422f",
    "1472a5bd7dedeee8c3306108118f467cb5c82cbc",
    "fa9cdb8474930d034b20a6035605e09b462d3839",
    "eb3e3bec67739bd116f289446181838d6603873c",
    "9b76003361aa6238fcece6ad0bafdbce15812163",
    "5a7d18baa9c2b43b5f3e02f27a6ed3b9f9e65a6c",
    "833664c6a39fc769ff0d12275bdeee883397fc42",
    "cc5dd7ed24abfa24fdc48aeb5f3fe7e5e7677521",
    "28f5f0763fee9184dbcef34684359867416129bf",
    "e33dd2510e4ca31a2e8f4ffaa67c2eaecb81cbe5",
    "c8779507ec5e8c5c6812da756005eee4b920d65d",
    "33c9d335353a3f0f2d23a1cad5b9b7edddd880c6",
    "1cfcb419dc9d88563fa8dec3a141940b7f02809e",
    "9207959dd4a608a294a578156d1398de6f267051",
    "2dde8139c667e8f1a55ab406dac3dad85e120c7a",
    "cad526a6310d6ce660daa9b110dc9ed56bb42095",
    "4137f4aa17d5d513da2a1bbc10b1e9e533a3832f",
    "ede508d0d1419563812434c7403d3522371711f4",
    "face928230ea69c69e92f3f451a014f4304f2ef1",
    "1ecb4fb3ab37d581da0d3776b0f6bae9d53d18c7",
    "2079f9ca26f9811cb5da8a8967eefae576ce55f9",
    "ab5acad2b6ff5ab93c0fc35b95a7a203ad28caad",
    "9238292ce32dc0630878c0096e74a50ee05d3f02",
    "10c7e31b4a87e0fb1106ffcb0016fee833e1c929",
    "34d4d99f89edb2cc3b55bf439c06629ae970bf1f",
    "cf4745bc59333037511ff3bbec096be5b992dcc0",
    "8f4dfb74b11d2c892584164189c47836b46dcde0",
    "8574585dd51db51a7a283afb655cc7fb5e790a32",
    "a74923df8e67ee27f1dc220a910dad2d04cc0267",
    "38357218f59b1344bd431cdbb605e5e208b5b96f",
    "5c948dc907dd379db1b060d5f13245511f62be2d",
    "4fc209fe0259254f3734f2e63213d25d6fb1bc55",
    "1d97ac155a9788ca1f7bae0c83a8496df5830e76",
    "af8b1ef04f8ac33576b7794aa7075b7726a56aa0",
    "1e8333cf85e6052f1718f27760d2d9d3e411acc9",
    "e4c11b4f6eaaa079856d81f39be8924e5bd1ba40",
    "dc67c56d146d74d26542bddc77c21601820a4f5f",
    "2174f1e6274bbe417ce5c5a2526699b0bcd8ec13",
    "54d86626380842f519514b2e8f9f74a3ef084c0c",
    "99079bf0bb1ec840558c8124dafb3ba5ea871bb9",
    "6c4d6224244557e0ad7ffe423390d388b6a1e736",
    "110e0292fdf62799f7c088d1b58b0d5118133403",
    "d5e5334f7a22015efaab7ec5aa802a84c229dcce",
    "4d8f85dfe7dbbf040a6d5a71c125ab1eb75c1f82",
    "89d8b82548cde91cb5801afaafeb6031901c8347",
    "254ff3be571f60be969668c66c531be4bab9e026",
    "da85d7361b597b67eb856663320da708f074ea20",
    "739a7677b12d9668e07730ca8a60d2ba646f5f24",
    "1dc866d8276330fccf896d9c766990c7c21679d7",
    "08fb31ebea65100196a7553775248c8ee4867ab3",
    "e54f2a86192ea5437e64107d6f4f3534b702ad8c",
    "72b97f98a354e7352ba218be16f601610387f5b2",
    "e70b5b1f7186d8bc4159b4e05c3e8d1ca93bcb26",
    "85317048752487e06ba31239eb5b4233c4337242",
    "4beb1fc208fa17b836a05e05f0bb80e81655d562",
    "dc5fb12a1342e1711e2e289b2d9ef38128f25399",
    "0692b401d6ccd3a6bd4d2ebd87326a52d493d6da",
    "1703d08be2e03f52fb5d2211bd8157ced4572408",
    "fc7c4cdc2ea4dd56b800c34443a62332a8c49338",
    "ea521c1ee2cfdfbc054739a15b166709a0aa86fa",
    "17bc0ff84559502ff6974a00ee0b2e9c61015e73",
    "3f7aeb0658f8b272f5fce1d95d164c210c387cee",
    "7ce0344f9b94e2e0661a96dd07a52801cb7fa679",
    "18c8da7705fda0ea1e919c5eb937fcff4dc960b7",
    "44fd2dcb9fe45e37adc92b9fb4021ad93b8389a9",
    "1e934631aeb726b5cd2a68d02a88f98c9e60ce71",
    "ec2893ce63a5e3d22b63ca3c7ff7e62e4a11d91e",
    "d4b822a4fedaddc1795d475f0a2ab431d0f20f6e",
    "37b21fbef9af92026e33b436ae08012822501b98",
    "07090d5c281bde0693ad82547c46e288d30fc4e1",
    "91af851f674aaa6064827527a3f40c733e51f742",
    "a2e30086789e1516a21042ceed3c3fa4ffdb008f",
    "56b695cbbd2e5784b25878328d0b5e57a0926591",
    "4a7bf3465bc58be4785c5f57caaeb941c4eb5483",
    "e3183645bfea3289b272d99430f8008a9bfc0589",
    "873a65022c91b98ae7d5bfdbe94cdceb8b9db4c6",
    "add637341443b4980988cd2096074f06019d6341",
    "40d1e0a1e8a861305f9354be747620782fc203ce",
    "d335b1ab64168530c9ecdf07fcbbf7ec87bea414",
    "ae63d95bf1b98d9263f162a03069533da95953c5",
    "19b460d0065662f331c06ea05dbe8218d277e346",
    "6c306b9b3dfe5976a750362b7d569d60af640998",
    "20df6f8959275e73e688041381e462d2780bd7ae",
    "50b3ea4c92bb6498a23b7e696ef4da75ecbe1ef1",
    "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
    "ba1bc45bc25d53d88ebd95ac54c310fe5ad5e14f",
    "fcd612dd83d2e26f5548c4c545960590631063a6",
    "9371e492fb89125eafadaf47f8082cf2df7f8f84",
    "4c2e1d89f0990cea66fe0be20b80ef2e8c2e4d09",
    "1053a3c5bced66e4ce07aec4e50b47539cd836b9",
    "ce7957628b6aa45f3db29ddc1ed013122cf05e44",
    "e3b3311afc0e305747162d8b61305d5763da7818",
    "54eba3f516b28b439a69a6189a42417fe7f05bc5",
    "1976dd56cc0b986b181025343f9f714987184576",
    "5c0c62a4a5b7ab7ba4b86e848f9cdab4e699eb38",
    "f29d1283fc60e1dbb84981548e73e0a03a86cf8c",
    "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
    "f03b4ff1b4943691cec703b508c0a91f2d97a881",
    "c8f41160130980c1ffead5a812cf2b3c6b03049f",
    "6e9f1f05b7bef4b5afaf25df6952bc6bf0662179",
    "024006d4c2a89f7acacc6e4438d156525b60a98f",
    "e89d656a39fc3b08af47ebb9a583e182a596dabe",
    "bb1a17010254abfa5e1f2a17553582ce449f8e16",
    "08883c7ca6a500342975e01cc5efc4b45e43ebec",
    "bb97f7ab1bbd36a6c2a18889f75efae414cf4752",
    "d388e15a41b98f5fde97bd6a50f73aa57d6e7801",
    "d0c61536927c2f5dc2ddb74664268a3623580b9c",
    "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
    "125d262f1c66f75f3af9c3c8759abcc02ebb4ef9",
    "e15cf50aa89fee8535703b9f9512fca5bfc43327",
    "2a1d2ccf4f9d63ecdd5f7373a04926a33f8d9e5d",
    "83b35ec3ca272ae0f583297d13d1de4e007ef3c7",
    "0db78a2047517227ca70b194fc02c9b12281dfce",
    "6bdb186ec4726e00a8051119636d4df3b94043b5",
    "12ecc2d786080f638a01b9999518e9386baa157d",
    "72b71e08399226858dac8ef9ed2a90289b89f140",
    "193edd20cae92c6759c18ce93eeea96afd9528eb",
    "2319a491378867c7049b3da055c5df60e1671158",
    "ca7f25d5b139c684a8d477e954380138dcba3a73",
    "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
    "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
    "b6bfae6efa1110a57a4d8362721d152d78aae358",
    "f22b01deebcd471ccee8a3039a6f0fd09ff78b03",
    "5ff5e41617829b090c649eacb2ca3277a820dbd8",
    "244539f454800697ed663326b7cfba337ca0c2ec",
    "cdfcac1e3291b10424b5858ac0ecc9daf702efc4",
    "9b223c8a31e0ea1d1f2c9787ffd8416dfc90c912",
    "260b98e772c4785fa06a5e8fe1c205eb05ec01e2",
    "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
    "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
    "84069287da0a6b488b8c933f3cb5be759cb6237e",
    "0f0d11429e5aaecbc9fce8445afaa3bad7a74888",
    "8bfda7a7f9c1483e2d51ed13ab9c21dc10392d95",
    "b977da44048fca64acfc66f96bb31682b41bebbb",
    "52449f97e09c7465adbc1d4f16e063802d392530",
    "398c296d0cc7f9d180f84969f8937e6d3a413796",
    "6658bbf68995731b2083195054ff45b4eca38b3a",
    "aaf42313fb89aa731f538c0066f34e8cac4dd878",
    "8e2c9cddd9317ded47bce49e96bda2f9d55a70db",
    "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
    "4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
    "85e4dbcff0b63773db298562ae3fff258eea195f",
    "79ab3c49903ec8cb339437ccf5cf998607fc313e",
    "f0640da2565bad7037d969e9f07276c10102083d",
    "e0456b744af84f07cb5e750217463214f96c921e",
    "9b925d8c60f53e929f2651c2ea24713f820d9feb",
    "0f32846ca519e1966de3dbfecacf7188347b8c7b",
    "1cacfca823b940c9741f244229fcf11741b0e278",
    "2039f94c8d8342e1e5d36e9cb525cc1f172ba8ed",
    "d2c733e34d48784a37d717fe43d9e93277a8c53e",
    "1e80f755bcbf10479afd2338cec05211fdbd325c",
    "220ad0189a4c9ee5b5c299f269a0e4bef290e8fd",
    "0645c8792d5095f0de45e95c5fd9de5238f426e6",
    "a90998e0023db48b207cee3b39b0441b3935aaa7",
    "3e4a739b9792b186342f6dd1c5e5156e1beb3dd9",
    "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
    "a96800c7b8e693f82a925298b9c177fa9b05e1fc",
    "5540bfd2b60fabcf767d08f2434d45a707ef917d",
    "56632f37604eb35586d1f72a75b95e940eec0354",
    "46f5b539cb51319a9f1dc3c59350e8f456877763",
    "ccd64fac85a1766c660c1d2e0eee0904b3d6139d",
    "92740d5a42268afec52f9d8a549cdb2559d68178",
    "57f25eb66bf97e15fdac0912a671a61e666ec6ee",
    "28271f5e25cabca2905d4c9113f0fef3f16751c8",
    "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
    "2b51d022d33c1d48a8ebddf38e68117740ba6dcd",
    "72d8fe43c1ebeceb7dbb1dd6faf10c0234a9b6bf",
    "5d0184c044e13feea0d6539f4a6b8c31e49e0e90",
    "5562a56da3a96dae82add7de705e2bd841eb00fc",
    "647ff4a5bbfcae590db99be32fc635729a793ffc",
    "fe869cb220083fb36d1c953eb712989d9c178ba7",
    "7eccec6d021c0f32b20020a4f1742161b14007b2",
    "2f7e990b73d871b7dc046e19ebfa321dd3eef779",
    "c32bbf351bfc06ae8b70a31216f90003d9af1a70",
    "210225cacf1460daba8fa8c604b44f27a825da9c",
    "86778c6ae42381fd6620f1ba8c870e3b887285fb",
    "69e68bfaadf2dccff800158749f5a50fe82d173b",
    "fc4420935ce04cbcc43ad5af5bde6f0e5f236529",
    "633eaaaeca98f6796bde40e4cf5c5d5865e5b68b",
    "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
    "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
    "fc2ffb9daf9f0dd07e755d7ad5633907efb0a4b7",
    "48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b",
    "4c915c1eecb217c123a36dc6d3ce52d12c742614",
    "5e405b71a021238cb612bfc65b77baa09a245073",
    "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c",
    "24919c3d4de529d9ec4df381554d514c7b087760",
    "7cba0547426a675ec06cfa0a3a82a8d4c8348a0a",
    "5e9dc8d71572719cec58ec815bbd331fbd07fa15",
    "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
    "c5a086109367fcaef613fa5573bd4035171f0e8e",
    "5a47ba057a858f8c024d2518cc3731fc7eb40de1"
  ],
  "response_count": 196,
  "raw_response": [
    {
      "paperId": "412f63efec05fb86bb5320ccf73018e4ad6d2591",
      "externalIds": {
        "DBLP": "journals/scl/bidiCHL25",
        "DOI": "10.1016/j.sysconle.2025.106102",
        "CorpusId": 270228115
      },
      "title": "A novel approach to feedback control with deep reinforcement learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.sysconle.2025.106102?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.sysconle.2025.106102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261082887",
          "name": "Kala Agbo bidi"
        },
        {
          "authorId": "14910331",
          "name": "J. Coron"
        },
        {
          "authorId": "2046132510",
          "name": "Amaury Hayat"
        },
        {
          "authorId": "1443832112",
          "name": "Nathan Lichtlé"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5ce560024ae6a64f46ec82172557876251146faa",
      "externalIds": {
        "ArXiv": "2506.16753",
        "CorpusId": 279464868
      },
      "title": "Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.16753, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2056444443",
          "name": "Kosuke Nakanishi"
        },
        {
          "authorId": "2319409714",
          "name": "Akihiro Kubo"
        },
        {
          "authorId": "2135011683",
          "name": "Yuji Yasui"
        },
        {
          "authorId": "2240892699",
          "name": "Shin Ishii"
        }
      ],
      "abstract": "Recently, robust reinforcement learning (RL) methods designed to handle adversarial input observations have received significant attention, motivated by RL's inherent vulnerabilities. While existing approaches have demonstrated reasonable success, addressing worst-case scenarios over long time horizons requires both minimizing the agent's cumulative rewards for adversaries and training agents to counteract them through alternating learning. However, this process introduces mutual dependencies between the agent and the adversary, making interactions with the environment inefficient and hindering the development of off-policy methods. In this work, we propose a novel off-policy method that eliminates the need for additional environmental interactions by reformulating adversarial learning as a soft-constrained optimization problem. Our approach is theoretically supported by the symmetric property of policy evaluation between the agent and the adversary. The implementation is available at https://github.com/nakanakakosuke/VALT_SAC."
    },
    {
      "paperId": "70d0678bc39671d72d2545e472ed991b8cef5554",
      "externalIds": {
        "ArXiv": "2506.15190",
        "CorpusId": 279447346
      },
      "title": "Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.15190, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2341668775",
          "name": "Jiyi Wang"
        },
        {
          "authorId": "2218262293",
          "name": "Jingyang Ke"
        },
        {
          "authorId": "2367524747",
          "name": "Bo Dai"
        },
        {
          "authorId": "2367756488",
          "name": "Anqi Wu"
        }
      ],
      "abstract": "Animals flexibly recombine a finite set of core motor primitives to meet diverse task demands, but existing behavior-segmentation methods oversimplify this process by imposing discrete syllables under restrictive generative assumptions. To reflect the animal behavior generation procedure, we introduce skill-based imitation learning (SKIL) for behavior understanding, a reinforcement learning-based imitation framework that (1) infers interpretable skill sets, i.e., latent basis functions of behavior, by leveraging representation learning on transition probabilities, and (2) parameterizes policies as dynamic mixtures of these skills. We validate our approach on a simple grid world, a discrete labyrinth, and unconstrained videos of freely moving animals. Across tasks, it identifies reusable skill components, learns continuously evolving compositional policies, and generates realistic trajectories beyond the capabilities of traditional discrete models. By exploiting generative behavior modeling with compositional representations, our method offers a concise, principled account of how complex animal behaviors emerge from dynamic combinations of fundamental motor primitives."
    },
    {
      "paperId": "4b407a3f30a08b75df24707f3b8440cfe0d8f7f0",
      "externalIds": {
        "ArXiv": "2506.13536",
        "DBLP": "conf/iclr/SaxenaBAWSNMX25",
        "CorpusId": 274510777
      },
      "title": "What Matters in Learning from Large-Scale Datasets for Robot Manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.13536, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2311885269",
          "name": "Vaibhav Saxena"
        },
        {
          "authorId": "2260124870",
          "name": "Matthew Bronars"
        },
        {
          "authorId": "2141127741",
          "name": "N. R. Arachchige"
        },
        {
          "authorId": "2333961642",
          "name": "Kuancheng Wang"
        },
        {
          "authorId": "2360757506",
          "name": "Woo-Chul Shin"
        },
        {
          "authorId": "3457048",
          "name": "Soroush Nasiriany"
        },
        {
          "authorId": "49686756",
          "name": "Ajay Mandlekar"
        },
        {
          "authorId": "2260291195",
          "name": "Danfei Xu"
        }
      ],
      "abstract": "Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a systematic understanding of what data should be collected to improve the utility of a robotics dataset and facilitate downstream policy learning. In this work, we conduct a large-scale dataset composition study to answer this question. We develop a data generation framework to procedurally emulate common sources of diversity in existing datasets (such as sensor placements and object types and arrangements), and use it to generate large-scale robot datasets with controlled compositions, enabling a suite of dataset composition studies that would be prohibitively expensive in the real world. We focus on two practical settings: (1) what types of diversity should be emphasized when future researchers collect large-scale datasets for robotics, and (2) how should current practitioners retrieve relevant demonstrations from existing datasets to maximize downstream policy performance on tasks of interest. Our study yields several critical insights -- for example, we find that camera poses and spatial arrangements are crucial dimensions for both diversity in collection and alignment in retrieval. In real-world robot learning settings, we find that not only do our insights from simulation carry over, but our retrieval strategies on existing datasets such as DROID allow us to consistently outperform existing training strategies by up to 70%. More results at https://robo-mimiclabs.github.io/"
    },
    {
      "paperId": "f9762ec9856e6210e0bfba77e8fd2487eb24422f",
      "externalIds": {
        "ArXiv": "2506.13498",
        "CorpusId": 279403201
      },
      "title": "A Survey on Imitation Learning for Contact-Rich Tasks in Robotics",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.13498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2336649969",
          "name": "Toshiaki Tsuji"
        },
        {
          "authorId": "2283348370",
          "name": "Yasuhiro Kato"
        },
        {
          "authorId": "2299715578",
          "name": "Gokhan Solak"
        },
        {
          "authorId": "2299917022",
          "name": "Heng Zhang"
        },
        {
          "authorId": "2237985286",
          "name": "Tadej Petrivc"
        },
        {
          "authorId": "2367190775",
          "name": "Francesco Nori"
        },
        {
          "authorId": "2349803262",
          "name": "Arash Ajoudani"
        }
      ],
      "abstract": "This paper comprehensively surveys research trends in imitation learning for contact-rich robotic tasks. Contact-rich tasks, which require complex physical interactions with the environment, represent a central challenge in robotics due to their nonlinear dynamics and sensitivity to small positional deviations. The paper examines demonstration collection methodologies, including teaching methods and sensory modalities crucial for capturing subtle interaction dynamics. We then analyze imitation learning approaches, highlighting their applications to contact-rich manipulation. Recent advances in multimodal learning and foundation models have significantly enhanced performance in complex contact tasks across industrial, household, and healthcare domains. Through systematic organization of current research and identification of challenges, this survey provides a foundation for future advancements in contact-rich robotic manipulation."
    },
    {
      "paperId": "1472a5bd7dedeee8c3306108118f467cb5c82cbc",
      "externalIds": {
        "ArXiv": "2506.13762",
        "CorpusId": 279410535
      },
      "title": "Touch begins where vision ends: Generalizable policies for contact-rich manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.13762, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367499023",
          "name": "Zifan Zhao"
        },
        {
          "authorId": "51445278",
          "name": "Siddhant Haldar"
        },
        {
          "authorId": "2367551136",
          "name": "Jinda Cui"
        },
        {
          "authorId": "2320806817",
          "name": "Lerrel Pinto"
        },
        {
          "authorId": "152466940",
          "name": "Raunaq M. Bhirangi"
        }
      ],
      "abstract": "Data-driven approaches struggle with precise manipulation; imitation learning requires many hard-to-obtain demonstrations, while reinforcement learning yields brittle, non-generalizable policies. We introduce VisuoTactile Local (ViTaL) policy learning, a framework that solves fine-grained manipulation tasks by decomposing them into two phases: a reaching phase, where a vision-language model (VLM) enables scene-level reasoning to localize the object of interest, and a local interaction phase, where a reusable, scene-agnostic ViTaL policy performs contact-rich manipulation using egocentric vision and tactile sensing. This approach is motivated by the observation that while scene context varies, the low-level interaction remains consistent across task instances. By training local policies once in a canonical setting, they can generalize via a localize-then-execute strategy. ViTaL achieves around 90% success on contact-rich tasks in unseen environments and is robust to distractors. ViTaL's effectiveness stems from three key insights: (1) foundation models for segmentation enable training robust visual encoders via behavior cloning; (2) these encoders improve the generalizability of policies learned using residual RL; and (3) tactile sensing significantly boosts performance in contact-rich tasks. Ablation studies validate each of these insights, and we demonstrate that ViTaL integrates well with high-level VLMs, enabling robust, reusable low-level skills. Results and videos are available at https://vitalprecise.github.io."
    },
    {
      "paperId": "fa9cdb8474930d034b20a6035605e09b462d3839",
      "externalIds": {
        "ArXiv": "2506.13867",
        "CorpusId": 279410494
      },
      "title": "ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.13867, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2260344636",
          "name": "Yunchu Zhang"
        },
        {
          "authorId": "2367272704",
          "name": "Shubham Mittal"
        },
        {
          "authorId": "2333477098",
          "name": "Zhengyu Zhang"
        },
        {
          "authorId": "3383717",
          "name": "Liyiming Ke"
        },
        {
          "authorId": "2329304903",
          "name": "Siddhartha Srinivasa"
        },
        {
          "authorId": "2260595777",
          "name": "Abhishek Gupta"
        }
      ],
      "abstract": "Visuomotor policies often suffer from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations, like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances.In this work, we leverage 2D keypoints - spatially consistent features in the image frame - as a flexible state representation for robust policy learning and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method, ATK, to automatically select keypoints in a task-driven manner so that the chosen keypoints are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of keypoints that focus on task-relevant parts while preserving policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively encodes states and transfers policies to the real-world evaluation scenario despite wide scene variations and perceptual challenges such as transparent objects, fine-grained tasks, and deformable objects manipulation. We validate ATK on various robotic tasks, demonstrating that these minimal keypoint representations significantly improve robustness to visual disturbances and environmental variations. See all experiments and more details on our website."
    },
    {
      "paperId": "eb3e3bec67739bd116f289446181838d6603873c",
      "externalIds": {
        "ArXiv": "2506.11172",
        "CorpusId": 279391422
      },
      "title": "Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.11172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2367354627",
          "name": "Xue Zhou"
        },
        {
          "authorId": "2107325",
          "name": "Dapeng Man"
        },
        {
          "authorId": "2353368753",
          "name": "Chenye Xu"
        },
        {
          "authorId": "2367041622",
          "name": "Fanyi Zeng"
        },
        {
          "authorId": "2293568463",
          "name": "Tao Liu"
        },
        {
          "authorId": "2367240302",
          "name": "Huan Wang"
        },
        {
          "authorId": "2367234985",
          "name": "Shucheng He"
        },
        {
          "authorId": "2367723421",
          "name": "Chaoyang Gao"
        },
        {
          "authorId": "2367093490",
          "name": "Wu Yang"
        }
      ],
      "abstract": "Offline reinforcement learning (RL) heavily relies on the coverage of pre-collected data over the target policy's distribution. Existing studies aim to improve data-policy coverage to mitigate distributional shifts, but overlook security risks from insufficient coverage, and the single-step analysis is not consistent with the multi-step decision-making nature of offline RL. To address this, we introduce the sequence-level concentrability coefficient to quantify coverage, and reveal its exponential amplification on the upper bound of estimation errors through theoretical analysis. Building on this, we propose the Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack. Considering the continuous nature of offline RL data, we convert state-action pairs into decision units, and extract representative decision patterns that capture multi-step behavior. We identify rare patterns likely to cause insufficient coverage, and poison them to reduce coverage and exacerbate distributional shifts. Experiments show that poisoning just 1% of the dataset can degrade agent performance by 90%. This finding provides new perspectives for analyzing and safeguarding the security of offline RL."
    },
    {
      "paperId": "9b76003361aa6238fcece6ad0bafdbce15812163",
      "externalIds": {
        "DOI": "10.1145/3744351",
        "CorpusId": 279256012
      },
      "title": "Runtime Learning Machine",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3744351?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3744351, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2320474682",
          "name": "Yihao Cai"
        },
        {
          "authorId": "2262087254",
          "name": "Yanbing Mao"
        },
        {
          "authorId": "2286131680",
          "name": "L. Sha"
        },
        {
          "authorId": "2280065247",
          "name": "Hongpeng Cao"
        },
        {
          "authorId": "2237987292",
          "name": "Marco Caccamo"
        }
      ],
      "abstract": "This paper proposes the runtime learning machine for safety-critical learning-enabled cyber-physical systems (CPS). The learning machine has three interactive components: a high-performance (HP)-Student, a high-assurance (HA)-Teacher, and a Coordinator. The HP-Student is a high-performance but not fully verified Phy-DRL (physics-regulated deep reinforcement learning) agent that performs runtime learning in real CPS, using real-time sensor data from real-time physical environments. On the other hand, HA-Teacher is a verified but simplified design, focusing on safety-critical functions only. As a complementary, HA-Teacher's novelty lies in real-time patch for two missions: i) correcting unsafe learning of HP-Student, and ii) backing up safety. The Coordinator manages the interaction between HP-Student and HA-Teacher. Powered by the three interactive components, the runtime learning machine notably features i) assuring lifetime safety (i.e., safety guarantee in any runtime learning stage), ii) tolerating unknown unknowns, iii) addressing Sim2Real gap, and iv) automatic hierarchy learning (i.e., safety-first learning, and then high-performance learning). Experiments involving a cart-pole system and two quadruped robots, as well as comparisons with state-of-the-art safe DRL, fault-tolerant DRL, and approaches for addressing Sim2Real gap, demonstrate the learning machine's effectiveness and unique features."
    },
    {
      "paperId": "5a7d18baa9c2b43b5f3e02f27a6ed3b9f9e65a6c",
      "externalIds": {
        "ArXiv": "2506.08630",
        "CorpusId": 279261198
      },
      "title": "Modular Recurrence in Contextual MDPs for Universal Morphology Control",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2103179136",
          "name": "Laurens Engwegen"
        },
        {
          "authorId": "2159061761",
          "name": "Daan Brinks"
        },
        {
          "authorId": "84187676",
          "name": "Wendelin Bohmer"
        }
      ],
      "abstract": "A universal controller for any robot morphology would greatly improve computational and data efficiency. By utilizing contextual information about the properties of individual robots and exploiting their modular structure in the architecture of deep reinforcement learning agents, steps have been made towards multi-robot control. Generalization to new, unseen robots, however, remains a challenge. In this paper we hypothesize that the relevant contextual information is partially observable, but that it can be inferred through interactions for better generalization to contexts that are not seen during training. To this extent, we implement a modular recurrent architecture and evaluate its generalization performance on a large set of MuJoCo robots. The results show a substantial improved performance on robots with unseen dynamics, kinematics, and topologies, in four different environments."
    },
    {
      "paperId": "833664c6a39fc769ff0d12275bdeee883397fc42",
      "externalIds": {
        "ArXiv": "2506.08639",
        "CorpusId": 279261277
      },
      "title": "Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08639, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2223806757",
          "name": "Amir Hossein Barjini"
        },
        {
          "authorId": "2328015682",
          "name": "Seyed Adel Alizadeh Kolagar"
        },
        {
          "authorId": "2274622239",
          "name": "S. Yaqubi"
        },
        {
          "authorId": "143939480",
          "name": "J. Mattila"
        }
      ],
      "abstract": "This article presents a motion planning and control framework for flexible robotic manipulators, integrating deep reinforcement learning (DRL) with a nonlinear partial differential equation (PDE) controller. Unlike conventional approaches that focus solely on control, we demonstrate that the desired trajectory significantly influences endpoint vibrations. To address this, a DRL motion planner, trained using the soft actor-critic (SAC) algorithm, generates optimized trajectories that inherently minimize vibrations. The PDE nonlinear controller then computes the required torques to track the planned trajectory while ensuring closed-loop stability using Lyapunov analysis. The proposed methodology is validated through both simulations and real-world experiments, demonstrating superior vibration suppression and tracking accuracy compared to traditional methods. The results underscore the potential of combining learning-based motion planning with model-based control for enhancing the precision and stability of flexible robotic manipulators."
    },
    {
      "paperId": "cc5dd7ed24abfa24fdc48aeb5f3fe7e5e7677521",
      "externalIds": {
        "ArXiv": "2506.07085",
        "CorpusId": 279250255
      },
      "title": "State Entropy Regularization for Robust Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.07085, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2366067452",
          "name": "Uri Koren"
        },
        {
          "authorId": "2366067794",
          "name": "Yonatan Ashlag"
        },
        {
          "authorId": "51004138",
          "name": "Mirco Mutti"
        },
        {
          "authorId": "46818360",
          "name": "E. Derman"
        },
        {
          "authorId": "145180695",
          "name": "Pierre-Luc Bacon"
        },
        {
          "authorId": "49185899",
          "name": "Shie Mannor"
        }
      ],
      "abstract": "State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation."
    },
    {
      "paperId": "28f5f0763fee9184dbcef34684359867416129bf",
      "externalIds": {
        "DBLP": "journals/corr/abs-2506-04022",
        "ArXiv": "2506.04022",
        "DOI": "10.48550/arXiv.2506.04022",
        "CorpusId": 279155007
      },
      "title": "Interpretability by Design for Efficient Multi-Objective Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.04022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365197289",
          "name": "Qiyue Xia"
        },
        {
          "authorId": "2365266162",
          "name": "J. M. Herrmann"
        }
      ],
      "abstract": "Multi-objective reinforcement learning (MORL) aims at optimising several, often conflicting goals in order to improve flexibility and reliability of RL in practical tasks. This can be achieved by finding diverse policies that are optimal for some objective preferences and non-dominated by optimal policies for other preferences so that they form a Pareto front in the multi-objective performance space. The relation between the multi-objective performance space and the parameter space that represents the policies is generally non-unique. Using a training scheme that is based on a locally linear map between the parameter space and the performance space, we show that an approximate Pareto front can provide an interpretation of the current parameter vectors in terms of the objectives which enables an effective search within contiguous solution domains. Experiments are conducted with and without retraining across different domains, and the comparison with previous methods demonstrates the efficiency of our approach."
    },
    {
      "paperId": "e33dd2510e4ca31a2e8f4ffaa67c2eaecb81cbe5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2506-03863",
        "ArXiv": "2506.03863",
        "DOI": "10.48550/arXiv.2506.03863",
        "CorpusId": 279155031
      },
      "title": "STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.03863, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2307185232",
          "name": "Haochuan Li"
        },
        {
          "authorId": "2305619699",
          "name": "Qi Lv"
        },
        {
          "authorId": "2267332946",
          "name": "Rui Shao"
        },
        {
          "authorId": "2267916805",
          "name": "Xiang Deng"
        },
        {
          "authorId": "2347549892",
          "name": "Yinchuan Li"
        },
        {
          "authorId": "2347881563",
          "name": "Jianye Hao"
        },
        {
          "authorId": "2258952251",
          "name": "Liqiang Nie"
        }
      ],
      "abstract": "Transforming complex actions into discrete skill abstractions has demonstrated strong potential for robotic manipulation. Existing approaches mainly leverage latent variable models, e.g., VQ-VAE, to learn skill abstractions through learned vectors (codebooks), while they suffer from codebook collapse and modeling the causal relationship between learned skills. To address these limitations, we present \\textbf{S}kill \\textbf{T}raining with \\textbf{A}ugmented \\textbf{R}otation (\\textbf{STAR}), a framework that advances both skill learning and composition to complete complex behaviors. Specifically, to prevent codebook collapse, we devise rotation-augmented residual skill quantization (RaRSQ). It encodes relative angles between encoder outputs into the gradient flow by rotation-based gradient mechanism. Points within the same skill code are forced to be either pushed apart or pulled closer together depending on gradient directions. Further, to capture the causal relationship between skills, we present causal skill transformer (CST) which explicitly models dependencies between skill representations through an autoregressive mechanism for coherent action generation. Extensive experiments demonstrate the superiority of STAR on both LIBERO benchmark and realworld tasks, with around 12\\% improvement over the baselines."
    },
    {
      "paperId": "c8779507ec5e8c5c6812da756005eee4b920d65d",
      "externalIds": {
        "ArXiv": "2506.03350",
        "DBLP": "journals/corr/abs-2506-03350",
        "DOI": "10.48550/arXiv.2506.03350",
        "CorpusId": 279154883
      },
      "title": "Adversarial Attacks on Robotic Vision Language Action Models",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.03350, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365448551",
          "name": "Eliot Krzysztof Jones"
        },
        {
          "authorId": "66684655",
          "name": "Alexander Robey"
        },
        {
          "authorId": "2357081377",
          "name": "Andy Zou"
        },
        {
          "authorId": "2248179910",
          "name": "Zachary Ravichandran"
        },
        {
          "authorId": "2254286806",
          "name": "George J. Pappas"
        },
        {
          "authorId": "2254218321",
          "name": "Hamed Hassani"
        },
        {
          "authorId": "2337690647",
          "name": "Matt Fredrikson"
        },
        {
          "authorId": "2242257227",
          "name": "J. Kolter"
        }
      ],
      "abstract": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs). However, LLMs are known to be susceptible to adversarial misuse, and given the significant physical risks inherent to robotics, questions remain regarding the extent to which VLAs inherit these vulnerabilities. Motivated by these concerns, in this work we initiate the study of adversarial attacks on VLA-controlled robots. Our main algorithmic contribution is the adaptation and application of LLM jailbreaking attacks to obtain complete control authority over VLAs. We find that textual attacks, which are applied once at the beginning of a rollout, facilitate full reachability of the action space of commonly used VLAs and often persist over longer horizons. This differs significantly from LLM jailbreaking literature, as attacks in the real world do not have to be semantically linked to notions of harm. We make all code available at https://github.com/eliotjones1/robogcg ."
    },
    {
      "paperId": "33c9d335353a3f0f2d23a1cad5b9b7edddd880c6",
      "externalIds": {
        "DBLP": "journals/ral/DongWZZWC25",
        "DOI": "10.1109/LRA.2025.3558648",
        "CorpusId": 277712886
      },
      "title": "Enhancing Robot Learning Through Cognitive Reasoning Trajectory Optimization Under Unknown Dynamics",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/LRA.2025.3558648?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/LRA.2025.3558648, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2053111604",
          "name": "Qingwei Dong"
        },
        {
          "authorId": "2303882475",
          "name": "Tingting Wu"
        },
        {
          "authorId": "2267044094",
          "name": "Peng Zeng"
        },
        {
          "authorId": "2806669",
          "name": "C. Zang"
        },
        {
          "authorId": "32229586",
          "name": "Guangxi Wan"
        },
        {
          "authorId": "2282586729",
          "name": "Shijie Cui"
        }
      ],
      "abstract": "In the domain of robot learning, equipping robots with the capability to swiftly acquire operational skills poses a significant challenge. Currently, reinforcement learning techniques are adept at addressing dynamic, unstructured problems involving rich contact scenarios. However, the convergence rate of these algorithms is often slow due to the high dimensionality of the robot state-action mapping space and the extensive initial policy search space. Meanwhile, advancements in large language models (LLMs) have endowed these models with a degree of logical reasoning ability, enabling them to take goal-oriented actions proactively during the initial phase of a robotic task. These models can implicitly generate features of states and uncover underlying patterns in trajectory generation. Yet, in complex manipulative tasks involving rich contact scenarios, LLMs still fall short. Thus, integrating the robust interactive capabilities of reinforcement learning with the strong logical reasoning of LLMs, and enhancing policy search with LLMs, could potentially accelerate the speed of policy searches significantly. In this letter, we introduce a Cognitive Reasoning Trajectory Optimization method. This approach utilizes Low-level Cognitive Control Tuning to enable LLMs with robust logical reasoning to make effective single-step decisions in Markov Decision Process (MDP) tasks. By fitting dynamic models with high-quality cognitive reasoning data and optimizing control strategies, this method constrains the policy search space and enhances the efficiency of trajectory optimization. Experimental results on various manipulative tasks using the Sawyer robot in the Mujoco simulator validate the effectiveness of the proposed algorithm."
    },
    {
      "paperId": "1cfcb419dc9d88563fa8dec3a141940b7f02809e",
      "externalIds": {
        "ArXiv": "2506.01167",
        "DBLP": "journals/corr/abs-2506-01167",
        "DOI": "10.48550/arXiv.2506.01167",
        "CorpusId": 279075492
      },
      "title": "Accelerated Learning with Linear Temporal Logic using Differentiable Simulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.01167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "144229259",
          "name": "A. Bozkurt"
        },
        {
          "authorId": "1730719",
          "name": "C. Belta"
        },
        {
          "authorId": "2365270590",
          "name": "Ming C. Lin"
        }
      ],
      "abstract": "To ensure learned controllers comply with safety and reliability requirements for reinforcement learning in real-world settings remains challenging. Traditional safety assurance approaches, such as state avoidance and constrained Markov decision processes, often inadequately capture trajectory requirements or may result in overly conservative behaviors. To address these limitations, recent studies advocate the use of formal specification languages such as linear temporal logic (LTL), enabling the derivation of correct-by-construction learning objectives from the specified requirements. However, the sparse rewards associated with LTL specifications make learning extremely difficult, whereas dense heuristic-based rewards risk compromising correctness. In this work, we propose the first method, to our knowledge, that integrates LTL with differentiable simulators, facilitating efficient gradient-based learning directly from LTL specifications by coupling with differentiable paradigms. Our approach introduces soft labeling to achieve differentiable rewards and states, effectively mitigating the sparse-reward issue intrinsic to LTL without compromising objective correctness. We validate the efficacy of our method through experiments, demonstrating significant improvements in both reward attainment and training time compared to the discrete methods."
    },
    {
      "paperId": "9207959dd4a608a294a578156d1398de6f267051",
      "externalIds": {
        "ArXiv": "2506.01185",
        "DBLP": "journals/corr/abs-2506-01185",
        "DOI": "10.48550/arXiv.2506.01185",
        "CorpusId": 279075306
      },
      "title": "HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and Whole-Body Control",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.01185, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "123235030",
          "name": "Priya Sundaresan"
        },
        {
          "authorId": "2364748167",
          "name": "Rhea Malhotra"
        },
        {
          "authorId": "2364749026",
          "name": "Phillip Miao"
        },
        {
          "authorId": "2261555773",
          "name": "Jingyun Yang"
        },
        {
          "authorId": "2261393585",
          "name": "Jimmy Wu"
        },
        {
          "authorId": "2265518772",
          "name": "Hengyuan Hu"
        },
        {
          "authorId": "39534622",
          "name": "Rika Antonova"
        },
        {
          "authorId": "2360691483",
          "name": "Francis Engelmann"
        },
        {
          "authorId": "1779671",
          "name": "Dorsa Sadigh"
        },
        {
          "authorId": "2300236693",
          "name": "Jeannette Bohg"
        }
      ],
      "abstract": "We introduce HoMeR, an imitation learning framework for mobile manipulation that combines whole-body control with hybrid action modes that handle both long-range and fine-grained motion, enabling effective performance on realistic in-the-wild tasks. At its core is a fast, kinematics-based whole-body controller that maps desired end-effector poses to coordinated motion across the mobile base and arm. Within this reduced end-effector action space, HoMeR learns to switch between absolute pose predictions for long-range movement and relative pose predictions for fine-grained manipulation, offloading low-level coordination to the controller and focusing learning on task-level decisions. We deploy HoMeR on a holonomic mobile manipulator with a 7-DoF arm in a real home. We compare HoMeR to baselines without hybrid actions or whole-body control across 3 simulated and 3 real household tasks such as opening cabinets, sweeping trash, and rearranging pillows. Across tasks, HoMeR achieves an overall success rate of 79.17% using just 20 demonstrations per task, outperforming the next best baseline by 29.17 on average. HoMeR is also compatible with vision-language models and can leverage their internet-scale priors to better generalize to novel object appearances, layouts, and cluttered scenes. In summary, HoMeR moves beyond tabletop settings and demonstrates a scalable path toward sample-efficient, generalizable manipulation in everyday indoor spaces. Code, videos, and supplementary material are available at: http://homer-manip.github.io"
    },
    {
      "paperId": "2dde8139c667e8f1a55ab406dac3dad85e120c7a",
      "externalIds": {
        "DOI": "10.1016/j.asoc.2025.113264",
        "CorpusId": 279233572
      },
      "title": "Optimizing stock investment strategies with Double Deep Q-Networks: Exploring the impact of oil and gold price signals",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.asoc.2025.113264?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.asoc.2025.113264, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2365999597",
          "name": "Yi-Ting Fu"
        },
        {
          "authorId": "2158103467",
          "name": "Wen-Chen Huang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "cad526a6310d6ce660daa9b110dc9ed56bb42095",
      "externalIds": {
        "DBLP": "journals/corr/abs-2505-23692",
        "ArXiv": "2505.23692",
        "DOI": "10.48550/arXiv.2505.23692",
        "CorpusId": 278996409
      },
      "title": "Mobi-π: Mobilizing Your Robot Learning Policy",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261555773",
          "name": "Jingyun Yang"
        },
        {
          "authorId": "2363988803",
          "name": "Isabella Huang"
        },
        {
          "authorId": "2262459400",
          "name": "Brandon Vu"
        },
        {
          "authorId": "2364056643",
          "name": "Max Bajracharya"
        },
        {
          "authorId": "39534622",
          "name": "Rika Antonova"
        },
        {
          "authorId": "2300236693",
          "name": "Jeannette Bohg"
        }
      ],
      "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the policy mobilization problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. Crucially, this problem formulation complements existing efforts to improve manipulation policy robustness to novel viewpoints and remains compatible with them. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes 3D Gaussian Splatting for novel view synthesis, a score function to evaluate pose suitability, and sampling-based optimization to identify optimal robot poses. We show that our approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization."
    },
    {
      "paperId": "4137f4aa17d5d513da2a1bbc10b1e9e533a3832f",
      "externalIds": {
        "ArXiv": "2505.23171",
        "DBLP": "journals/corr/abs-2505-23171",
        "DOI": "10.48550/arXiv.2505.23171",
        "CorpusId": 278996043
      },
      "title": "RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2332597209",
          "name": "Liu Liu"
        },
        {
          "authorId": "2242976725",
          "name": "Xiaofeng Wang"
        },
        {
          "authorId": "2276187860",
          "name": "Guosheng Zhao"
        },
        {
          "authorId": "2363287104",
          "name": "Keyu Li"
        },
        {
          "authorId": "2333236231",
          "name": "Wenkang Qin"
        },
        {
          "authorId": "2333060613",
          "name": "Jiaxiong Qiu"
        },
        {
          "authorId": "2109516240",
          "name": "Zheng Zhu"
        },
        {
          "authorId": "2256954306",
          "name": "Guan Huang"
        },
        {
          "authorId": "2267501979",
          "name": "Zhizhong Su"
        }
      ],
      "abstract": "Imitation Learning has become a fundamental approach in robotic manipulation. However, collecting large-scale real-world robot demonstrations is prohibitively expensive. Simulators offer a cost-effective alternative, but the sim-to-real gap make it extremely challenging to scale. Therefore, we introduce RoboTransfer, a diffusion-based video generation framework for robotic data synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry with explicit control over scene components, such as background and object attributes. By incorporating cross-view feature interactions and global depth/normal conditions, RoboTransfer ensures geometry consistency across views. This framework allows fine-grained control, including background edits and object swaps. Experiments demonstrate that RoboTransfer is capable of generating multi-view videos with enhanced geometric consistency and visual fidelity. In addition, policies trained on the data generated by RoboTransfer achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ setting and a substantial 251% relative improvement in the more challenging DIFF-ALL scenario. Explore more demos on our project page: https://horizonrobotics.github.io/robot_lab/robotransfer"
    },
    {
      "paperId": "ede508d0d1419563812434c7403d3522371711f4",
      "externalIds": {
        "ArXiv": "2505.22979",
        "DBLP": "journals/corr/abs-2505-22979",
        "DOI": "10.48550/arXiv.2505.22979",
        "CorpusId": 278996011
      },
      "title": "Learning Recommender Mechanisms for Bayesian Stochastic Games",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.22979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2142445414",
          "name": "Bengisu Guresti"
        },
        {
          "authorId": "2284132058",
          "name": "Chongjie Zhang"
        },
        {
          "authorId": "1699600",
          "name": "Yevgeniy Vorobeychik"
        }
      ],
      "abstract": "An important challenge in non-cooperative game theory is coordinating on a single (approximate) equilibrium from many possibilities - a challenge that becomes even more complex when players hold private information. Recommender mechanisms tackle this problem by recommending strategies to players based on their reported type profiles. A key consideration in such mechanisms is to ensure that players are incentivized to participate, report their private information truthfully, and follow the recommendations. While previous work has focused on designing recommender mechanisms for one-shot and extensive-form games, these approaches cannot be effectively applied to stochastic games, particularly if we constrain recommendations to be Markov stationary policies. To bridge this gap, we introduce a novel bi-level reinforcement learning approach for automatically designing recommender mechanisms in Bayesian stochastic games. Our method produces a mechanism represented by a parametric function (such as a neural network), and is therefore highly efficient at execution time. Experimental results on two repeated and two stochastic games demonstrate that our approach achieves social welfare levels competitive with cooperative multi-agent reinforcement learning baselines, while also providing significantly improved incentive properties."
    },
    {
      "paperId": "face928230ea69c69e92f3f451a014f4304f2ef1",
      "externalIds": {
        "ArXiv": "2505.23150",
        "DBLP": "journals/corr/abs-2505-23150",
        "DOI": "10.48550/arXiv.2505.23150",
        "CorpusId": 278996508
      },
      "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.23150, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344088186",
          "name": "Michal Nauman"
        },
        {
          "authorId": "2264354287",
          "name": "Marek Cygan"
        },
        {
          "authorId": "47218071",
          "name": "Carmelo Sferrazza"
        },
        {
          "authorId": "2364082294",
          "name": "Aviral Kumar"
        },
        {
          "authorId": "2253464956",
          "name": "Pieter Abbeel"
        }
      ],
      "abstract": "Recent advances in language modeling and vision stem from training large models on diverse, multi-task data. This paradigm has had limited impact in value-based reinforcement learning (RL), where improvements are often driven by small models trained in a single-task context. This is because in multi-task RL sparse rewards and gradient conflicts make optimization of temporal difference brittle. Practical workflows for generalist policies therefore avoid online training, instead cloning expert trajectories or distilling collections of single-task policies into one agent. In this work, we show that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi-task training. We test our approach on 7 multi-task benchmarks with over 280 unique tasks, spanning high degree-of-freedom humanoid control and discrete vision-based RL. We find that, despite its simplicity, the proposed approach leads to state-of-the-art single and multi-task performance, as well as sample-efficient transfer to new tasks."
    },
    {
      "paperId": "1ecb4fb3ab37d581da0d3776b0f6bae9d53d18c7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2506-03173",
        "ArXiv": "2506.03173",
        "DOI": "10.48550/arXiv.2506.03173",
        "CorpusId": 279155142
      },
      "title": "FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.03173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2331378967",
          "name": "Xiaoyi Liu"
        },
        {
          "authorId": "2331271995",
          "name": "Hao Tang"
        }
      ],
      "abstract": "Physical intelligence -- anticipating and shaping the world from partial, multisensory observations -- is critical for next-generation world models. We propose FOLIAGE, a physics-informed multimodal world model for unbounded accretive surface growth. In its Action-Perception loop, a unified context encoder maps images, mesh connectivity, and point clouds to a shared latent state. A physics-aware predictor, conditioned on physical control actions, advances this latent state in time to align with the target latent of the surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network (AGN) captures dynamic connectivity through Age Positional Encoding and Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances global context with local dynamics. We create SURF-GARDEN, a world model learning platform comprising a Counterfactual Physics Simulator, a Multimodal Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation suite, evaluates six core tasks -- topology recognition, inverse material estimation, growth-stage classification, latent roll-out, cross-modal retrieval, and dense correspondence -- and four stress tests -- sensor dropout, zero-shot modality transfer, long-horizon prediction, and physics ablation -- to probe resilience. FOLIAGE outperforms specialized baselines while remaining robust across dynamic environments, establishing a new world-model based, multimodal pathway to physical intelligence."
    },
    {
      "paperId": "2079f9ca26f9811cb5da8a8967eefae576ce55f9",
      "externalIds": {
        "ArXiv": "2505.21852",
        "DBLP": "journals/corr/abs-2505-21852",
        "DOI": "10.48550/arXiv.2505.21852",
        "CorpusId": 278959737
      },
      "title": "A Provable Approach for End-to-End Safe Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.21852, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "41036308",
          "name": "Akifumi Wachi"
        },
        {
          "authorId": "2344747358",
          "name": "Kohei Miyaguchi"
        },
        {
          "authorId": "2074114653",
          "name": "Takumi Tanabe"
        },
        {
          "authorId": "2296992379",
          "name": "Rei Sato"
        },
        {
          "authorId": "2258718252",
          "name": "Youhei Akimoto"
        }
      ],
      "abstract": "A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation."
    },
    {
      "paperId": "ab5acad2b6ff5ab93c0fc35b95a7a203ad28caad",
      "externalIds": {
        "ArXiv": "2505.19785",
        "DBLP": "journals/corr/abs-2505-19785",
        "DOI": "10.48550/arXiv.2505.19785",
        "CorpusId": 278904868
      },
      "title": "MedDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.19785, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2318264230",
          "name": "Qianyi Xu"
        },
        {
          "authorId": "2016381637",
          "name": "Gousia Habib"
        },
        {
          "authorId": "27035609",
          "name": "Dilruk Perera"
        },
        {
          "authorId": "2270027408",
          "name": "Mengling Feng"
        }
      ],
      "abstract": "Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses vary significantly and evolve over time. Clinical data used to support these decisions are often irregularly sampled, sparse, and noisy. Existing decision support systems commonly rely on discretization and imputation, which can distort critical temporal dynamics and degrade decision quality. Moreover, they often overlook the clinical significance of irregular recording frequencies, filtering out patterns in how and when data is collected. Reinforcement Learning (RL) is a natural fit for clinical decision-making, enabling sequential, long-term optimization in dynamic, uncertain environments. However, most existing treatment recommendation systems are model-free and trained solely on offline data, making them sample-inefficient, sensitive to data quality, and poorly generalizable across tasks or cohorts. To address these limitations, we propose MedDreamer, a two-phase model-based RL framework for personalized treatment recommendation. MedDreamer uses a world model with an Adaptive Feature Integration (AFI) module to effectively model irregular, sparse clinical data. Through latent imagination, it simulates plausible patient trajectories to enhance learning, refining its policy using a mix of real and imagined experiences. This enables learning policies that go beyond suboptimal historical decisions while remaining grounded in clinical data. To our knowledge, this is the first application of latent imagination to irregular healthcare data. Evaluations on sepsis and mechanical ventilation (MV) treatment using two large-scale EHR datasets show that MedDreamer outperforms both model-free and model-based baselines in clinical outcomes and off-policy metrics."
    },
    {
      "paperId": "9238292ce32dc0630878c0096e74a50ee05d3f02",
      "externalIds": {
        "ArXiv": "2505.20503",
        "DBLP": "journals/corr/abs-2505-20503",
        "DOI": "10.48550/arXiv.2505.20503",
        "CorpusId": 278910819
      },
      "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.20503, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2291963080",
          "name": "Matthew Lisondra"
        },
        {
          "authorId": "1719617",
          "name": "B. Benhabib"
        },
        {
          "authorId": "2497882",
          "name": "G. Nejat"
        }
      ],
      "abstract": "Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems."
    },
    {
      "paperId": "10c7e31b4a87e0fb1106ffcb0016fee833e1c929",
      "externalIds": {
        "ArXiv": "2505.18487",
        "DBLP": "journals/corr/abs-2505-18487",
        "DOI": "10.48550/arXiv.2505.18487",
        "CorpusId": 278905533
      },
      "title": "Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2363490385",
          "name": "Junlin Wang"
        },
        {
          "authorId": "2363942835",
          "name": "Zhiyun Lin"
        }
      ],
      "abstract": "Learning effective visual representations for robotic manipulation remains a fundamental challenge due to the complex body dynamics involved in action execution. In this paper, we study how visual representations that carry body-relevant cues can enable efficient policy learning for downstream robotic manipulation tasks. We present $\\textbf{I}$nter-token $\\textbf{Con}$trast ($\\textbf{ICon}$), a contrastive learning method applied to the token-level representations of Vision Transformers (ViTs). ICon enforces a separation in the feature space between agent-specific and environment-specific tokens, resulting in agent-centric visual representations that embed body-specific inductive biases. This framework can be seamlessly integrated into end-to-end policy learning by incorporating the contrastive loss as an auxiliary objective. Our experiments show that ICon not only improves policy performance across various manipulation tasks but also facilitates policy transfer across different robots. The project website: https://github.com/HenryWJL/icon"
    },
    {
      "paperId": "34d4d99f89edb2cc3b55bf439c06629ae970bf1f",
      "externalIds": {
        "PubMedCentral": "12101691",
        "DOI": "10.1371/journal.pone.0318372",
        "CorpusId": 278859533,
        "PubMed": "40408327"
      },
      "title": "Autonomous agents: Augmenting visual information with raw audio data",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12101691, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2205469512",
          "name": "Enoch Solomon"
        }
      ],
      "abstract": "In the realm of game playing, deep reinforcement learning predominantly relies on visual input to map states to actions. The visual data extracted from the game environment serves as the primary foundation for state representation in reinforcement learning agents. However, humans leverage additional sensory inputs, such as audio cues, which play a pivotal role in perception and decision-making. Therefore, incorporating raw audio along with visual information shows potential for offering valuable insights to reinforcement learning agents. This study advocates for the integration of raw audio samples as complementary information to visual data in state representation. By using raw audio with visual cues, our objective is to enrich the decision-making process of the agent at each stage. Experimental evaluation were conducted employing Deep Q Networks (DQN) and Proximal Policy Optimization (PPO) algorithms within ViZDoom and Unity reinforcement learning environments. The results of our experiments reveal that augmenting visual information with raw audio samples yields superior rewards and expedites the learning rate compared to relying solely on visual data. Additionally, the findings suggest that considering both visual and audio features enhances the agent’s behavior, a trend observed across Unity and ViZDoom environments. This study underscores the potential advantages of incorporating multisensory information, particularly raw audio, into the state representation of reinforcement learning agents. Such insights contribute to advancing our understanding of how agents perceive and engage with their environments, ultimately enhancing performance in complex gaming scenarios."
    },
    {
      "paperId": "cf4745bc59333037511ff3bbec096be5b992dcc0",
      "externalIds": {
        "DOI": "10.1109/ICAISISAS64483.2025.11051633",
        "CorpusId": 279899250
      },
      "title": "Intelligent Satellite Control for Multi-Target Staring Imaging",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICAISISAS64483.2025.11051633?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICAISISAS64483.2025.11051633, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2370178713",
          "name": "Di Zhang"
        },
        {
          "authorId": "2343550642",
          "name": "Jun Zhou"
        },
        {
          "authorId": "2281795979",
          "name": "Jianguo Guo"
        },
        {
          "authorId": "2278979508",
          "name": "Jiaolong Zhang"
        },
        {
          "authorId": "2370570089",
          "name": "Peng Li"
        },
        {
          "authorId": "2274618149",
          "name": "Tong Liu"
        }
      ],
      "abstract": "Current CMOS chip technology is becoming increasingly advanced, and more remote sensing small satellites are utilizing planar array CMOS cameras to achieve staring imaging of multiple targets. The selection of imaging paths and the accuracy of imaging time control for satellite staring imaging of numerous targets are influenced by the satellite's relative position to the observation targets and its maneuvering capabilities. In this paper, we establish a satellite attitude dynamics model and design an adaptive controller for target staring control. We also present a path selection strategy for multi-target planning, which incorporates the constraints of satellite imaging. Additionally, we propose an intelligent control approach based on deep reinforcement learning to optimize control parameters and imaging paths, enhancing multi-target staring imaging time and stability for optimal benefits. Our solution offers an intelligent method for practical engineering applications involving multi-target staring imaging. Finally, the mathematical simulation results show that this method can improve control stability and best imagine time."
    },
    {
      "paperId": "8f4dfb74b11d2c892584164189c47836b46dcde0",
      "externalIds": {
        "ArXiv": "2505.16969",
        "DBLP": "journals/corr/abs-2505-16969",
        "DOI": "10.48550/arXiv.2505.16969",
        "CorpusId": 278788892
      },
      "title": "3D Equivariant Visuomotor Policy Learning via Spherical Projection",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.16969, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312110011",
          "name": "Boce Hu"
        },
        {
          "authorId": "2119264352",
          "name": "Dian Wang"
        },
        {
          "authorId": "146507635",
          "name": "David Klee"
        },
        {
          "authorId": "2356238198",
          "name": "Heng Tian"
        },
        {
          "authorId": "2145238065",
          "name": "Xu Zhu"
        },
        {
          "authorId": "2143569284",
          "name": "Hao-zhe Huang"
        },
        {
          "authorId": "2280136750",
          "name": "Robert Platt"
        },
        {
          "authorId": "2287354248",
          "name": "Robin Walters"
        }
      ],
      "abstract": "Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in SO(3) without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work is the first SO(3)-equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs."
    },
    {
      "paperId": "8574585dd51db51a7a283afb655cc7fb5e790a32",
      "externalIds": {
        "ArXiv": "2505.15143",
        "DBLP": "journals/corr/abs-2505-15143",
        "DOI": "10.48550/arXiv.2505.15143",
        "CorpusId": 278782794
      },
      "title": "Filtering Learning Histories Enhances In-Context Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.15143, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2109780068",
          "name": "Weiqin Chen"
        },
        {
          "authorId": "2362638381",
          "name": "Xinjie Zhang"
        },
        {
          "authorId": "1762215",
          "name": "D. Subramanian"
        },
        {
          "authorId": "2346980344",
          "name": "Santiago Paternain"
        }
      ],
      "abstract": "Transformer models (TMs) have exhibited remarkable in-context reinforcement learning (ICRL) capabilities, allowing them to generalize to and improve in previously unseen environments without re-training or fine-tuning. This is typically accomplished by imitating the complete learning histories of a source RL algorithm over a substantial amount of pretraining environments, which, however, may transfer suboptimal behaviors inherited from the source algorithm/dataset. Therefore, in this work, we address the issue of inheriting suboptimality from the perspective of dataset preprocessing. Motivated by the success of the weighted empirical risk minimization, we propose a simple yet effective approach, learning history filtering (LHF), to enhance ICRL by reweighting and filtering the learning histories based on their improvement and stability characteristics. To the best of our knowledge, LHF is the first approach to avoid source suboptimality by dataset preprocessing, and can be combined with the current state-of-the-art (SOTA) ICRL algorithms. We substantiate the effectiveness of LHF through a series of experiments conducted on the well-known ICRL benchmarks, encompassing both discrete environments and continuous robotic manipulation tasks, with three SOTA ICRL algorithms (AD, DPT, DICP) as the backbones. LHF exhibits robust performance across a variety of suboptimal scenarios, as well as under varying hyperparameters and sampling strategies. Notably, the superior performance of LHF becomes more pronounced in the presence of noisy data, indicating the significance of filtering learning histories."
    },
    {
      "paperId": "a74923df8e67ee27f1dc220a910dad2d04cc0267",
      "externalIds": {
        "ArXiv": "2505.14526",
        "DBLP": "journals/corr/abs-2505-14526",
        "DOI": "10.48550/arXiv.2505.14526",
        "CorpusId": 278768523
      },
      "title": "NavBench: A Unified Robotics Benchmark for Reinforcement Learning-Based Autonomous Navigation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.14526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2219976290",
          "name": "Matteo El Hariry"
        },
        {
          "authorId": "50589695",
          "name": "Antoine Richard"
        },
        {
          "authorId": "2362503919",
          "name": "Ricard M. Castan"
        },
        {
          "authorId": "2310700886",
          "name": "Luis F. W. Batista"
        },
        {
          "authorId": "2269464568",
          "name": "Matthieu Geist"
        },
        {
          "authorId": "2266562418",
          "name": "Cédric Pradalier"
        },
        {
          "authorId": "2241386003",
          "name": "M. Olivares-Mendez"
        }
      ],
      "abstract": "Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present NavBench, a multi-domain benchmark for training and evaluating RL-based navigation policies across diverse robotic platforms and operational environments. Built on IsaacLab, our framework standardizes task definitions, enabling different robots to tackle various navigation challenges without the need for ad-hoc task redesigns or custom evaluation metrics. Our benchmark addresses three key challenges: (1) Unified cross-medium benchmarking, enabling direct evaluation of diverse actuation methods (thrusters, wheels, water-based propulsion) in realistic environments; (2) Scalable and modular design, facilitating seamless robot-task interchangeability and reproducible training pipelines; and (3) Robust sim-to-real validation, demonstrated through successful policy transfer to multiple real-world robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle. By ensuring consistency between simulation and real-world deployment, NavBench simplifies the development of adaptable RL-based navigation strategies. Its modular design allows researchers to easily integrate custom robots and tasks by following the framework's predefined templates, making it accessible for a wide range of applications. Our code is publicly available at NavBench."
    },
    {
      "paperId": "38357218f59b1344bd431cdbb605e5e208b5b96f",
      "externalIds": {
        "ArXiv": "2505.13982",
        "DBLP": "journals/corr/abs-2505-13982",
        "DOI": "10.48550/arXiv.2505.13982",
        "CorpusId": 278769069
      },
      "title": "Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.13982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2322990907",
          "name": "Jinzhou Li"
        },
        {
          "authorId": "2239159032",
          "name": "Tianhao Wu"
        },
        {
          "authorId": "2239160954",
          "name": "Jiyao Zhang"
        },
        {
          "authorId": "2362661308",
          "name": "Zeyuan Chen"
        },
        {
          "authorId": "2362452295",
          "name": "Haotian Jin"
        },
        {
          "authorId": "2238948679",
          "name": "Mingdong Wu"
        },
        {
          "authorId": "2362572333",
          "name": "Yujun Shen"
        },
        {
          "authorId": "2292209373",
          "name": "Yaodong Yang"
        },
        {
          "authorId": "2347878145",
          "name": "Hao Dong"
        }
      ],
      "abstract": "Effectively utilizing multi-sensory data is important for robots to generalize across diverse tasks. However, the heterogeneous nature of these modalities makes fusion challenging. Existing methods propose strategies to obtain comprehensively fused features but often ignore the fact that each modality requires different levels of attention at different manipulation stages. To address this, we propose a force-guided attention fusion module that adaptively adjusts the weights of visual and tactile features without human labeling. We also introduce a self-supervised future force prediction auxiliary task to reinforce the tactile modality, improve data imbalance, and encourage proper adjustment. Our method achieves an average success rate of 93% across three fine-grained, contactrich tasks in real-world experiments. Further analysis shows that our policy appropriately adjusts attention to each modality at different manipulation stages. The videos can be viewed at https://adaptac-dex.github.io/."
    },
    {
      "paperId": "5c948dc907dd379db1b060d5f13245511f62be2d",
      "externalIds": {
        "DOI": "10.1109/ICCC65605.2025.11022859",
        "CorpusId": 279268893
      },
      "title": "A Novel Graph Neural Network Approach for Inverse Kinematics in Robotic Arms",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCC65605.2025.11022859?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCC65605.2025.11022859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2223956848",
          "name": "Ali Jlidi"
        },
        {
          "authorId": "102568025",
          "name": "Rabab Benotsmane"
        },
        {
          "authorId": "2366235506",
          "name": "Kovács László"
        },
        {
          "authorId": "48318536",
          "name": "Attila Trohák"
        }
      ],
      "abstract": "The accurate prediction of kinematic configurations, including joint limit violations, collisions, and trajectory anomalies, is essential for ensuring the safety and efficiency of 6-DOF robotic arms in industrial applications. In this study, we develop a novel inverse kinematics (IK) solver based on a data-driven approach utilizing Graph Neural Networks (GNNs). Our model effectively captures the complex spatial relationships governing kinematic behavior by representing the robotic system as a graph—where nodes correspond to joints and edges represent physical linkages. Trained on a dataset of direct kinematics, our GNN-based model infers joint angles given an end-effector position with high accuracy and efficiency. The proposed approach achieves an accuracy of 92% with a 100.0% success rate and a computational runtime of 2.4 ms, outperforming conventional numerical and optimization-based IK solvers. These results highlight the potential of GNNs in real-time inverse kinematics prediction, enabling improved motion planning, reduced computational costs, and enhanced adaptability in dynamic environments. This research paves the way for more efficient and scalable solutions in industrial automation, human-robot collaboration, and autonomous robotic systems."
    },
    {
      "paperId": "4fc209fe0259254f3734f2e63213d25d6fb1bc55",
      "externalIds": {
        "DOI": "10.32473/flairs.38.1.138986",
        "CorpusId": 278683838
      },
      "title": "Curvature-Adaptive Learning Rate Optimizer: Theoretical Insights and Empirical Evaluation on Neural Network Training",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.32473/flairs.38.1.138986?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.32473/flairs.38.1.138986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "51936405",
          "name": "K. D. G. Maduranga"
        }
      ],
      "abstract": "Optimizing neural networks often encounters challenges such as saddle points, plateaus, and ill-conditioned curvature, limiting the effectiveness of standard optimizers like Adam, Nadam, and RMSProp. To address these limitations, we propose the Curvature-Adaptive Learning Rate (CALR) optimizer, a novel method that leverages local curvature estimates to dynamically adjust learning rates. CALR, along with its variants incorporating gradient clipping and cosine annealing schedules, offers enhanced robustness and faster convergence across diverse optimization tasks. Theoretical analysis confirms CALR’s convergence properties, while empirical evaluations on benchmark functions—Rosenbrock, Himmelblau, and Saddle Point—highlight its efficiency in complex optimization landscapes. Furthermore, CALR demonstrates superior performance on neural network training tasks using MNIST and CIFAR-10 datasets, achieving faster convergence, lower loss, and better generalization compared to traditional optimizers. These results establish CALR as a promising optimization strategy for challenging neural network training problems."
    },
    {
      "paperId": "1d97ac155a9788ca1f7bae0c83a8496df5830e76",
      "externalIds": {
        "DBLP": "journals/corr/abs-2505-08949",
        "ArXiv": "2505.08949",
        "DOI": "10.48550/arXiv.2505.08949",
        "CorpusId": 278602383
      },
      "title": "Multi-step manipulation task and motion planning guided by video demonstration",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.08949, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2139654485",
          "name": "Kateryna Zorina"
        },
        {
          "authorId": "2221123075",
          "name": "David Kovář"
        },
        {
          "authorId": "151428061",
          "name": "Médéric Fourmy"
        },
        {
          "authorId": "2333648838",
          "name": "Florent Lamiraux"
        },
        {
          "authorId": "2265755156",
          "name": "Nicolas Mansard"
        },
        {
          "authorId": "2332561775",
          "name": "Justin Carpentier"
        },
        {
          "authorId": "2265755786",
          "name": "Josef Sivic"
        },
        {
          "authorId": "2265755285",
          "name": "Vladimír Petrík"
        }
      ],
      "abstract": "This work aims to leverage instructional video to solve complex multi-step task-and-motion planning tasks in robotics. Towards this goal, we propose an extension of the well-established Rapidly-Exploring Random Tree (RRT) planner, which simultaneously grows multiple trees around grasp and release states extracted from the guiding video. Our key novelty lies in combining contact states and 3D object poses extracted from the guiding video with a traditional planning algorithm that allows us to solve tasks with sequential dependencies, for example, if an object needs to be placed at a specific location to be grasped later. We also investigate the generalization capabilities of our approach to go beyond the scene depicted in the instructional video. To demonstrate the benefits of the proposed video-guided planning approach, we design a new benchmark with three challenging tasks: (I) 3D re-arrangement of multiple objects between a table and a shelf, (ii) multi-step transfer of an object through a tunnel, and (iii) transferring objects using a tray similar to a waiter transfers dishes. We demonstrate the effectiveness of our planning algorithm on several robots, including the Franka Emika Panda and the KUKA KMR iiwa. For a seamless transfer of the obtained plans to the real robot, we develop a trajectory refinement approach formulated as an optimal control problem (OCP)."
    },
    {
      "paperId": "af8b1ef04f8ac33576b7794aa7075b7726a56aa0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2505-07637",
        "ArXiv": "2505.07637",
        "DOI": "10.48550/arXiv.2505.07637",
        "CorpusId": 278535103
      },
      "title": "Chronocept: Instilling a Sense of Time in Machines",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2360666086",
          "name": "Krish Goel"
        },
        {
          "authorId": "2361903732",
          "name": "Sanskar Pandey"
        },
        {
          "authorId": "2360701317",
          "name": "KS Mahadevan"
        },
        {
          "authorId": "2360704582",
          "name": "Harsh Kumar"
        },
        {
          "authorId": "2360692809",
          "name": "Vishesh Khadaria"
        }
      ],
      "abstract": "Human cognition is deeply intertwined with a sense of time, known as Chronoception. This sense allows us to judge how long facts remain valid and when knowledge becomes outdated. Despite progress in vision, language, and motor control, AI still struggles to reason about temporal validity. We introduce Chronocept, the first benchmark to model temporal validity as a continuous probability distribution over time. Using skew-normal curves fitted along semantically decomposed temporal axes, Chronocept captures nuanced patterns of emergence, decay, and peak relevance. It includes two datasets: Benchmark I (atomic facts) and Benchmark II (multi-sentence passages). Annotations show strong inter-annotator agreement (84% and 89%). Our baselines predict curve parameters - location, scale, and skewness - enabling interpretable, generalizable learning and outperforming classification-based approaches. Chronocept fills a foundational gap in AI's temporal reasoning, supporting applications in knowledge grounding, fact-checking, retrieval-augmented generation (RAG), and proactive agents. Code and data are publicly available."
    },
    {
      "paperId": "1e8333cf85e6052f1718f27760d2d9d3e411acc9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2505-07096",
        "ArXiv": "2505.07096",
        "DOI": "10.48550/arXiv.2505.07096",
        "CorpusId": 278501460
      },
      "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.07096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2228665381",
          "name": "Prithwish Dan"
        },
        {
          "authorId": "34904511",
          "name": "K. Kedia"
        },
        {
          "authorId": "2288204357",
          "name": "Angela Chao"
        },
        {
          "authorId": "2360358263",
          "name": "Edward Weiyi Duan"
        },
        {
          "authorId": "2288255136",
          "name": "M. Pace"
        },
        {
          "authorId": "2360361973",
          "name": "Wei-Chiu Ma"
        },
        {
          "authorId": "2266752840",
          "name": "Sanjiban Choudhury"
        }
      ],
      "abstract": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/."
    },
    {
      "paperId": "e4c11b4f6eaaa079856d81f39be8924e5bd1ba40",
      "externalIds": {
        "ArXiv": "2505.04619",
        "DBLP": "journals/corr/abs-2505-04619",
        "DOI": "10.48550/arXiv.2505.04619",
        "CorpusId": 278368130
      },
      "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.04619, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2187682402",
          "name": "Abdulaziz Almuzairee"
        },
        {
          "authorId": "2359451095",
          "name": "Rohan Patil"
        },
        {
          "authorId": "2186560150",
          "name": "Dwait Bhatt"
        },
        {
          "authorId": "2359450018",
          "name": "Henrik I. Christensen"
        }
      ],
      "abstract": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"
    },
    {
      "paperId": "dc67c56d146d74d26542bddc77c21601820a4f5f",
      "externalIds": {
        "ArXiv": "2505.02288",
        "DBLP": "journals/corr/abs-2505-02288",
        "DOI": "10.48550/arXiv.2505.02288",
        "CorpusId": 278327226
      },
      "title": "Universal Approximation Theorem of Deep Q-Networks",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02288, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2359147091",
          "name": "Qian Qi"
        }
      ],
      "abstract": "We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data."
    },
    {
      "paperId": "2174f1e6274bbe417ce5c5a2526699b0bcd8ec13",
      "externalIds": {
        "ArXiv": "2505.02232",
        "DBLP": "journals/corr/abs-2505-02232",
        "DOI": "10.48550/arXiv.2505.02232",
        "CorpusId": 278327501
      },
      "title": "Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2114858131",
          "name": "Malte Mosbach"
        },
        {
          "authorId": "2291962765",
          "name": "Sven Behnke"
        }
      ],
      "abstract": "Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io"
    },
    {
      "paperId": "54d86626380842f519514b2e8f9f74a3ef084c0c",
      "externalIds": {
        "DBLP": "journals/eaai/ZhangSYL25",
        "DOI": "10.1016/j.engappai.2025.110373",
        "CorpusId": 276939054
      },
      "title": "A plug-and-play fully on-the-job real-time reinforcement learning algorithm for a direct-drive tandem-wing experiment platforms under multiple random operating conditions",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.engappai.2025.110373?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.engappai.2025.110373, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2302812857",
          "name": "Minghao Zhang"
        },
        {
          "authorId": "2172390504",
          "name": "Bifeng Song"
        },
        {
          "authorId": "2345858580",
          "name": "Xiaojun Yang"
        },
        {
          "authorId": "2310217874",
          "name": "Wang Liang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "99079bf0bb1ec840558c8124dafb3ba5ea871bb9",
      "externalIds": {
        "DOI": "10.1016/j.paerosci.2025.101100",
        "CorpusId": 278920394
      },
      "title": "The state of hybrid artificial intelligence for interstellar missions",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.paerosci.2025.101100?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.paerosci.2025.101100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2363627608",
          "name": "Alex Ellery"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6c4d6224244557e0ad7ffe423390d388b6a1e736",
      "externalIds": {
        "PubMedCentral": "12075287",
        "DOI": "10.3389/fnbot.2025.1585386",
        "CorpusId": 278252520,
        "PubMed": "40370636"
      },
      "title": "FOCUS: object-centric world models for robotic manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12075287, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "97900088",
          "name": "Stefano Ferraro"
        },
        {
          "authorId": "2098445287",
          "name": "Pietro Mazzaglia"
        },
        {
          "authorId": "2413244",
          "name": "Tim Verbelen"
        },
        {
          "authorId": "2318635981",
          "name": "Bart Dhoedt"
        }
      ],
      "abstract": "Understanding the world in terms of objects and the possible interactions with them is an important cognitive ability. However, current world models adopted in reinforcement learning typically lack this structure and represent the world state in a global latent vector. To address this, we propose FOCUS, a model-based agent that learns an object-centric world model. This novel representation also enables the design of an object-centric exploration mechanism, which encourages the agent to interact with objects and discover useful interactions. We benchmark FOCUS in several robotic manipulation settings, where we found that our method can be used to improve manipulation skills. The object-centric world model leads to more accurate predictions of the objects in the scene and it enables more efficient learning. The object-centric exploration strategy fosters interactions with the objects in the environment, such as reaching, moving, and rotating them, and it allows fast adaptation of the agent to sparse reward reinforcement learning tasks. Using a Franka Emika robot arm, we also showcase how FOCUS proves useful in real-world applications. Website: focus-manipulation.github.io."
    },
    {
      "paperId": "110e0292fdf62799f7c088d1b58b0d5118133403",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-19498",
        "ArXiv": "2504.19498",
        "DOI": "10.48550/arXiv.2504.19498",
        "CorpusId": 278165033
      },
      "title": "Motion Generation for Food Topping Challenge 2024: Serving Salmon Roe Bowl and Picking Fried Chicken",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.19498, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2282136121",
          "name": "Koki Inami"
        },
        {
          "authorId": "2331416718",
          "name": "Masashi Konosu"
        },
        {
          "authorId": "2218089897",
          "name": "Koki Yamane"
        },
        {
          "authorId": "2281105270",
          "name": "Nozomu Masuya"
        },
        {
          "authorId": "2357978731",
          "name": "Yunhan Li"
        },
        {
          "authorId": "2357960827",
          "name": "Yu-Han Shu"
        },
        {
          "authorId": "2331579147",
          "name": "Hiroshi Sato"
        },
        {
          "authorId": "2357970580",
          "name": "Shinnosuke Homma"
        },
        {
          "authorId": "2123512",
          "name": "S. Sakaino"
        }
      ],
      "abstract": "Although robots have been introduced in many industries, food production robots are yet to be widely employed because the food industry requires not only delicate movements to handle food but also complex movements that adapt to the environment. Force control is important for handling delicate objects such as food. In addition, achieving complex movements is possible by making robot motions based on human teachings. Four-channel bilateral control is proposed, which enables the simultaneous teaching of position and force information. Moreover, methods have been developed to reproduce motions obtained through human teachings and generate adaptive motions using learning. We demonstrated the effectiveness of these methods for food handling tasks in the Food Topping Challenge at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024). For the task of serving salmon roe on rice, we achieved the best performance because of the high reproducibility and quick motion of the proposed method. Further, for the task of picking fried chicken, we successfully picked the most pieces of fried chicken among all participating teams. This paper describes the implementation and performance of these methods."
    },
    {
      "paperId": "d5e5334f7a22015efaab7ec5aa802a84c229dcce",
      "externalIds": {
        "DOI": "10.1109/ICKECS65700.2025.11034908",
        "CorpusId": 279595598
      },
      "title": "Deep Reinforcement Learning-Based Vehicle Simulation for Handling Variable-Dimension Observations",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICKECS65700.2025.11034908?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICKECS65700.2025.11034908, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2368451692",
          "name": "M. Shunmuga Sundari"
        },
        {
          "authorId": "2346547529",
          "name": "Girish Jadhav"
        },
        {
          "authorId": "2368450099",
          "name": "K. B. Ramkumar"
        },
        {
          "authorId": "2368449986",
          "name": "Ashok Pati"
        }
      ],
      "abstract": "This work introduces a deep reinforcement learning (DRL)-based framework designed to simulate vehicle control in environments with variable-dimension observations. Autonomous driving systems often encounter challenges in processing dynamic and irregular input dimensions due to sensor variations or environmental changes. The proposed method addresses these challenges by incorporating an adaptive observation encoder, which processes variable-dimension inputs into a fixed-length representation, enabling effective integration into policy networks. A dynamic policy network, optimized using Proximal Policy Optimization (PPO), ensures robust decision-making by adapting to changes in input dimensions and environmental conditions. The framework is further enhanced by a multi-objective reward shaping mechanism that prioritizes safety, efficiency, and passenger comfort. The methodology is evaluated using a simulated urban driving environment, demonstrating superior performance in terms of collision avoidance, fuel efficiency, and policy stability compared to traditional reinforcement learning approaches. Results indicate a significant improvement in handling variable observations, with a 35% reduction in collision rates and a 20% enhancement in efficiency metrics. This study highlights the potential of DRL in addressing critical challenges associated with dynamic observations, paving the way for more reliable autonomous vehicle systems. Future directions include extending the framework to multi-agent scenarios and real-world applications to further validate its efficacy and scalability."
    },
    {
      "paperId": "4d8f85dfe7dbbf040a6d5a71c125ab1eb75c1f82",
      "externalIds": {
        "DOI": "10.3390/robotics14050057",
        "CorpusId": 278261635
      },
      "title": "Quadruped Robots: Bridging Mechanical Design, Control, and Applications",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/robotics14050057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/robotics14050057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "47422507",
          "name": "Qimeng Li"
        },
        {
          "authorId": "2294585436",
          "name": "Franco Cicirelli"
        },
        {
          "authorId": "2221489692",
          "name": "Andrea Vinci"
        },
        {
          "authorId": "143627888",
          "name": "Antonio Guerrieri"
        },
        {
          "authorId": "2287339398",
          "name": "Wen Qi"
        },
        {
          "authorId": "2154575899",
          "name": "G. Fortino"
        }
      ],
      "abstract": "Quadruped robots have emerged as a prominent field of research due to their exceptional mobility and adaptability in complex terrains. This paper presents an overview of quadruped robots, encompassing their design principles, control mechanisms, perception systems, and applications across various industries. We review the historical evolution and technological milestones that have shaped quadruped robotics. To understand their impact on performance and functionality, key aspects of mechanical design are analyzed, including leg configurations, actuation systems, and material selection. Control strategies for locomotion, balance, and navigation are all examined, highlighting the integration of artificial intelligence and machine learning to enhance adaptability and autonomy. This review also explores perception and sensing technologies that enable environmental interaction and decision-making capabilities. Furthermore, we systematically examine the diverse applications of quadruped robots in sectors including the military, search and rescue, industrial inspection, agriculture, and entertainment. Finally, we address challenges and limitations, including technical hurdles, ethical considerations, and regulatory issues, and propose future research directions to advance the field. By structuring this review as a systematic study, we ensure clarity and a comprehensive understanding of the domain, making it a valuable resource for researchers and engineers in quadruped robotics."
    },
    {
      "paperId": "89d8b82548cde91cb5801afaafeb6031901c8347",
      "externalIds": {
        "ArXiv": "2504.18253",
        "DBLP": "journals/corr/abs-2504-18253",
        "DOI": "10.48550/arXiv.2504.18253",
        "CorpusId": 278129768
      },
      "title": "Depth-Constrained ASV Navigation with Deep RL and Limited Sensing",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18253, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2357720447",
          "name": "Amirhossein Zhalehmehrabi"
        },
        {
          "authorId": "2243453778",
          "name": "Daniele Meli"
        },
        {
          "authorId": "2320669063",
          "name": "Francesco Dal Santo"
        },
        {
          "authorId": "2148531629",
          "name": "Francesco Trotti"
        },
        {
          "authorId": "2308401982",
          "name": "Alessandro Farinelli"
        }
      ],
      "abstract": "Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments."
    },
    {
      "paperId": "254ff3be571f60be969668c66c531be4bab9e026",
      "externalIds": {
        "ArXiv": "2504.18328",
        "DBLP": "journals/corr/abs-2504-18328",
        "DOI": "10.1109/TIV.2024.3496797",
        "CorpusId": 274107682
      },
      "title": "AI Safety Assurance for Automated Vehicles: A Survey on Research, Standardization, Regulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18328, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279919299",
          "name": "Lars Ullrich"
        },
        {
          "authorId": "2306780920",
          "name": "Michael Buchholz"
        },
        {
          "authorId": "2253395293",
          "name": "Klaus C. J. Dietmayer"
        },
        {
          "authorId": "2248576886",
          "name": "Knut Graichen"
        }
      ],
      "abstract": "Assuring safety of artificial intelligence (AI) applied to safety-critical systems is of paramount importance. Especially since research in the field of automated driving shows that AI is able to outperform classical approaches, to handle higher complexities, and to reach new levels of autonomy. At the same time, the safety assurance required for the use of AI in such safety-critical systems is still not in place. Due to the dynamic and far-reaching nature of the technology, research on safeguarding AI is being conducted in parallel to AI standardization and regulation. The parallel progress necessitates simultaneous consideration in order to carry out targeted research and development of AI systems in the context of automated driving. Therefore, in contrast to existing surveys that focus primarily on research aspects, this paper considers research, standardization and regulation in a concise way. Accordingly, the survey takes into account the interdependencies arising from the triplet of research, standardization and regulation in a forward-looking perspective and anticipates and discusses open questions and possible future directions. In this way, the survey ultimately serves to provide researchers and safety experts with a compact, holistic perspective that discusses the current status, emerging trends, and possible future developments."
    },
    {
      "paperId": "da85d7361b597b67eb856663320da708f074ea20",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-17493",
        "ArXiv": "2504.17493",
        "DOI": "10.48550/arXiv.2504.17493",
        "CorpusId": 278033556
      },
      "title": "Goal-Oriented Time-Series Forecasting: Foundation Framework Design",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.17493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2357082968",
          "name": "Luca-Andrei Fechete"
        },
        {
          "authorId": "2357084161",
          "name": "Mohamed Sana"
        },
        {
          "authorId": "70486867",
          "name": "Fadhel Ayed"
        },
        {
          "authorId": "3393923",
          "name": "Nicola Piovesan"
        },
        {
          "authorId": "2357972172",
          "name": "Wenjie Li"
        },
        {
          "authorId": "2261362548",
          "name": "Antonio De Domenico"
        },
        {
          "authorId": "2047485098",
          "name": "T. Si Salem"
        }
      ],
      "abstract": "Traditional time-series forecasting often focuses only on minimizing prediction errors, ignoring the specific requirements of real-world applications that employ them. This paper presents a new training methodology, which allows a forecasting model to dynamically adjust its focus based on the importance of forecast ranges specified by the end application. Unlike previous methods that fix these ranges beforehand, our training approach breaks down predictions over the entire signal range into smaller segments, which are then dynamically weighted and combined to produce accurate forecasts within a region of interest. We tested our method on standard datasets, including a new wireless communication dataset, and found that not only it improves prediction accuracy but also enhances the performance of end application employing the forecasting model. This research provides a basis for creating forecasting systems that better connect prediction and decision-making in various practical applications."
    },
    {
      "paperId": "739a7677b12d9668e07730ca8a60d2ba646f5f24",
      "externalIds": {
        "DOI": "10.3390/sym17050638",
        "CorpusId": 278041979
      },
      "title": "Classification-Based Q-Value Estimation for Continuous Actor-Critic Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/sym17050638?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/sym17050638, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2249060569",
          "name": "Chayoung Kim"
        }
      ],
      "abstract": "Stable Q-value estimation is critical for effective policy learning in deep reinforcement learning (DRL), especially continuous control tasks. Traditional algorithms like Soft Actor-Critic (SAC) and Twin Delayed Deep Deterministic (TD3) policy gradients rely on Mean Squared Error (MSE) loss for Q-value approximation, which may cause instability due to misestimation and overestimation biases. Although distributional reinforcement learning (RL) algorithms like C51 have improved robustness in discrete action spaces, their application to continuous control remains computationally expensive owing to distribution projection needs. To address this, we propose a classification-based Q-value learning method that reformulates Q-value estimation as a classification problem rather than a regression task. Replacing MSE loss with cross-entropy (CE) and Kullback–Leibler (KL) divergence losses, the proposed method improves learning stability and mitigates overestimation errors. Our statistical analysis across 30 independent runs shows that the approach achieves an approximately 10% lower Q-value estimation error in the pendulum environment and a 40–60% reduced training time compared to SAC and Continuous Twin Delayed Distributed Deep Deterministic (CTD4) Policy Gradient. Experimental results on OpenAI Gym benchmark environments demonstrate that our approach, with up to 77% fewer parameters, outperforms the SAC and CTD4 policy gradients regarding training stability and convergence speed, while maintaining a competitive final policy performance."
    },
    {
      "paperId": "1dc866d8276330fccf896d9c766990c7c21679d7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-15643",
        "ArXiv": "2504.15643",
        "DOI": "10.48550/arXiv.2504.15643",
        "CorpusId": 277993844
      },
      "title": "Multimodal Perception for Goal-oriented Navigation: A Survey",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2356787839",
          "name": "I-Tak Ieong"
        },
        {
          "authorId": "2357076584",
          "name": "Hao Tang"
        }
      ],
      "abstract": "Goal-oriented navigation presents a fundamental challenge for autonomous systems, requiring agents to navigate complex environments to reach designated targets. This survey offers a comprehensive analysis of multimodal navigation approaches through the unifying perspective of inference domains, exploring how agents perceive, reason about, and navigate environments using visual, linguistic, and acoustic information. Our key contributions include organizing navigation methods based on their primary environmental reasoning mechanisms across inference domains; systematically analyzing how shared computational foundations support seemingly disparate approaches across different navigation tasks; identifying recurring patterns and distinctive strengths across various navigation paradigms; and examining the integration challenges and opportunities of multimodal perception to enhance navigation capabilities. In addition, we review approximately 200 relevant articles to provide an in-depth understanding of the current landscape."
    },
    {
      "paperId": "08fb31ebea65100196a7553775248c8ee4867ab3",
      "externalIds": {
        "DOI": "10.1080/01691864.2025.2483929",
        "CorpusId": 277900779
      },
      "title": "ForceMapping: learning visual-force features from vision for soft objects manipulation",
      "openAccessPdf": {
        "url": "https://doi.org/10.1080/01691864.2025.2483929",
        "status": "HYBRID",
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1080/01691864.2025.2483929?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1080/01691864.2025.2483929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2327238835",
          "name": "Abdullah Mustafa"
        },
        {
          "authorId": "2243398622",
          "name": "Ryo Hanai"
        },
        {
          "authorId": "2356149638",
          "name": "Ixchel Ramirez"
        },
        {
          "authorId": "3211515",
          "name": "Floris Erich"
        },
        {
          "authorId": "3411577",
          "name": "Ryoichi Nakajo"
        },
        {
          "authorId": "2512607",
          "name": "Y. Domae"
        },
        {
          "authorId": "2176111734",
          "name": "Tetsuya Ogata"
        }
      ],
      "abstract": "To realize delicate manipulation of soft/rigid objects, incorporating some notion of force into the controller is essential. Such forces can either be measured explicitly via onboard sensors or estimated from visual input through an offline-trained network. For visuomotor control, unlike state-based models, visual input (i.e. RGB images) is first encoded into a low-dimensional latent before feeding into some low-level policy. The encoder network is usually trained end-to-end using supervised policy loss or pre-trained with unsupervised RGB reconstruction loss. This work proposes a learning approach that implicitly integrates force-relevant features into the latent image encoding, rather than explicit force integration. Our proposal, ForceMapping, incorporates a supervised force prediction loss as an auxiliary optimization objective alongside unsupervised RGB reconstruction loss or policy loss. For soft bodies, forces are estimated from the raw RGB image input, given the correlation between object deformation and imposed forces. To validate our approach, we designed a soft object stacking task requiring force-aware manipulation. Soft-body deformation was efficiently rendered through a fast and stable pipeline, utilizing states obtained from rigid-body simulation. Our experiments demonstrated that ForceMapping encouraged attention to and encoding of force-relevant features, leading to increased task success rates (from 25% to 75%) and reduced deviations from a target critical load (from 300% to 40%) compared to only-RGB model. GRAPHICAL ABSTRACT"
    },
    {
      "paperId": "e54f2a86192ea5437e64107d6f4f3534b702ad8c",
      "externalIds": {
        "ArXiv": "2504.11247",
        "DBLP": "journals/corr/abs-2504-11247",
        "DOI": "10.48550/arXiv.2504.11247",
        "CorpusId": 277787312
      },
      "title": "Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.11247, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2076056688",
          "name": "Fikrican Özgür"
        },
        {
          "authorId": "2362571762",
          "name": "René Zurbrügg"
        },
        {
          "authorId": "2355566081",
          "name": "Suryansh Kumar"
        }
      ],
      "abstract": "Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art algorithm for achieving sample-efficient multi-goal reinforcement learning (RL) in robotic manipulation tasks with binary rewards. HER facilitates learning from failed attempts by replaying trajectories with redefined goals. However, it relies on a heuristic-based replay method that lacks a principled framework. To address this limitation, we introduce a novel replay strategy,\"Next-Future\", which focuses on rewarding single-step transitions. This approach significantly enhances sample efficiency and accuracy in learning multi-goal Markov decision processes (MDPs), particularly under stringent accuracy requirements -- a critical aspect for performing complex and precise robotic-arm tasks. We demonstrate the efficacy of our method by highlighting how single-step learning enables improved value approximation within the multi-goal RL framework. The performance of the proposed replay strategy is evaluated across eight challenging robotic manipulation tasks, using ten random seeds for training. Our results indicate substantial improvements in sample efficiency for seven out of eight tasks and higher success rates in six tasks. Furthermore, real-world experiments validate the practical feasibility of the learned policies, demonstrating the potential of\"Next-Future\"in solving complex robotic-arm tasks."
    },
    {
      "paperId": "72b97f98a354e7352ba218be16f601610387f5b2",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-09927",
        "ArXiv": "2504.09927",
        "DOI": "10.48550/arXiv.2504.09927",
        "CorpusId": 277781044
      },
      "title": "Efficient Task-specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.09927, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2337278285",
          "name": "Haiyong Yu"
        },
        {
          "authorId": "2355398726",
          "name": "Yanqiong Jin"
        },
        {
          "authorId": "2336250303",
          "name": "Yonghao He"
        },
        {
          "authorId": "2355352818",
          "name": "Wei Sui"
        }
      ],
      "abstract": "Imitation learning, particularly Diffusion Policies based methods, has recently gained significant traction in embodied AI as a powerful approach to action policy generation. These models efficiently generate action policies by learning to predict noise. However, conventional Diffusion Policy methods rely on iterative denoising, leading to inefficient inference and slow response times, which hinder real-time robot control. To address these limitations, we propose a Classifier-Free Shortcut Diffusion Policy (CF-SDP) that integrates classifier-free guidance with shortcut-based acceleration, enabling efficient task-specific action generation while significantly improving inference speed. Furthermore, we extend diffusion modeling to the SO(3) manifold in shortcut model, defining the forward and reverse processes in its tangent space with an isotropic Gaussian distribution. This ensures stable and accurate rotational estimation, enhancing the effectiveness of diffusion-based control. Our approach achieves nearly 5x acceleration in diffusion inference compared to DDIM-based Diffusion Policy while maintaining task performance. Evaluations both on the RoboTwin simulation platform and real-world scenarios across various tasks demonstrate the superiority of our method."
    },
    {
      "paperId": "e70b5b1f7186d8bc4159b4e05c3e8d1ca93bcb26",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-07779",
        "ArXiv": "2504.07779",
        "DOI": "10.48550/arXiv.2504.07779",
        "CorpusId": 277667709
      },
      "title": "Genetic Programming with Reinforcement Learning Trained Transformer for Real-World Dynamic Scheduling Problems",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2354620639",
          "name": "Xian Chen"
        },
        {
          "authorId": "2269086070",
          "name": "Rong Qu"
        },
        {
          "authorId": "2284302767",
          "name": "Jing Dong"
        },
        {
          "authorId": "2283072986",
          "name": "Ruibin Bai"
        },
        {
          "authorId": "2306876083",
          "name": "Yaochu Jin"
        }
      ],
      "abstract": "Dynamic scheduling in real-world environments often struggles to adapt to unforeseen disruptions, making traditional static scheduling methods and human-designed heuristics inadequate. This paper introduces an innovative approach that combines Genetic Programming (GP) with a Transformer trained through Reinforcement Learning (GPRT), specifically designed to tackle the complexities of dynamic scheduling scenarios. GPRT leverages the Transformer to refine heuristics generated by GP while also seeding and guiding the evolution of GP. This dual functionality enhances the adaptability and effectiveness of the scheduling heuristics, enabling them to better respond to the dynamic nature of real-world tasks. The efficacy of this integrated approach is demonstrated through a practical application in container terminal truck scheduling, where the GPRT method outperforms traditional GP, standalone Transformer methods, and other state-of-the-art competitors. The key contribution of this research is the development of the GPRT method, which showcases a novel combination of GP and Reinforcement Learning (RL) to produce robust and efficient scheduling solutions. Importantly, GPRT is not limited to container port truck scheduling; it offers a versatile framework applicable to various dynamic scheduling challenges. Its practicality, coupled with its interpretability and ease of modification, makes it a valuable tool for diverse real-world scenarios."
    },
    {
      "paperId": "85317048752487e06ba31239eb5b4233c4337242",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-07283",
        "ArXiv": "2504.07283",
        "DOI": "10.48550/arXiv.2504.07283",
        "CorpusId": 277667730
      },
      "title": "Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2355375550",
          "name": "Licheng Luo"
        },
        {
          "authorId": "2354718220",
          "name": "Mingyu Cai"
        }
      ],
      "abstract": "Deep Reinforcement Learning (DRL) has emerged as a powerful model-free paradigm for learning optimal policies. However, in navigation tasks with cluttered environments, DRL methods often suffer from insufficient exploration, especially under sparse rewards or complex dynamics with system disturbances. To address this challenge, we bridge general graph-based motion planning with DRL, enabling agents to explore cluttered spaces more effectively and achieve desired navigation performance. Specifically, we design a dense reward function grounded in a graph structure that spans the entire state space. This graph provides rich guidance, steering the agent toward optimal strategies. We validate our approach in challenging environments, demonstrating substantial improvements in exploration efficiency and task success rates."
    },
    {
      "paperId": "4beb1fc208fa17b836a05e05f0bb80e81655d562",
      "externalIds": {
        "ArXiv": "2504.06156",
        "DBLP": "journals/corr/abs-2504-06156",
        "DOI": "10.48550/arXiv.2504.06156",
        "CorpusId": 276864499
      },
      "title": "ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.06156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2279745979",
          "name": "Fangchen Liu"
        },
        {
          "authorId": "2310944952",
          "name": "Chuanyu Li"
        },
        {
          "authorId": "2355365055",
          "name": "Yihua Qin"
        },
        {
          "authorId": "2354250616",
          "name": "Ankit Shaw"
        },
        {
          "authorId": "2321185480",
          "name": "Jing Xu"
        },
        {
          "authorId": "2257184474",
          "name": "Pieter Abbeel"
        },
        {
          "authorId": "2332011508",
          "name": "Rui Chen"
        }
      ],
      "abstract": "Tactile information plays a crucial role for humans and robots to interact effectively with their environment, particularly for tasks requiring the understanding of contact properties. Solving such dexterous manipulation tasks often relies on imitation learning from demonstration datasets, which are typically collected via teleoperation systems and often demand substantial time and effort. To address these challenges, we present ViTaMIn, an embodiment-free manipulation interface that seamlessly integrates visual and tactile sensing into a hand-held gripper, enabling data collection without the need for teleoperation. Our design employs a compliant Fin Ray gripper with tactile sensing, allowing operators to perceive force feedback during manipulation for more intuitive operation. Additionally, we propose a multimodal representation learning strategy to obtain pre-trained tactile representations, improving data efficiency and policy robustness. Experiments on seven contact-rich manipulation tasks demonstrate that ViTaMIn significantly outperforms baseline methods, demonstrating its effectiveness for complex manipulation tasks."
    },
    {
      "paperId": "dc5fb12a1342e1711e2e289b2d9ef38128f25399",
      "externalIds": {
        "ArXiv": "2504.03983",
        "DBLP": "journals/corr/abs-2504-03983",
        "DOI": "10.48550/arXiv.2504.03983",
        "CorpusId": 277621141
      },
      "title": "I Can Hear You Coming: RF Sensing for Uncooperative Satellite Evasion",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.03983, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319935076",
          "name": "Cameron Mehlman"
        },
        {
          "authorId": "2319927721",
          "name": "Gregory Falco"
        }
      ],
      "abstract": "This work presents a novel method for leveraging intercepted Radio Frequency (RF) signals to inform a constrained Reinforcement Learning (RL) policy for robust control of a satellite operating in contested environments. Uncooperative satellite engagements with nation-state actors prompts the need for enhanced maneuverability and agility on-orbit. However, robust, autonomous and rapid adversary avoidance capabilities for the space environment is seldom studied. Further, the capability constrained nature of many space vehicles does not afford robust space situational awareness capabilities that can be used for well informed maneuvering. We present a\"Cat&Mouse\"system for training optimal adversary avoidance algorithms using RL. We propose the novel approach of utilizing intercepted radio frequency communication and dynamic spacecraft state as multi-modal input that could inform paths for a mouse to outmaneuver the cat satellite. Given the current ubiquitous use of RF communications, our proposed system can be applicable to a diverse array of satellites. In addition to providing a comprehensive framework for training and implementing a constrained RL policy capable of providing control for robust adversary avoidance, we also explore several optimization based methods for adversarial avoidance. These methods were then tested on real-world data obtained from the Space Surveillance Network (SSN) to analyze the benefits and limitations of different avoidance methods."
    },
    {
      "paperId": "0692b401d6ccd3a6bd4d2ebd87326a52d493d6da",
      "externalIds": {
        "DBLP": "journals/corr/abs-2504-01637",
        "ArXiv": "2504.01637",
        "DOI": "10.48550/arXiv.2504.01637",
        "CorpusId": 277502169
      },
      "title": "LLM-mediated Dynamic Plan Generation with a Multi-Agent Approach",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01637, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2353282265",
          "name": "Reo Abe"
        },
        {
          "authorId": "2353496354",
          "name": "Akifumi Ito"
        },
        {
          "authorId": "2353281967",
          "name": "Kanata Takayasu"
        },
        {
          "authorId": "2353278452",
          "name": "Satoshi Kurihara"
        }
      ],
      "abstract": "Planning methods with high adaptability to dynamic environments are crucial for the development of autonomous and versatile robots. We propose a method for leveraging a large language model (GPT-4o) to automatically generate networks capable of adapting to dynamic environments. The proposed method collects environmental\"status,\"representing conditions and goals, and uses them to generate agents. These agents are interconnected on the basis of specific conditions, resulting in networks that combine flexibility and generality. We conducted evaluation experiments to compare the networks automatically generated with the proposed method with manually constructed ones, confirming the comprehensiveness of the proposed method's networks and their higher generality. This research marks a significant advancement toward the development of versatile planning methods applicable to robotics, autonomous vehicles, smart systems, and other complex environments."
    },
    {
      "paperId": "1703d08be2e03f52fb5d2211bd8157ced4572408",
      "externalIds": {
        "DBLP": "journals/tkde/LiuLXZ25",
        "DOI": "10.1109/TKDE.2025.3535961",
        "CorpusId": 275995835
      },
      "title": "Learning Latent and Changing Dynamics in Real Non-Stationary Environments",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TKDE.2025.3535961?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TKDE.2025.3535961, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284186308",
          "name": "Zihe Liu"
        },
        {
          "authorId": "2272678886",
          "name": "Jie Lu"
        },
        {
          "authorId": "2207125",
          "name": "Junyu Xuan"
        },
        {
          "authorId": "46266495",
          "name": "Guangquan Zhang"
        }
      ],
      "abstract": "Model-based reinforcement learning (RL) aims to learn the underlying dynamics of a given environment. The success of most existing works is built on the critical assumption that the dynamic is fixed, which is unrealistic in many open-world scenarios, such as drone delivery and online chatting, where agents may need to deal with environments with unpredictable changing dynamics (hereafter, real non-stationary environment). Therefore, learning changing dynamics in a real non-stationary environment offers both significant benefits and challenges. This paper proposes a new model-based reinforcement learning algorithm that proactively and dynamically detects possible changes and Learns these Latent and Changing Dynamics (LLCD) in a latent Markovian space for real non-stationary environments. To ensure the Markovian property of the RL model and improve computational efficiency, we employ a latent space model to learn the environment’s transition dynamics. Furthermore, we perform online change detection in the latent space to promptly identify change points in non-stationary environments. Then, we utilize the detected information to help the agent adapt to new conditions. Experiments indicate that the rewards of the proposed algorithm accumulate for the most rapid adaptions to environmental change, among other benefits. This work has a strong potential to enhance environmentally suitable model-based reinforcement learning capabilities."
    },
    {
      "paperId": "fc7c4cdc2ea4dd56b800c34443a62332a8c49338",
      "externalIds": {
        "DBLP": "journals/swevo/DingLRYM25",
        "DOI": "10.1016/j.swevo.2025.101907",
        "CorpusId": 277027058
      },
      "title": "A novel deep self-learning method for flexible job-shop scheduling problems with multiplicity: Deep reinforcement learning assisted the fluid master-apprentice evolutionary algorithm",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.swevo.2025.101907?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.swevo.2025.101907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2153731753",
          "name": "Linshan Ding"
        },
        {
          "authorId": "2350339101",
          "name": "Dan Luo"
        },
        {
          "authorId": "73777550",
          "name": "Mudassar Rauf"
        },
        {
          "authorId": "2291572187",
          "name": "Lei Yue"
        },
        {
          "authorId": "2117876375",
          "name": "Lei-lei Meng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ea521c1ee2cfdfbc054739a15b166709a0aa86fa",
      "externalIds": {
        "DBLP": "journals/cogsr/MoraisSCSGC25",
        "DOI": "10.1016/j.cogsys.2025.101354",
        "CorpusId": 277466663
      },
      "title": "A general framework for reinforcement learning in cognitive architectures",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.cogsys.2025.101354?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.cogsys.2025.101354, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2192849215",
          "name": "Gustavo Morais"
        },
        {
          "authorId": "2192567651",
          "name": "E. Y. Sakabe"
        },
        {
          "authorId": "2238254091",
          "name": "Paula D. P. Costa"
        },
        {
          "authorId": "2192849085",
          "name": "A. Simões"
        },
        {
          "authorId": "2317097823",
          "name": "Ricardo R. Gudwin"
        },
        {
          "authorId": "2261087767",
          "name": "Esther Colombini"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "17bc0ff84559502ff6974a00ee0b2e9c61015e73",
      "externalIds": {
        "ArXiv": "2503.23571",
        "DBLP": "journals/corr/abs-2503-23571",
        "DOI": "10.48550/arXiv.2503.23571",
        "CorpusId": 277452067
      },
      "title": "Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.23571, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2256776713",
          "name": "Shutong Jin"
        },
        {
          "authorId": "2352939146",
          "name": "Axel Kaliff"
        },
        {
          "authorId": "2255293781",
          "name": "Ruiyu Wang"
        },
        {
          "authorId": "2244621477",
          "name": "Muhammad Zahid"
        },
        {
          "authorId": "1881469",
          "name": "Florian T. Pokorny"
        }
      ],
      "abstract": "Human demonstrations have been key to recent advancements in robotic manipulation, but their scalability is hampered by the substantial cost of the required human labor. In this paper, we focus on random exploration data-video sequences and actions produced autonomously via motions to randomly sampled positions in the workspace-as an often overlooked resource for training visuo-motor policies in robotic manipulation. Within the scope of imitation learning, we examine random exploration data through two paradigms: (a) by investigating the use of random exploration video frames with three self-supervised learning objectives-reconstruction, contrastive, and distillation losses-and evaluating their applicability to visual pre-training; and (b) by analyzing random motor commands in the context of a staged learning framework to assess their effectiveness in autonomous data collection. Towards this goal, we present a large-scale experimental study based on over 750 hours of robot data collection, comprising 400 successful and 12,000 failed episodes. Our results indicate that: (a) among the three self-supervised learning objectives, contrastive loss appears most effective for visual pre-training while leveraging random exploration video frames; (b) data collected with random motor commands may play a crucial role in balancing the training data distribution and improving success rates in autonomous data collection within this study. The source code and dataset will be made publicly available at https://cloudgripper.org."
    },
    {
      "paperId": "3f7aeb0658f8b272f5fce1d95d164c210c387cee",
      "externalIds": {
        "ArXiv": "2503.18738",
        "DBLP": "journals/corr/abs-2503-18738",
        "DOI": "10.48550/arXiv.2503.18738",
        "CorpusId": 277272352
      },
      "title": "RoboEngine: Plug-and-Play Robot Data Augmentation with Semantic Robot Segmentation and Background Generation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18738, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2280187985",
          "name": "Chengbo Yuan"
        },
        {
          "authorId": "2353558108",
          "name": "Suraj Joshi"
        },
        {
          "authorId": "2216251632",
          "name": "Shaoting Zhu"
        },
        {
          "authorId": "2302165612",
          "name": "Hang Su"
        },
        {
          "authorId": "2312396784",
          "name": "Hang Zhao"
        },
        {
          "authorId": "2330756947",
          "name": "Yang Gao"
        }
      ],
      "abstract": "Visual augmentation has become a crucial technique for enhancing the visual robustness of imitation learning. However, existing methods are often limited by prerequisites such as camera calibration or the need for controlled environments (e.g., green screen setups). In this work, we introduce RoboEngine, the first plug-and-play visual robot data augmentation toolkit. For the first time, users can effortlessly generate physics- and task-aware robot scenes with just a few lines of code. To achieve this, we present a novel robot scene segmentation dataset, a generalizable high-quality robot segmentation model, and a fine-tuned background generation model, which together form the core components of the out-of-the-box toolkit. Using RoboEngine, we demonstrate the ability to generalize robot manipulation tasks across six entirely new scenes, based solely on demonstrations collected from a single scene, achieving a more than 200% performance improvement compared to the no-augmentation baseline. All datasets, model weights, and the toolkit will be publicly released."
    },
    {
      "paperId": "7ce0344f9b94e2e0661a96dd07a52801cb7fa679",
      "externalIds": {
        "DBLP": "conf/iclr/WangGWQL25",
        "ArXiv": "2503.18871",
        "DOI": "10.48550/arXiv.2503.18871",
        "CorpusId": 277596175
      },
      "title": "Bootstrapped Model Predictive Control",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.18871, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2347201151",
          "name": "Yuhang Wang"
        },
        {
          "authorId": "2354141850",
          "name": "Hanwei Guo"
        },
        {
          "authorId": "2354070568",
          "name": "Sizhe Wang"
        },
        {
          "authorId": "2292326961",
          "name": "Long Qian"
        },
        {
          "authorId": "2287922954",
          "name": "Xuguang Lan"
        }
      ],
      "abstract": "Model Predictive Control (MPC) has been demonstrated to be effective in continuous control tasks. When a world model and a value function are available, planning a sequence of actions ahead of time leads to a better policy. Existing methods typically obtain the value function and the corresponding policy in a model-free manner. However, we find that such an approach struggles with complex tasks, resulting in poor policy learning and inaccurate value estimation. To address this problem, we leverage the strengths of MPC itself. In this work, we introduce Bootstrapped Model Predictive Control (BMPC), a novel algorithm that performs policy learning in a bootstrapped manner. BMPC learns a network policy by imitating an MPC expert, and in turn, uses this policy to guide the MPC process. Combined with model-based TD-learning, our policy learning yields better value estimation and further boosts the efficiency of MPC. We also introduce a lazy reanalyze mechanism, which enables computationally efficient imitation learning. Our method achieves superior performance over prior works on diverse continuous control tasks. In particular, on challenging high-dimensional locomotion tasks, BMPC significantly improves data efficiency while also enhancing asymptotic performance and training stability, with comparable training time and smaller network sizes. Code is available at https://github.com/wertyuilife2/bmpc."
    },
    {
      "paperId": "18c8da7705fda0ea1e919c5eb937fcff4dc960b7",
      "externalIds": {
        "DOI": "10.1109/SmartIndustryCon65166.2025.10986196",
        "CorpusId": 278431866
      },
      "title": "Approach for Task Transfer and Demonstration Learning Optimization in Industrial Robotic Manipulators",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SmartIndustryCon65166.2025.10986196?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SmartIndustryCon65166.2025.10986196, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2319604976",
          "name": "Tianci Gao"
        },
        {
          "authorId": "2359953012",
          "name": "Bo Yang"
        },
        {
          "authorId": "2319824077",
          "name": "Shengren Rao"
        }
      ],
      "abstract": "This paper proposes a refined method to enhance the performance and adaptability of industrial robotic manipulators by integrating demonstration learning with reinforcement learning, specifically using Proximal Policy Optimization (PPO). The approach starts with generating an initial trajectory through demonstration-based learning, which provides a robust foundation. Subsequently, PPO is employed to fine-tune the policy, improving precision while minimizing the amount of data required. Experimental evaluations reveal that our method achieves a 15% reduction in training data needs compared to standard approaches, along with enhanced adaptability across different tasks. This makes the method well-suited for flexible manufacturing settings where rapid task transfer is crucial."
    },
    {
      "paperId": "44fd2dcb9fe45e37adc92b9fb4021ad93b8389a9",
      "externalIds": {
        "DOI": "10.1109/INFOTEH64129.2025.10959192",
        "CorpusId": 277798688
      },
      "title": "AI-Driven Human-Robot Interaction for a 3D-Printed Robotic Arm Using Gemini AI and ROS",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/INFOTEH64129.2025.10959192?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/INFOTEH64129.2025.10959192, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2162757290",
          "name": "Amer Sarajlić"
        },
        {
          "authorId": "2355491901",
          "name": "Lejla Banjanović-Mehmedović"
        }
      ],
      "abstract": "This paper presents a novel AI-driven control system for a 3D-printed robotic arm that leverages natural language processing (NLP) and the Robot Operating System (ROS) to enable intuitive human-robot interaction. The proposed system integrates Gemini AI, a state-of-the-art NLP model, to interpret user commands and translate them into executable ROS commands. These commands are transmitted via a ROSBridge WebSocket server to a controller that drives the robotic arm. A mobile application serves as the primary user interface, ensuring seamless, real-time communication between the operator and the robotic system. Furthermore, the arm’s movements are pre-validated in RViz via simulation to guarantee safety and accuracy. Experimental results demonstrate high command execution accuracy and low latency, thus contributing to scalable and user-friendly robotic control frameworks suitable for industrial automation, healthcare, and educational applications."
    },
    {
      "paperId": "1e934631aeb726b5cd2a68d02a88f98c9e60ce71",
      "externalIds": {
        "ArXiv": "2503.10110",
        "DBLP": "journals/corr/abs-2503-10110",
        "DOI": "10.48550/arXiv.2503.10110",
        "CorpusId": 276961034
      },
      "title": "IMPACT: Intelligent Motion Planning with Acceptable Contact Trajectories via Vision-Language Models",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.10110, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2349798230",
          "name": "Yiyang Ling"
        },
        {
          "authorId": "2135551642",
          "name": "Karan Owalekar"
        },
        {
          "authorId": "2349806087",
          "name": "Oluwatobiloba Adesanya"
        },
        {
          "authorId": "8307674",
          "name": "Erdem Biyik"
        },
        {
          "authorId": "2310236368",
          "name": "Daniel Seita"
        }
      ],
      "abstract": "Motion planning involves determining a sequence of robot configurations to reach a desired pose, subject to movement and safety constraints. Traditional motion planning finds collision-free paths, but this is overly restrictive in clutter, where it may not be possible for a robot to accomplish a task without contact. In addition, contacts range from relatively benign (e.g., brushing a soft pillow) to more dangerous (e.g., toppling a glass vase). Due to this diversity, it is difficult to characterize which contacts may be acceptable or unacceptable. In this paper, we propose IMPACT, a novel motion planning framework that uses Vision-Language Models (VLMs) to infer environment semantics, identifying which parts of the environment can best tolerate contact based on object properties and locations. Our approach uses the VLM's outputs to produce a dense 3D\"cost map\"that encodes contact tolerances and seamlessly integrates with standard motion planners. We perform experiments using 20 simulation and 10 real-world scenes and assess using task success rate, object displacements, and feedback from human evaluators. Our results over 3620 simulation and 200 real-world trials suggest that IMPACT enables efficient contact-rich motion planning in cluttered settings while outperforming alternative methods and ablations. Supplementary material is available at https://impact-planning.github.io/."
    },
    {
      "paperId": "ec2893ce63a5e3d22b63ca3c7ff7e62e4a11d91e",
      "externalIds": {
        "ArXiv": "2503.09010",
        "DBLP": "journals/corr/abs-2503-09010",
        "DOI": "10.48550/arXiv.2503.09010",
        "CorpusId": 276938321
      },
      "title": "HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09010, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2230253713",
          "name": "Qiang Zhang"
        },
        {
          "authorId": "2191217616",
          "name": "Zhang Zhang"
        },
        {
          "authorId": "2308839373",
          "name": "Wei Cui"
        },
        {
          "authorId": "2243709540",
          "name": "Jingkai Sun"
        },
        {
          "authorId": "1406166829",
          "name": "Jiahang Cao"
        },
        {
          "authorId": "2304744634",
          "name": "Yijie Guo"
        },
        {
          "authorId": "2304988250",
          "name": "Gang Han"
        },
        {
          "authorId": "2304593772",
          "name": "Wen Zhao"
        },
        {
          "authorId": "2242768739",
          "name": "Jiaxu Wang"
        },
        {
          "authorId": "2349637311",
          "name": "Chenghao Sun"
        },
        {
          "authorId": "2292080350",
          "name": "Lingfeng Zhang"
        },
        {
          "authorId": "2288359399",
          "name": "Hao Cheng"
        },
        {
          "authorId": "2305242168",
          "name": "Yujie Chen"
        },
        {
          "authorId": "2350076169",
          "name": "Lin Wang"
        },
        {
          "authorId": "2349650508",
          "name": "Jian Tang"
        },
        {
          "authorId": "2243405867",
          "name": "Renjing Xu"
        }
      ],
      "abstract": "The perceptual system design for humanoid robots poses unique challenges due to inherent structural constraints that cause severe self-occlusion and limited field-of-view (FOV). We present HumanoidPano, a novel hybrid cross-modal perception framework that synergistically integrates panoramic vision and LiDAR sensing to overcome these limitations. Unlike conventional robot perception systems that rely on monocular cameras or standard multi-sensor configurations, our method establishes geometrically-aware modality alignment through a spherical vision transformer, enabling seamless fusion of 360 visual context with LiDAR's precise depth measurements. First, Spherical Geometry-aware Constraints (SGC) leverage panoramic camera ray properties to guide distortion-regularized sampling offsets for geometric alignment. Second, Spatial Deformable Attention (SDA) aggregates hierarchical 3D features via spherical offsets, enabling efficient 360{\\deg}-to-BEV fusion with geometrically complete object representations. Third, Panoramic Augmentation (AUG) combines cross-view transformations and semantic alignment to enhance BEV-panoramic feature consistency during data augmentation. Extensive evaluations demonstrate state-of-the-art performance on the 360BEV-Matterport benchmark. Real-world deployment on humanoid platforms validates the system's capability to generate accurate BEV segmentation maps through panoramic-LiDAR co-perception, directly enabling downstream navigation tasks in complex environments. Our work establishes a new paradigm for embodied perception in humanoid robotics."
    },
    {
      "paperId": "d4b822a4fedaddc1795d475f0a2ab431d0f20f6e",
      "externalIds": {
        "ArXiv": "2503.09078",
        "DBLP": "journals/corr/abs-2503-09078",
        "DOI": "10.48550/arXiv.2503.09078",
        "CorpusId": 276937989
      },
      "title": "Sequential Multi-Object Grasping with One Dexterous Hand",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09078, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2310814135",
          "name": "Sicheng He"
        },
        {
          "authorId": "100740035",
          "name": "Zeyu Shangguan"
        },
        {
          "authorId": "2350478101",
          "name": "Kuanning Wang"
        },
        {
          "authorId": "2314937056",
          "name": "Yongchong Gu"
        },
        {
          "authorId": "2118638474",
          "name": "Yu Fu"
        },
        {
          "authorId": "2286953627",
          "name": "Yanwei Fu"
        },
        {
          "authorId": "2310236368",
          "name": "Daniel Seita"
        }
      ],
      "abstract": "Sequentially grasping multiple objects with multi-fingered hands is common in daily life, where humans can fully leverage the dexterity of their hands to enclose multiple objects. However, the diversity of object geometries and the complex contact interactions required for high-DOF hands to grasp one object while enclosing another make sequential multi-object grasping challenging for robots. In this paper, we propose SeqMultiGrasp, a system for sequentially grasping objects with a four-fingered Allegro Hand. We focus on sequentially grasping two objects, ensuring that the hand fully encloses one object before lifting it and then grasps the second object without dropping the first. Our system first synthesizes single-object grasp candidates, where each grasp is constrained to use only a subset of the hand's links. These grasps are then validated in a physics simulator to ensure stability and feasibility. Next, we merge the validated single-object grasp poses to construct multi-object grasp configurations. For real-world deployment, we train a diffusion model conditioned on point clouds to propose grasp poses, followed by a heuristic-based execution strategy. We test our system using $8 \\times 8$ object combinations in simulation and $6 \\times 3$ object combinations in real. Our diffusion-based grasp model obtains an average success rate of 65.8% over 1600 simulation trials and 56.7% over 90 real-world trials, suggesting that it is a promising approach for sequential multi-object grasping with multi-fingered hands. Supplementary material is available on our project website: https://hesic73.github.io/SeqMultiGrasp."
    },
    {
      "paperId": "37b21fbef9af92026e33b436ae08012822501b98",
      "externalIds": {
        "ArXiv": "2503.07111",
        "DBLP": "journals/corr/abs-2503-07111",
        "DOI": "10.48550/arXiv.2503.07111",
        "CorpusId": 276902739
      },
      "title": "PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping with VLM",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.07111, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2326990900",
          "name": "Alan Dao"
        },
        {
          "authorId": "2326991488",
          "name": "Dinh Bach Vu"
        },
        {
          "authorId": "2349376123",
          "name": "Tuan Le Duc Anh"
        },
        {
          "authorId": "39503766",
          "name": "Bui Quang Huy"
        }
      ],
      "abstract": "This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset."
    },
    {
      "paperId": "07090d5c281bde0693ad82547c46e288d30fc4e1",
      "externalIds": {
        "ArXiv": "2503.04877",
        "DBLP": "journals/corr/abs-2503-04877",
        "DOI": "10.48550/arXiv.2503.04877",
        "CorpusId": 276884956
      },
      "title": "Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.04877, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2312326990",
          "name": "Albert Wilcox"
        },
        {
          "authorId": "2349235315",
          "name": "Mohamed Ghanem"
        },
        {
          "authorId": "2265319511",
          "name": "Masoud Moghani"
        },
        {
          "authorId": "2349234988",
          "name": "Pierre Barroso"
        },
        {
          "authorId": "2349251111",
          "name": "Benjamin Joffe"
        },
        {
          "authorId": "2261377967",
          "name": "Animesh Garg"
        }
      ],
      "abstract": "Imitation Learning can train robots to perform complex and diverse manipulation tasks, but learned policies are brittle with observations outside of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to mitigate this, but in our evaluations with unseen embodiments and camera viewpoints they show only modest improvement. To address those challenges, we propose Adapt3R, a general-purpose 3D observation encoder which synthesizes data from calibrated RGBD cameras into a vector that can be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information, using 3D only as a medium to localize this information with respect to the end-effector. We show across 93 simulated and 6 real tasks that when trained end-to-end with a variety of IL algorithms, Adapt3R maintains these algorithms' learning capacity while enabling zero-shot transfer to novel embodiments and camera poses."
    },
    {
      "paperId": "91af851f674aaa6064827527a3f40c733e51f742",
      "externalIds": {
        "ArXiv": "2503.03464",
        "DBLP": "journals/corr/abs-2503-03464",
        "DOI": "10.48550/arXiv.2503.03464",
        "CorpusId": 276781799
      },
      "title": "Generative Artificial Intelligence in Robotic Manipulation: A Survey",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03464, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2164891540",
          "name": "Kun Zhang"
        },
        {
          "authorId": "2348540931",
          "name": "Peng Yun"
        },
        {
          "authorId": "2348540936",
          "name": "Jun Cen"
        },
        {
          "authorId": "50490278",
          "name": "Junhao Cai"
        },
        {
          "authorId": "2340942765",
          "name": "Didi Zhu"
        },
        {
          "authorId": "2349208749",
          "name": "Hangjie Yuan"
        },
        {
          "authorId": "2348696149",
          "name": "Chao Zhao"
        },
        {
          "authorId": "2348542636",
          "name": "Tao Feng"
        },
        {
          "authorId": "2348696927",
          "name": "Michael Yu Wang"
        },
        {
          "authorId": "2115931029",
          "name": "Qifeng Chen"
        },
        {
          "authorId": "2274201540",
          "name": "Jia Pan"
        },
        {
          "authorId": "2309564492",
          "name": "Wei Zhang"
        },
        {
          "authorId": "2348692423",
          "name": "Bo Yang"
        },
        {
          "authorId": "2144213446",
          "name": "Hua Chen"
        }
      ],
      "abstract": "This survey provides a comprehensive review on recent advancements of generative learning models in robotic manipulation, addressing key challenges in the field. Robotic manipulation faces critical bottlenecks, including significant challenges in insufficient data and inefficient data acquisition, long-horizon and complex task planning, and the multi-modality reasoning ability for robust policy learning performance across diverse environments. To tackle these challenges, this survey introduces several generative model paradigms, including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), diffusion models, probabilistic flow models, and autoregressive models, highlighting their strengths and limitations. The applications of these models are categorized into three hierarchical layers: the Foundation Layer, focusing on data generation and reward generation; the Intermediate Layer, covering language, code, visual, and state generation; and the Policy Layer, emphasizing grasp generation and trajectory generation. Each layer is explored in detail, along with notable works that have advanced the state of the art. Finally, the survey outlines future research directions and challenges, emphasizing the need for improved efficiency in data utilization, better handling of long-horizon tasks, and enhanced generalization across diverse robotic scenarios. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/GAI4Manipulation/AwesomeGAIManipulation"
    },
    {
      "paperId": "a2e30086789e1516a21042ceed3c3fa4ffdb008f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2503-03556",
        "ArXiv": "2503.03556",
        "DOI": "10.48550/arXiv.2503.03556",
        "CorpusId": 276782236
      },
      "title": "Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.03556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348639546",
          "name": "Xiaomeng Zhu"
        },
        {
          "authorId": "2261448933",
          "name": "Yuyang Li"
        },
        {
          "authorId": "2266112512",
          "name": "Leiyao Cui"
        },
        {
          "authorId": "2268023731",
          "name": "Pengfei Li"
        },
        {
          "authorId": "2221148510",
          "name": "Huan-ang Gao"
        },
        {
          "authorId": "2290017516",
          "name": "Yixin Zhu"
        },
        {
          "authorId": "2326064308",
          "name": "Hao Zhao"
        }
      ],
      "abstract": "Object affordance reasoning, the ability to infer object functionalities based on physical properties, is fundamental for task-oriented planning and activities in both humans and Artificial Intelligence (AI). This capability, required for planning and executing daily activities in a task-oriented manner, relies on commonsense knowledge of object physics and functionalities, extending beyond simple object recognition. Current computational models for affordance reasoning from perception lack generalizability, limiting their applicability in novel scenarios. Meanwhile, comprehensive Large Language Models (LLMs) with emerging reasoning capabilities are challenging to deploy on local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance the generalizability of affordance reasoning from perception. Utilizing this dataset, we develop Afford-X, an end-to-end trainable affordance reasoning model that incorporates Verb Attention and Bi-Fusion modules to improve multi-modal understanding. This model achieves up to a 12.1% performance improvement over the best-reported results from non-LLM methods, while also demonstrating a 1.2% enhancement compared to our previous conference paper. Additionally, it maintains a compact 187M parameter size and infers nearly 50 times faster than the GPT-4V API. Our work demonstrates the potential for efficient, generalizable affordance reasoning models that can be deployed on local devices for task-oriented manipulations. We showcase Afford-X's effectiveness in enabling task-oriented manipulations for robots across various tasks and environments, underscoring its efficiency and broad implications for advancing robotics and AI systems in real-world applications."
    },
    {
      "paperId": "56b695cbbd2e5784b25878328d0b5e57a0926591",
      "externalIds": {
        "ArXiv": "2503.02405",
        "DBLP": "journals/corr/abs-2503-02405",
        "DOI": "10.48550/arXiv.2503.02405",
        "CorpusId": 276767801
      },
      "title": "A comparison of visual representations for real-world reinforcement learning in the context of vacuum gripping",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348443395",
          "name": "Nico Sutter"
        },
        {
          "authorId": "2348447151",
          "name": "Valentin N. Hartmann"
        },
        {
          "authorId": "2138444534",
          "name": "Stelian Coros"
        }
      ],
      "abstract": "When manipulating objects in the real world, we need reactive feedback policies that take into account sensor information to inform decisions. This study aims to determine how different encoders can be used in a reinforcement learning (RL) framework to interpret the spatial environment in the local surroundings of a robot arm. Our investigation focuses on comparing real-world vision with 3D scene inputs, exploring new architectures in the process. We built on the SERL framework, providing us with a sample efficient and stable RL foundation we could build upon, while keeping training times minimal. The results of this study indicate that spatial information helps to significantly outperform the visual counterpart, tested on a box picking task with a vacuum gripper. The code and videos of the evaluations are available at https://github.com/nisutte/voxel-serl."
    },
    {
      "paperId": "4a7bf3465bc58be4785c5f57caaeb941c4eb5483",
      "externalIds": {
        "ArXiv": "2503.01439",
        "DBLP": "journals/corr/abs-2503-01439",
        "DOI": "10.48550/arXiv.2503.01439",
        "CorpusId": 276767541
      },
      "title": "AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint and Focal Length Optimization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.01439, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2348411949",
          "name": "Yushan Liu"
        },
        {
          "authorId": "2089563034",
          "name": "Shilong Mu"
        },
        {
          "authorId": "2348439323",
          "name": "Xintao Chao"
        },
        {
          "authorId": "2348664964",
          "name": "Zizhen Li"
        },
        {
          "authorId": "2348606790",
          "name": "Yao Mu"
        },
        {
          "authorId": "2316455829",
          "name": "Tianxing Chen"
        },
        {
          "authorId": "2155443623",
          "name": "Shoujie Li"
        },
        {
          "authorId": "2321455346",
          "name": "Chuqiao Lyu"
        },
        {
          "authorId": "2271214144",
          "name": "Xiao-Ping Zhang"
        },
        {
          "authorId": "2280885422",
          "name": "Wenbo Ding"
        }
      ],
      "abstract": "Robotic manipulation within dynamic environments presents challenges to precise control and adaptability. Traditional fixed-view camera systems face challenges adapting to change viewpoints and scale variations, limiting perception and manipulation precision. To tackle these issues, we propose the Active Vision-driven Robotic (AVR) framework, a teleoperation hardware solution that supports dynamic viewpoint and dynamic focal length adjustments to continuously center targets and maintain optimal scale, accompanied by a corresponding algorithm that effectively enhances the success rates of various operational tasks. Using the RoboTwin platform with a real-time image processing plugin, AVR framework improves task success rates by 5%-16% on five manipulation tasks. Physical deployment on a dual-arm system demonstrates in collaborative tasks and 36% precision in screwdriver insertion, outperforming baselines by over 25%. Experimental results confirm that AVR framework enhances environmental perception, manipulation repeatability (40% $\\le $1 cm error), and robustness in complex scenarios, paving the way for future robotic precision manipulation methods in the pursuit of human-level robot dexterity and precision."
    },
    {
      "paperId": "e3183645bfea3289b272d99430f8008a9bfc0589",
      "externalIds": {
        "DBLP": "journals/corr/abs-2503-00345",
        "ArXiv": "2503.00345",
        "DOI": "10.48550/arXiv.2503.00345",
        "CorpusId": 276741411
      },
      "title": "Towards Understanding the Benefit of Multitask Representation Learning in Decision Process",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.00345, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2301205439",
          "name": "Rui Lu"
        },
        {
          "authorId": "2256993684",
          "name": "Yang Yue"
        },
        {
          "authorId": "2136104377",
          "name": "Andrew Zhao"
        },
        {
          "authorId": "2349750096",
          "name": "Simon Du"
        },
        {
          "authorId": "2350439231",
          "name": "Gao Huang"
        }
      ],
      "abstract": "Multitask Representation Learning (MRL) has emerged as a prevalent technique to improve sample efficiency in Reinforcement Learning (RL). Empirical studies have found that training agents on multiple tasks simultaneously within online and transfer learning environments can greatly improve efficiency. Despite its popularity, a comprehensive theoretical framework that elucidates its operational efficacy remains incomplete. Prior analyses have predominantly assumed that agents either possess a pre-known representation function or utilize functions from a linear class, where both are impractical. The complexity of real-world applications typically requires the use of sophisticated, non-linear functions such as neural networks as representation function, which are not pre-existing but must be learned. Our work tries to fill the gap by extending the analysis to \\textit{unknown non-linear} representations, giving a comprehensive analysis for its mechanism in online and transfer learning setting. We consider the setting that an agent simultaneously playing $M$ contextual bandits (or MDPs), developing a shared representation function $\\phi$ from a non-linear function class $\\Phi$ using our novel Generalized Functional Upper Confidence Bound algorithm (GFUCB). We formally prove that this approach yields a regret upper bound that outperforms the lower bound associated with learning $M$ separate tasks, marking the first demonstration of MRL's efficacy in a general function class. This framework also explains the contribution of representations to transfer learning when faced with new, yet related tasks, and identifies key conditions for successful transfer. Empirical experiments further corroborate our theoretical findings."
    },
    {
      "paperId": "873a65022c91b98ae7d5bfdbe94cdceb8b9db4c6",
      "externalIds": {
        "DBLP": "journals/nn/WangWLWL25",
        "DOI": "10.1016/j.neunet.2025.107342",
        "CorpusId": 276940854,
        "PubMed": "40090299"
      },
      "title": "Rethinking exploration-exploitation trade-off in reinforcement learning via cognitive consistency",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neunet.2025.107342?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neunet.2025.107342, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2111219093",
          "name": "Da Wang"
        },
        {
          "authorId": "2247657064",
          "name": "Weipeng Wei"
        },
        {
          "authorId": "2297283012",
          "name": "Lin Li"
        },
        {
          "authorId": "2350235920",
          "name": "Xin Wang"
        },
        {
          "authorId": "2274437137",
          "name": "Jiye Liang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "add637341443b4980988cd2096074f06019d6341",
      "externalIds": {
        "DBLP": "conf/iclr/KimHLLSLL25",
        "ArXiv": "2502.20630",
        "DOI": "10.48550/arXiv.2502.20630",
        "CorpusId": 276724804
      },
      "title": "Subtask-Aware Visual Reward Learning from Segmented Demonstrations",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20630, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2110230345",
          "name": "Changyeon Kim"
        },
        {
          "authorId": "2218050062",
          "name": "Minho Heo"
        },
        {
          "authorId": "2218134064",
          "name": "Doohyun Lee"
        },
        {
          "authorId": "2261688831",
          "name": "Jinwoo Shin"
        },
        {
          "authorId": "2296302312",
          "name": "Honglak Lee"
        },
        {
          "authorId": "2348185990",
          "name": "Joseph J. Lim"
        },
        {
          "authorId": "3436470",
          "name": "Kimin Lee"
        }
      ],
      "abstract": "Reinforcement Learning (RL) agents have demonstrated their potential across various robotic tasks. However, they still heavily rely on human-engineered reward functions, requiring extensive trial-and-error and access to target behavior information, often unavailable in real-world settings. This paper introduces REDS: REward learning from Demonstration with Segmentations, a novel reward learning framework that leverages action-free videos with minimal supervision. Specifically, REDS employs video demonstrations segmented into subtasks from diverse sources and treats these segments as ground-truth rewards. We train a dense reward function conditioned on video segments and their corresponding subtasks to ensure alignment with ground-truth reward signals by minimizing the Equivalent-Policy Invariant Comparison distance. Additionally, we employ contrastive learning objectives to align video representations with subtasks, ensuring precise subtask inference during online interactions. Our experiments show that REDS significantly outperforms baseline methods on complex robotic manipulation tasks in Meta-World and more challenging real-world tasks, such as furniture assembly in FurnitureBench, with minimal human intervention. Moreover, REDS facilitates generalization to unseen tasks and robot embodiments, highlighting its potential for scalable deployment in diverse environments."
    },
    {
      "paperId": "40d1e0a1e8a861305f9354be747620782fc203ce",
      "externalIds": {
        "DOI": "10.3390/ai6030046",
        "CorpusId": 276590554
      },
      "title": "Deep Reinforcement Learning: A Chronological Overview and Methods",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/ai6030046?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/ai6030046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3161727",
          "name": "Juan R. Terven"
        }
      ],
      "abstract": "Introduction: Deep reinforcement learning (deep RL) integrates the principles of reinforcement learning with deep neural networks, enabling agents to excel in diverse tasks ranging from playing board games such as Go and Chess to controlling robotic systems and autonomous vehicles. By leveraging foundational concepts of value functions, policy optimization, and temporal difference methods, deep RL has rapidly evolved and found applications in areas such as gaming, robotics, finance, and healthcare. Objective: This paper seeks to provide a comprehensive yet accessible overview of the evolution of deep RL and its leading algorithms. It aims to serve both as an introduction for newcomers to the field and as a practical guide for those seeking to select the most appropriate methods for specific problem domains. Methods: We begin by outlining fundamental reinforcement learning principles, followed by an exploration of early tabular Q-learning methods. We then trace the historical development of deep RL, highlighting key milestones such as the advent of deep Q-networks (DQN). The survey extends to policy gradient methods, actor–critic architectures, and state-of-the-art algorithms such as proximal policy optimization, soft actor–critic, and emerging model-based approaches. Throughout, we discuss the current challenges facing deep RL, including issues of sample efficiency, interpretability, and safety, as well as open research questions involving large-scale training, hierarchical architectures, and multi-task learning. Results: Our analysis demonstrates how critical breakthroughs have driven deep RL into increasingly complex application domains. We highlight existing limitations and ongoing bottlenecks, such as high data requirements and the need for more transparent, ethically aligned systems. Finally, we survey potential future directions, highlighting the importance of reliability and ethical considerations for real-world deployments."
    },
    {
      "paperId": "d335b1ab64168530c9ecdf07fcbbf7ec87bea414",
      "externalIds": {
        "DBLP": "journals/corr/abs-2502-17100",
        "ArXiv": "2502.17100",
        "DOI": "10.48550/arXiv.2502.17100",
        "CorpusId": 276574988
      },
      "title": "Generative Models in Decision Making: A Survey",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "47002988",
          "name": "Yinchuan Li"
        },
        {
          "authorId": "2346978956",
          "name": "Xinyu Shao"
        },
        {
          "authorId": "2347180767",
          "name": "Jianping Zhang"
        },
        {
          "authorId": "2328611534",
          "name": "Haozhi Wang"
        },
        {
          "authorId": "2276430050",
          "name": "L. Brunswic"
        },
        {
          "authorId": "2346737740",
          "name": "Kaiwen Zhou"
        },
        {
          "authorId": "2347353629",
          "name": "Jiqian Dong"
        },
        {
          "authorId": "2347011277",
          "name": "Kaiyang Guo"
        },
        {
          "authorId": "2347143646",
          "name": "Xiu Li"
        },
        {
          "authorId": "2330755425",
          "name": "Zhitang Chen"
        },
        {
          "authorId": "2327096473",
          "name": "Jun Wang"
        },
        {
          "authorId": "2327843646",
          "name": "Jianye Hao"
        }
      ],
      "abstract": "In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models."
    },
    {
      "paperId": "ae63d95bf1b98d9263f162a03069533da95953c5",
      "externalIds": {
        "DBLP": "journals/corr/abs-2502-16932",
        "ArXiv": "2502.16932",
        "DOI": "10.48550/arXiv.2502.16932",
        "CorpusId": 276575314
      },
      "title": "DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16932, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2239106154",
          "name": "Zhengrong Xue"
        },
        {
          "authorId": "2293829732",
          "name": "Shuying Deng"
        },
        {
          "authorId": "2321516735",
          "name": "Zhenyang Chen"
        },
        {
          "authorId": "2346981238",
          "name": "Yixuan Wang"
        },
        {
          "authorId": "2156151359",
          "name": "Zhecheng Yuan"
        },
        {
          "authorId": "2239159959",
          "name": "Huazhe Xu"
        }
      ],
      "abstract": "Visuomotor policies have shown great promise in robotic manipulation but often require substantial amounts of human-collected data for effective performance. A key reason underlying the data demands is their limited spatial generalization capability, which necessitates extensive data collection across different object configurations. In this work, we present DemoGen, a low-cost, fully synthetic approach for automatic demonstration generation. Using only one human-collected demonstration per task, DemoGen generates spatially augmented demonstrations by adapting the demonstrated action trajectory to novel object configurations. Visual observations are synthesized by leveraging 3D point clouds as the modality and rearranging the subjects in the scene via 3D editing. Empirically, DemoGen significantly enhances policy performance across a diverse range of real-world manipulation tasks, showing its applicability even in challenging scenarios involving deformable objects, dexterous hand end-effectors, and bimanual platforms. Furthermore, DemoGen can be extended to enable additional out-of-distribution capabilities, including disturbance resistance and obstacle avoidance."
    },
    {
      "paperId": "19b460d0065662f331c06ea05dbe8218d277e346",
      "externalIds": {
        "ArXiv": "2502.13519",
        "DBLP": "journals/corr/abs-2502-13519",
        "DOI": "10.48550/arXiv.2502.13519",
        "CorpusId": 276444697
      },
      "title": "MILE: Model-based Intervention Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13519, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2284986698",
          "name": "Yigit Korkmaz"
        },
        {
          "authorId": "8307674",
          "name": "Erdem Biyik"
        }
      ],
      "abstract": "Imitation learning techniques have been shown to be highly effective in real-world control scenarios, such as robotics. However, these approaches not only suffer from compounding error issues but also require human experts to provide complete trajectories. Although there exist interactive methods where an expert oversees the robot and intervenes if needed, these extensions usually only utilize the data collected during intervention periods and ignore the feedback signal hidden in non-intervention timesteps. In this work, we create a model to formulate how the interventions occur in such cases, and show that it is possible to learn a policy with just a handful of expert interventions. Our key insight is that it is possible to get crucial information about the quality of the current state and the optimality of the chosen action from expert feedback, regardless of the presence or the absence of intervention. We evaluate our method on various discrete and continuous simulation environments, a real-world robotic manipulation task, as well as a human subject study. Videos and the code can be found at https://liralab.usc.edu/mile ."
    },
    {
      "paperId": "6c306b9b3dfe5976a750362b7d569d60af640998",
      "externalIds": {
        "ArXiv": "2502.06051",
        "CorpusId": 276249132
      },
      "title": "Towards a Sharp Analysis of Offline Policy Learning for $f$-Divergence-Regularized Contextual Bandits",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06051, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2344858316",
          "name": "Qingyue Zhao"
        },
        {
          "authorId": "2277448856",
          "name": "Kaixuan Ji"
        },
        {
          "authorId": "2141339020",
          "name": "Heyang Zhao"
        },
        {
          "authorId": "2345000846",
          "name": "Tong Zhang"
        },
        {
          "authorId": "2253398016",
          "name": "Quanquan Gu"
        }
      ],
      "abstract": "Although many popular reinforcement learning algorithms are underpinned by $f$-divergence regularization, their sample complexity with respect to the \\emph{regularized objective} still lacks a tight characterization. In this paper, we analyze $f$-divergence-regularized offline policy learning. For reverse Kullback-Leibler (KL) divergence, arguably the most commonly used one, we give the first $\\tilde{O}(\\epsilon^{-1})$ sample complexity under single-policy concentrability for contextual bandits, surpassing existing $\\tilde{O}(\\epsilon^{-1})$ bound under all-policy concentrability and $\\tilde{O}(\\epsilon^{-2})$ bound under single-policy concentrability. Our analysis for general function approximation leverages the principle of pessimism in the face of uncertainty to refine a mean-value-type argument to its extreme. This in turn leads to a novel moment-based technique, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. We further propose a lower bound, demonstrating that a multiplicative dependency on single-policy concentrability is necessary to maximally exploit the strong convexity of reverse KL. In addition, for $f$-divergences with strongly convex $f$, to which reverse KL \\emph{does not} belong, we show that the sharp sample complexity $\\tilde{\\Theta}(\\epsilon^{-1})$ is achievable even without single-policy concentrability. In this case, the algorithm design can get rid of pessimistic estimators. We further extend our analysis to dueling bandits, and we believe these results take a significant step toward a comprehensive understanding of $f$-divergence-regularized policy learning."
    },
    {
      "paperId": "20df6f8959275e73e688041381e462d2780bd7ae",
      "externalIds": {
        "DBLP": "journals/corr/abs-2502-03270",
        "ArXiv": "2502.03270",
        "DOI": "10.48550/arXiv.2502.03270",
        "CorpusId": 276116283
      },
      "title": "When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.03270, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "6853840",
          "name": "Nikolaos Tsagkas"
        },
        {
          "authorId": "2353032142",
          "name": "Andreas Sochopoulos"
        },
        {
          "authorId": "2051683028",
          "name": "Duolikun Danier"
        },
        {
          "authorId": "2292774928",
          "name": "Chris Xiaoxuan Lu"
        },
        {
          "authorId": "2918822",
          "name": "Oisin Mac Aodha"
        }
      ],
      "abstract": "The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations."
    },
    {
      "paperId": "50b3ea4c92bb6498a23b7e696ef4da75ecbe1ef1",
      "externalIds": {
        "DBLP": "journals/corr/abs-2502-02133",
        "ArXiv": "2502.02133",
        "DOI": "10.48550/arXiv.2502.02133",
        "CorpusId": 276107071
      },
      "title": "Synthesis of Model Predictive Control and Reinforcement Learning: Survey and Classification",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02133, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2067243153",
          "name": "Rudolf Reiter"
        },
        {
          "authorId": "2193555726",
          "name": "Jasper Hoffmann"
        },
        {
          "authorId": "2267973767",
          "name": "Dirk Reinhardt"
        },
        {
          "authorId": "152209644",
          "name": "Florian Messerer"
        },
        {
          "authorId": "51477262",
          "name": "Katrin Baumgärtner"
        },
        {
          "authorId": "2343739773",
          "name": "Shamburaj Sawant"
        },
        {
          "authorId": "2199642320",
          "name": "Joschka Boedecker"
        },
        {
          "authorId": "2303558998",
          "name": "Moritz Diehl"
        },
        {
          "authorId": "2244010986",
          "name": "Sébastien Gros"
        }
      ],
      "abstract": "The fields of MPC and RL consider two successful control techniques for Markov decision processes. Both approaches are derived from similar fundamental principles, and both are widely used in practical applications, including robotics, process control, energy systems, and autonomous driving. Despite their similarities, MPC and RL follow distinct paradigms that emerged from diverse communities and different requirements. Various technical discrepancies, particularly the role of an environment model as part of the algorithm, lead to methodologies with nearly complementary advantages. Due to their orthogonal benefits, research interest in combination methods has recently increased significantly, leading to a large and growing set of complex ideas leveraging MPC and RL. This work illuminates the differences, similarities, and fundamentals that allow for different combination algorithms and categorizes existing work accordingly. Particularly, we focus on the versatile actor-critic RL approach as a basis for our categorization and examine how the online optimization approach of MPC can be used to improve the overall closed-loop performance of a policy."
    },
    {
      "paperId": "78ed3c59818bf5d44cd6f75e23c811cd2f5d2cc0",
      "externalIds": {
        "DBLP": "journals/corr/abs-2502-01616",
        "ArXiv": "2502.01616",
        "DOI": "10.48550/arXiv.2502.01616",
        "CorpusId": 276107940
      },
      "title": "Preference VLM: Leveraging VLMs for Scalable Preference-Based Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.01616, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2324576657",
          "name": "Udita Ghosh"
        },
        {
          "authorId": "30164077",
          "name": "Dripta S. Raychaudhuri"
        },
        {
          "authorId": "2324686339",
          "name": "Jiachen Li"
        },
        {
          "authorId": "2309244861",
          "name": "Konstantinos Karydis"
        },
        {
          "authorId": "2262047475",
          "name": "Amit K. Roy-Chowdhury"
        }
      ],
      "abstract": "Preference-based reinforcement learning (RL) offers a promising approach for aligning policies with human intent but is often constrained by the high cost of human feedback. In this work, we introduce PrefVLM, a framework that integrates Vision-Language Models (VLMs) with selective human feedback to significantly reduce annotation requirements while maintaining performance. Our method leverages VLMs to generate initial preference labels, which are then filtered to identify uncertain cases for targeted human annotation. Additionally, we adapt VLMs using a self-supervised inverse dynamics loss to improve alignment with evolving policies. Experiments on Meta-World manipulation tasks demonstrate that PrefVLM achieves comparable or superior success rates to state-of-the-art methods while using up to 2 x fewer human annotations. Furthermore, we show that adapted VLMs enable efficient knowledge transfer across tasks, further minimizing feedback needs. Our results highlight the potential of combining VLMs with selective human supervision to make preference-based RL more scalable and practical."
    },
    {
      "paperId": "ba1bc45bc25d53d88ebd95ac54c310fe5ad5e14f",
      "externalIds": {
        "DOI": "10.1109/AI2E64943.2025.10983455",
        "CorpusId": 278544921
      },
      "title": "Exploring the Multifaceted Applications of Artificial Intelligence in Earthquake Engineering: A Comprehensive Review of Literature, Techniques, and Future Trends",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AI2E64943.2025.10983455?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AI2E64943.2025.10983455, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2188462334",
          "name": "Anas Ansari"
        },
        {
          "authorId": "2360782639",
          "name": "Abdullah Ansari"
        }
      ],
      "abstract": "This comprehensive review investigates the multifaceted applications of Artificial Intelligence (AI) within the field of earthquake engineering, highlighting its transformative potential and diverse methodologies. As seismic events pose significant risks to infrastructure and public safety, the integration of AI technologies has emerged as a vital approach to enhancing earthquake resilience. This study synthesizes existing literature to explore various AI applications, including Machine Learning (ML), Deep Learning (DL), and data-driven techniques, in critical areas such as seismic hazard assessment, structural health monitoring, damage detection, and post-earthquake recovery. The findings aim to provide a holistic understanding of how AI can revolutionize earthquake engineering practices, ultimately contributing to improved safety and resilience in regions vulnerable to seismic activity. Through this exploration, the paper emphasizes the need for continued research and innovation in AI applications to mitigate the impacts of earthquakes effectively."
    },
    {
      "paperId": "fcd612dd83d2e26f5548c4c545960590631063a6",
      "externalIds": {
        "DBLP": "journals/ras/SoualhiCLRG25",
        "DOI": "10.1016/j.robot.2025.104920",
        "CorpusId": 276388246
      },
      "title": "Leveraging motion perceptibility and deep reinforcement learning for visual control of nonholonomic mobile robots",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.robot.2025.104920?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.robot.2025.104920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2313115465",
          "name": "Takieddine Soualhi"
        },
        {
          "authorId": "2490230",
          "name": "Nathan Crombez"
        },
        {
          "authorId": "2307009889",
          "name": "Alexandre Lombard"
        },
        {
          "authorId": "2257249325",
          "name": "Yassine Ruichek"
        },
        {
          "authorId": "2268589951",
          "name": "Stéphane Galland"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "9371e492fb89125eafadaf47f8082cf2df7f8f84",
      "externalIds": {
        "DBLP": "journals/ijon/PanZGPWYGLCLHDZZLGC25",
        "DOI": "10.1016/j.neucom.2025.129644",
        "CorpusId": 276436720
      },
      "title": "Morphology generalizable reinforcement learning via multi-level graph features",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neucom.2025.129644?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neucom.2025.129644, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2293569231",
          "name": "Yansong Pan"
        },
        {
          "authorId": "2118404461",
          "name": "Rui Zhang"
        },
        {
          "authorId": "47093626",
          "name": "Jiaming Guo"
        },
        {
          "authorId": "2072713784",
          "name": "Shaohui Peng"
        },
        {
          "authorId": "2264621137",
          "name": "Fan Wu"
        },
        {
          "authorId": "2142841221",
          "name": "Kaizhao Yuan"
        },
        {
          "authorId": "2117957883",
          "name": "Yunkai Gao"
        },
        {
          "authorId": "2219976844",
          "name": "Siming Lan"
        },
        {
          "authorId": "2118229954",
          "name": "Rui Chen"
        },
        {
          "authorId": "2319805216",
          "name": "Ling Li"
        },
        {
          "authorId": "2109635961",
          "name": "Xingui Hu"
        },
        {
          "authorId": "1678776",
          "name": "Zidong Du"
        },
        {
          "authorId": "2303434266",
          "name": "Zihao Zhang"
        },
        {
          "authorId": "2346720755",
          "name": "Xin Zhang"
        },
        {
          "authorId": "2256598373",
          "name": "Wei Li"
        },
        {
          "authorId": "2266139212",
          "name": "Qi Guo"
        },
        {
          "authorId": "2261914661",
          "name": "Yunji Chen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4c2e1d89f0990cea66fe0be20b80ef2e8c2e4d09",
      "externalIds": {
        "DOI": "10.1016/j.jhydrol.2025.132895",
        "CorpusId": 276520486
      },
      "title": "Enhancing representation of data-scarce reservoir-regulated river basins using a hybrid DL-process based approach",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.jhydrol.2025.132895?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.jhydrol.2025.132895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "50143475",
          "name": "Liang Deng"
        },
        {
          "authorId": "2303407019",
          "name": "Xiang Zhang"
        },
        {
          "authorId": "2237147232",
          "name": "L. Slater"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1053a3c5bced66e4ce07aec4e50b47539cd836b9",
      "externalIds": {
        "DBLP": "journals/corr/abs-2501-19116",
        "ArXiv": "2501.19116",
        "DOI": "10.48550/arXiv.2501.19116",
        "CorpusId": 276079876
      },
      "title": "A Theoretical Justification for Asymmetric Actor-Critic Algorithms",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.19116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2131349333",
          "name": "Gaspard Lambrechts"
        },
        {
          "authorId": "2307449037",
          "name": "Damien Ernst"
        },
        {
          "authorId": "2343506858",
          "name": "Aditya Mahajan"
        }
      ],
      "abstract": "In reinforcement learning for partially observable environments, many successful algorithms have been developed within the asymmetric learning paradigm. This paradigm leverages additional state information available at training time for faster learning. Although the proposed learning objectives are usually theoretically sound, these methods still lack a precise theoretical justification for their potential benefits. We propose such a justification for asymmetric actor-critic algorithms with linear function approximators by adapting a finite-time convergence analysis to this setting. The resulting finite-time bound reveals that the asymmetric critic eliminates error terms arising from aliasing in the agent state."
    },
    {
      "paperId": "ce7957628b6aa45f3db29ddc1ed013122cf05e44",
      "externalIds": {
        "DBLP": "journals/corr/abs-2501-17667",
        "ArXiv": "2501.17667",
        "DOI": "10.48550/arXiv.2501.17667",
        "CorpusId": 275954142
      },
      "title": "CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.17667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2309079614",
          "name": "Derui Wang"
        },
        {
          "authorId": "2309005534",
          "name": "Kristen Moore"
        },
        {
          "authorId": "2342685426",
          "name": "Diksha Goel"
        },
        {
          "authorId": "2309120486",
          "name": "Minjune Kim"
        },
        {
          "authorId": "2342885295",
          "name": "Gang Li"
        },
        {
          "authorId": "2344188318",
          "name": "Yang Li"
        },
        {
          "authorId": "2342685790",
          "name": "Robin Doss"
        },
        {
          "authorId": "2257208795",
          "name": "Minhui Xue"
        },
        {
          "authorId": "2304873862",
          "name": "Bo Li"
        },
        {
          "authorId": "1811469",
          "name": "S. Çamtepe"
        },
        {
          "authorId": "2297517695",
          "name": "Liming Zhu"
        }
      ],
      "abstract": "Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training. Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl."
    },
    {
      "paperId": "e3b3311afc0e305747162d8b61305d5763da7818",
      "externalIds": {
        "ArXiv": "2501.14377",
        "DBLP": "journals/corr/abs-2501-14377",
        "DOI": "10.48550/arXiv.2501.14377",
        "CorpusId": 275907125
      },
      "title": "Dream to Fly: Model-Based Reinforcement Learning for Vision-Based Drone Flight",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.14377, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2069137528",
          "name": "Angel Romero"
        },
        {
          "authorId": "2194650970",
          "name": "Ashwin Shenai"
        },
        {
          "authorId": "2335672940",
          "name": "Ismail Geles"
        },
        {
          "authorId": "35402354",
          "name": "Elie Aljalbout"
        },
        {
          "authorId": "2239202737",
          "name": "Davide Scaramuzza"
        }
      ],
      "abstract": "Autonomous drone racing has risen as a challenging robotic benchmark for testing the limits of learning, perception, planning, and control. Expert human pilots are able to agilely fly a drone through a race track by mapping the real-time feed from a single onboard camera directly to control commands. Recent works in autonomous drone racing attempting direct pixel-to-commands control policies (without explicit state estimation) have relied on either intermediate representations that simplify the observation space or performed extensive bootstrapping using Imitation Learning (IL). This paper introduces an approach that learns policies from scratch, allowing a quadrotor to autonomously navigate a race track by directly mapping raw onboard camera pixels to control commands, just as human pilots do. By leveraging model-based reinforcement learning~(RL) - specifically DreamerV3 - we train visuomotor policies capable of agile flight through a race track using only raw pixel observations. While model-free RL methods such as PPO struggle to learn under these conditions, DreamerV3 efficiently acquires complex visuomotor behaviors. Moreover, because our policies learn directly from pixel inputs, the perception-aware reward term employed in previous RL approaches to guide the training process is no longer needed. Our experiments demonstrate in both simulation and real-world flight how the proposed approach can be deployed on agile quadrotors. This approach advances the frontier of vision-based autonomous flight and shows that model-based RL is a promising direction for real-world robotics."
    },
    {
      "paperId": "54eba3f516b28b439a69a6189a42417fe7f05bc5",
      "externalIds": {
        "DBLP": "conf/sii/TamakiNYDO25",
        "DOI": "10.1109/SII59315.2025.10870593",
        "CorpusId": 276289257
      },
      "title": "Predicting human behavior using knowledge information in jig operation and robot collaborative action generation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/SII59315.2025.10870593?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/SII59315.2025.10870593, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2345014940",
          "name": "Mone Tamaki"
        },
        {
          "authorId": "3411577",
          "name": "Ryoichi Nakajo"
        },
        {
          "authorId": "1696822",
          "name": "N. Yamanobe"
        },
        {
          "authorId": "2512607",
          "name": "Y. Domae"
        },
        {
          "authorId": "2283496159",
          "name": "Tetsuya Ogata"
        }
      ],
      "abstract": "In human-robot collaborative tasks, learning-based models that can deal with behavior beyond the scope of human description are progressing rapidly. Deep learning is effective in capturing complex nonlinear relationships, making it valuable in scenarios with intricate interactions between the environment and tasks, such as collaborative tasks. Deep learning improves the performance by incorporating multiple information sources. Human knowledge, which is regarded as supplemental information obtained from the environment, has been shown to enhance the generalization ability of task execution when it is appropriately incorporated into the learning process for robot motion generation. Among the various models, those that utilize action labels subjectively defined by humans for robot behavior enable the robot to comprehend its own actions better, leading to higher generalization. This approach also suggests that estimating human actions contributes to predicting robot movements in human-robot collaboration (HRC). However, the performance of learning-based methods is significantly influenced by the quality of the training data. Therefore, capturing appropriate human information and integrating this information into the learning process are critical for improving the ability of the robot to learn collaborative tasks. In this study, we propose a learning model that not only provides a robot with action labels for its own behavior but also includes human action labels, encouraging the robot to respond to human actions. The optimal amount of human information to be used in learning is evaluated by adjusting the methods for defining human action labels and the quantity of human data utilized. Experiments were conducted with a task in which the robot handled the manipulation of jigs in an assembly operation involving both humans and robots. The results of the learning process suggest that estimating human behavior can assist in generating collaborative robot actions."
    },
    {
      "paperId": "1976dd56cc0b986b181025343f9f714987184576",
      "externalIds": {
        "ArXiv": "2501.10869",
        "DBLP": "conf/hri/Martin-OzimekJM25a",
        "DOI": "10.1109/HRI61500.2025.10973990",
        "CorpusId": 275757972
      },
      "title": "Diffusion-Based Imitation Learning for Social Pose Generation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.10869, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2341330263",
          "name": "Antonio Lech Martin-Ozimek"
        },
        {
          "authorId": "9396481",
          "name": "Isuru Jayarathne"
        },
        {
          "authorId": "2201772859",
          "name": "Su Larb Mon"
        },
        {
          "authorId": "2065741",
          "name": "Jouh Yeong Chew"
        }
      ],
      "abstract": "Intelligent agents, such as robots and virtual agents, must understand the dynamics of complex social interactions to interact with humans. Effectively representing social dynamics is challenging because we require multi-modal, synchronized observations to understand a scene. We explore how using a single modality, the pose behavior, of multiple individuals in a social interaction can be used to generate nonverbal social cues for the facilitator of that interaction. The facilitator acts to make a social interaction proceed smoothly and is an essential role for intelligent agents to replicate in human-robot interactions. In this paper, we adapt an existing diffusion behavior cloning model to learn and replicate facilitator behaviors. Furthermore, we evaluate two representations of pose observations from a scene, one representation has pre-processing applied and one does not. The purpose of this paper is to introduce a new use for diffusion behavior cloning for pose generation in social interactions. The second is to understand the relationship between performance and computational load for generating social pose behavior using two different techniques for collecting scene observations. As such, we are essentially testing the effectiveness of two different types of conditioning for a diffusion model. We then evaluate the resulting generated behavior from each technique using quantitative measures such as mean per-joint position error (MPJPE), training time, and inference time. Additionally, we plot training and inference time against MPJPE to examine the trade-offs between efficiency and performance. Our results suggest that the further pre-processed data can successfully condition diffusion models to generate realistic social behavior, with reasonable trade-offs in accuracy and processing time. Future work will focus on extending this approach to generate multiple nonverbal social cues, on generalizing this method to multiple types of social activity, and on evaluating the results using human evaluators using a virtual agent or robot as a facilitator."
    },
    {
      "paperId": "5c0c62a4a5b7ab7ba4b86e848f9cdab4e699eb38",
      "externalIds": {
        "DBLP": "journals/corr/abs-2501-10100",
        "ArXiv": "2501.10100",
        "DOI": "10.48550/arXiv.2501.10100",
        "CorpusId": 275606663
      },
      "title": "Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.10100, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2340932099",
          "name": "Chenhao Li"
        },
        {
          "authorId": "2340685384",
          "name": "Andreas Krause"
        },
        {
          "authorId": "2340685198",
          "name": "Marco Hutter"
        }
      ],
      "abstract": "Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications."
    },
    {
      "paperId": "f29d1283fc60e1dbb84981548e73e0a03a86cf8c",
      "externalIds": {
        "DOI": "10.1109/ICMERR64601.2025.10949903",
        "CorpusId": 277676765
      },
      "title": "A Comprehensive Review of Robotics Advancements Through Imitation Learning for Self-Learning Systems",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICMERR64601.2025.10949903?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICMERR64601.2025.10949903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1729599654",
          "name": "Yagna Jadeja"
        },
        {
          "authorId": "2354666321",
          "name": "Mahmoud Shafik"
        },
        {
          "authorId": "2317212076",
          "name": "Paul Wood"
        },
        {
          "authorId": "51391650",
          "name": "Aaisha Makkar"
        }
      ],
      "abstract": "In recent years, robotics and artificial intelligence (AI) have witnessed significant growth, particularly in self-learning systems. This paper examines the remarkable progress made in this area, with a particular focus on the utilisation of imitation learning. Self-learning robotics systems have demonstrated the autonomous acquisition of new skills, making them highly adaptable and versatile. Imitation learning is a crucial technique that allows robots to gain knowledge from human demonstrations. This paradigm allows machines to learn and replicate human actions, thus enhancing the capabilities of self-learning robotic technology. The primary objective of this research was to investigate the potential of imitation learning and evaluate its impact on the advancement of self-learning robotics. This paper provides a comprehensive overview of self-learning robotic systems using imitation learning, examining the foundational concepts, essential methodologies, and various applications in this intriguing area. Furthermore, we highlight recent developments, discuss current trends, and outline potential research initiatives to guide the continued development of self-learning robotic systems using imitation learning. This review aims to contribute to the evolving landscape of autonomous robotics by consolidating knowledge, identifying challenges, and fostering further innovation in the pursuit of intelligent self-learning machines."
    },
    {
      "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
      "externalIds": {
        "DBLP": "conf/cvpr/HeZRS16",
        "MAG": "2949650786",
        "ArXiv": "1512.03385",
        "DOI": "10.1109/cvpr.2016.90",
        "CorpusId": 206594692
      },
      "title": "Deep Residual Learning for Image Recognition",
      "openAccessPdf": {
        "url": "https://repositorio.unal.edu.co/bitstream/unal/81443/1/98670607.2022.pdf",
        "status": "GREEN",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1512.03385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "39353098",
          "name": "Kaiming He"
        },
        {
          "authorId": "1771551",
          "name": "X. Zhang"
        },
        {
          "authorId": "3080683",
          "name": "Shaoqing Ren"
        },
        {
          "authorId": null,
          "name": "Jian Sun"
        }
      ],
      "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
    },
    {
      "paperId": "f03b4ff1b4943691cec703b508c0a91f2d97a881",
      "externalIds": {
        "MAG": "2201912979",
        "DBLP": "journals/corr/PintoG15",
        "ArXiv": "1509.06825",
        "DOI": "10.1109/ICRA.2016.7487517",
        "CorpusId": 3177253
      },
      "title": "Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours",
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/1509.06825",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.06825, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "34026610",
          "name": "Lerrel Pinto"
        },
        {
          "authorId": "1726095131",
          "name": "A. Gupta"
        }
      ],
      "abstract": "Current model free learning-based robot grasping approaches exploit human-labeled datasets for training the models. However, there are two problems with such a methodology: (a) since each object can be grasped in multiple ways, manually labeling grasp locations is not a trivial task; (b) human labeling is biased by semantics. While there have been attempts to train robots using trial-and-error experiments, the amount of data used in such experiments remains substantially low and hence makes the learner prone to over-fitting. In this paper, we take the leap of increasing the available training data to 40 times more than prior work, leading to a dataset size of 50K data points collected over 700 hours of robot grasping attempts. This allows us to train a Convolutional Neural Network (CNN) for the task of predicting grasp locations without severe overfitting. In our formulation, we recast the regression problem to an 18-way binary classification over image patches. We also present a multi-stage learning approach where a CNN trained in one stage is used to collect hard negatives in subsequent stages. Our experiments clearly show the benefit of using large-scale datasets (and multi-stage training) for the task of grasping. We also compare to several baselines and show state-of-the-art performance on generalization to unseen objects for grasping."
    },
    {
      "paperId": "c8f41160130980c1ffead5a812cf2b3c6b03049f",
      "externalIds": {
        "DBLP": "conf/icra/FinnTDDLA16",
        "MAG": "2210483910",
        "ArXiv": "1509.06113",
        "DOI": "10.1109/ICRA.2016.7487173",
        "CorpusId": 1616437
      },
      "title": "Deep spatial autoencoders for visuomotor learning",
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1509.06113",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.06113, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "46881670",
          "name": "Chelsea Finn"
        },
        {
          "authorId": "48391424",
          "name": "X. Tan"
        },
        {
          "authorId": "144581158",
          "name": "Yan Duan"
        },
        {
          "authorId": "1753210",
          "name": "Trevor Darrell"
        },
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "1689992",
          "name": "P. Abbeel"
        }
      ],
      "abstract": "Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm."
    },
    {
      "paperId": "6e9f1f05b7bef4b5afaf25df6952bc6bf0662179",
      "externalIds": {
        "MAG": "2266673418",
        "DBLP": "journals/corr/FinnTDDLA15",
        "CorpusId": 16981636
      },
      "title": "Learning Visual Feature Spaces for Robotic Manipulation with Deep Spatial Autoencoders",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "46881670",
          "name": "Chelsea Finn"
        },
        {
          "authorId": "48391424",
          "name": "X. Tan"
        },
        {
          "authorId": "144581158",
          "name": "Yan Duan"
        },
        {
          "authorId": "1753210",
          "name": "Trevor Darrell"
        },
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "1689992",
          "name": "P. Abbeel"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "024006d4c2a89f7acacc6e4438d156525b60a98f",
      "externalIds": {
        "MAG": "2173248099",
        "ArXiv": "1509.02971",
        "DBLP": "journals/corr/LillicrapHPHETS15",
        "CorpusId": 16326763
      },
      "title": "Continuous control with deep reinforcement learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1509.02971, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2542999",
          "name": "T. Lillicrap"
        },
        {
          "authorId": "2323922",
          "name": "Jonathan J. Hunt"
        },
        {
          "authorId": "1863250",
          "name": "A. Pritzel"
        },
        {
          "authorId": "2801204",
          "name": "N. Heess"
        },
        {
          "authorId": "1968210",
          "name": "Tom Erez"
        },
        {
          "authorId": "2109481",
          "name": "Yuval Tassa"
        },
        {
          "authorId": "145824029",
          "name": "David Silver"
        },
        {
          "authorId": "1688276",
          "name": "D. Wierstra"
        }
      ],
      "abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs."
    },
    {
      "paperId": "e89d656a39fc3b08af47ebb9a583e182a596dabe",
      "externalIds": {
        "MAG": "2281096776",
        "DBLP": "conf/rss/LenzKS15",
        "DOI": "10.15607/RSS.2015.XI.012",
        "CorpusId": 10130184
      },
      "title": "DeepMPC: Learning Deep Latent Features for Model Predictive Control",
      "openAccessPdf": {
        "url": "https://doi.org/10.15607/rss.2015.xi.012",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15607/RSS.2015.XI.012?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15607/RSS.2015.XI.012, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1966934",
          "name": "Ian Lenz"
        },
        {
          "authorId": "2678110",
          "name": "Ross A. Knepper"
        },
        {
          "authorId": "1681995",
          "name": "Ashutosh Saxena"
        }
      ],
      "abstract": "Designing controllers for tasks with complex nonlinear dynamics is extremely challenging, time-consuming, and in many cases, infeasible. This difficulty is exacerbated in tasks such as robotic food-cutting, in which dynamics might vary both with environmental properties, such as material and tool class, and with time while acting. In this work, we present DeepMPC, an online real-time model-predictive control approach designed to handle such difficult tasks. Rather than hand-design a dynamics model for the task, our approach uses a novel deep architecture and learning algorithm, learning controllers for complex tasks directly from data. We validate our method in experiments on a large-scale dataset of 1488 material cuts for 20 diverse classes, and in 450 real-world robotic experiments, demonstrating significant improvement over several other approaches."
    },
    {
      "paperId": "bb1a17010254abfa5e1f2a17553582ce449f8e16",
      "externalIds": {
        "MAG": "2950564096",
        "DBLP": "conf/nips/WatterSBR15",
        "ArXiv": "1506.07365",
        "CorpusId": 1731857
      },
      "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1506.07365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2409233",
          "name": "M. Watter"
        },
        {
          "authorId": "2060551",
          "name": "Jost Tobias Springenberg"
        },
        {
          "authorId": "145581493",
          "name": "J. Boedecker"
        },
        {
          "authorId": "3137672",
          "name": "Martin A. Riedmiller"
        }
      ],
      "abstract": "We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems."
    },
    {
      "paperId": "08883c7ca6a500342975e01cc5efc4b45e43ebec",
      "externalIds": {
        "DBLP": "conf/aistats/Hoof0N15",
        "MAG": "314779054",
        "CorpusId": 14478766
      },
      "title": "Learning of Non-Parametric Control Policies with High-Dimensional State Features",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "47662867",
          "name": "H. V. Hoof"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        },
        {
          "authorId": "26599977",
          "name": "G. Neumann"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "bb97f7ab1bbd36a6c2a18889f75efae414cf4752",
      "externalIds": {
        "ArXiv": "1504.03071",
        "DBLP": "journals/corr/SungJS15",
        "MAG": "2952083707",
        "DOI": "10.1007/978-3-319-60916-4_40",
        "CorpusId": 7082854
      },
      "title": "Robobarista: Object Part Based Transfer of Manipulation Trajectories from Crowd-Sourcing in 3D Pointclouds",
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1504.03071",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1504.03071, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1720619",
          "name": "Jaeyong Sung"
        },
        {
          "authorId": "23154729",
          "name": "Seok Hyun Jin"
        },
        {
          "authorId": "1681995",
          "name": "Ashutosh Saxena"
        }
      ],
      "abstract": "There is a large variety of objects and appliances in human environments, such as stoves, coffee dispensers, juice extractors, and so on. It is challenging for a roboticist to program a robot for each of these object types and for each of their instantiations. In this work, we present a novel approach to manipulation planning based on the idea that many household objects share similarly-operated object parts. We formulate the manipulation planning as a structured prediction problem and design a deep learning model that can handle large noise in the manipulation demonstrations and learns features from three different modalities: point-clouds, language and trajectory. In order to collect a large number of manipulation demonstrations for different objects, we developed a new crowd-sourcing platform called Robobarista. We test our model on our dataset consisting of 116 objects with 249 parts along with 250 language instructions, for which there are 1225 crowd-sourced manipulation demonstrations. We further show that our robot can even manipulate objects it has never seen before."
    },
    {
      "paperId": "d388e15a41b98f5fde97bd6a50f73aa57d6e7801",
      "externalIds": {
        "DBLP": "conf/icra/LevineWA15",
        "MAG": "2952842465",
        "ArXiv": "1501.05611",
        "DOI": "10.1109/ICRA.2015.7138994",
        "CorpusId": 13097121
      },
      "title": "Learning contact-rich manipulation skills with guided policy search",
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/1501.05611",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1501.05611, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "40234261",
          "name": "Nolan Wagener"
        },
        {
          "authorId": "1689992",
          "name": "P. Abbeel"
        }
      ],
      "abstract": "Autonomous learning of object manipulation skills can enable robots to acquire rich behavioral repertoires that scale to the variety of objects found in the real world. However, current motion skill learning methods typically restrict the behavior to a compact, low-dimensional representation, limiting its expressiveness and generality. In this paper, we extend a recently developed policy search method [1] and use it to learn a range of dynamic manipulation behaviors with highly general policy representations, without using known models or example demonstrations. Our approach learns a set of trajectories for the desired motion skill by using iteratively refitted time-varying linear models, and then unifies these trajectories into a single control policy that can generalize to new situations. To enable this method to run on a real robot, we introduce several improvements that reduce the sample count and automate parameter selection. We show that our method can acquire fast, fluent behaviors after only minutes of interaction time, and can learn robust controllers for complex tasks, including putting together a toy airplane, stacking tight-fitting lego blocks, placing wooden rings onto tight-fitting pegs, inserting a shoe tree into a shoe, and screwing bottle caps onto bottles."
    },
    {
      "paperId": "d0c61536927c2f5dc2ddb74664268a3623580b9c",
      "externalIds": {
        "MAG": "2121103318",
        "DBLP": "conf/nips/LevineA14",
        "CorpusId": 2341332
      },
      "title": "Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "1689992",
          "name": "P. Abbeel"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "b6cc21b30912bdaecd9f178d700a4c545b1d0838",
      "externalIds": {
        "MAG": "2151210636",
        "DBLP": "conf/nips/GuoSLLW14",
        "CorpusId": 2187487
      },
      "title": "Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1955964",
          "name": "Xiaoxiao Guo"
        },
        {
          "authorId": "1699868",
          "name": "Satinder Singh"
        },
        {
          "authorId": "1697141",
          "name": "Honglak Lee"
        },
        {
          "authorId": "46328485",
          "name": "Richard L. Lewis"
        },
        {
          "authorId": "2107975180",
          "name": "Xiaoshi Wang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "125d262f1c66f75f3af9c3c8759abcc02ebb4ef9",
      "externalIds": {
        "MAG": "2001095967",
        "DBLP": "conf/icra/LioutikovPPN14",
        "DOI": "10.1109/ICRA.2014.6907424",
        "CorpusId": 11838528
      },
      "title": "Sample-based informationl-theoretic stochastic optimal control",
      "openAccessPdf": {
        "url": "http://eprints.lincoln.ac.uk/id/eprint/25771/1/Lioutikov_ICRA_2014.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA.2014.6907424?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA.2014.6907424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2931067",
          "name": "Rudolf Lioutikov"
        },
        {
          "authorId": "1762536",
          "name": "A. Paraschos"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        },
        {
          "authorId": "26599977",
          "name": "G. Neumann"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
      "externalIds": {
        "DBLP": "journals/corr/SzegedyLJSRAEVR14",
        "MAG": "2097117768",
        "ArXiv": "1409.4842",
        "DOI": "10.1109/CVPR.2015.7298594",
        "CorpusId": 206592484
      },
      "title": "Going deeper with convolutions",
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/1409.4842",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1409.4842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2574060",
          "name": "Christian Szegedy"
        },
        {
          "authorId": "2157222093",
          "name": "Wei Liu"
        },
        {
          "authorId": "39978391",
          "name": "Yangqing Jia"
        },
        {
          "authorId": "3142556",
          "name": "P. Sermanet"
        },
        {
          "authorId": "144828948",
          "name": "Scott E. Reed"
        },
        {
          "authorId": "1838674",
          "name": "Dragomir Anguelov"
        },
        {
          "authorId": "1761978",
          "name": "D. Erhan"
        },
        {
          "authorId": "2657155",
          "name": "Vincent Vanhoucke"
        },
        {
          "authorId": "39863668",
          "name": "Andrew Rabinovich"
        }
      ],
      "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
    },
    {
      "paperId": "2a1d2ccf4f9d63ecdd5f7373a04926a33f8d9e5d",
      "externalIds": {
        "DBLP": "conf/rss/MordatchT14",
        "MAG": "2295431040",
        "DOI": "10.15607/RSS.2014.X.052",
        "CorpusId": 5564734
      },
      "title": "Combining the benefits of function approximation and trajectory optimization",
      "openAccessPdf": {
        "url": "https://doi.org/10.15607/rss.2014.x.052",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15607/RSS.2014.X.052?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15607/RSS.2014.X.052, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2316241382",
          "name": "Igor Mordatch"
        },
        {
          "authorId": "144832491",
          "name": "E. Todorov"
        }
      ],
      "abstract": "Neural networks have recently solved many hard problems in Machine Learning, but their impact in control remains limited. Trajectory optimization has recently solved many hard problems in robotic control, but using it online remains challenging. Here we leverage the high-fidelity solutions obtained by trajectory optimization to speed up the training of neural network controllers. The two learning problems are coupled using the Alternating Direction Method of Multipliers (ADMM). This coupling enables the trajectory optimizer to act as a teacher, gradually guiding the network towards better solutions. We develop a new trajectory optimizer based on inverse contact dynamics, and provide not only the trajectories but also the feedback gains as training data to the network. The method is illustrated on rolling, reaching, swimming and walking tasks."
    },
    {
      "paperId": "83b35ec3ca272ae0f583297d13d1de4e007ef3c7",
      "externalIds": {
        "DBLP": "conf/rss/JonschkowskiB14",
        "MAG": "2296691800",
        "DOI": "10.15607/RSS.2014.X.019",
        "CorpusId": 5467016
      },
      "title": "State Representation Learning in Robotics: Using Prior Knowledge about Physical Interaction",
      "openAccessPdf": {
        "url": "https://doi.org/10.15607/rss.2014.x.019",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15607/RSS.2014.X.019?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15607/RSS.2014.X.019, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2699042",
          "name": "Rico Jonschkowski"
        },
        {
          "authorId": "1717724",
          "name": "O. Brock"
        }
      ],
      "abstract": "State representations critically affect the effectiveness of learning in robots. In this paper, we propose a roboticsspecific approach to learning such state representations. Robots accomplish tasks by interacting with the physical world. Physics in turn imposes structure on both the changes in the world and on the way robots can effect these changes. Using prior knowledge about interacting with the physical world, robots can learn state representations that are consistent with physics. We identify five robotic priors and explain how they can be used for representation learning. We demonstrate the effectiveness of this approach in a simulated slot car racing task and a simulated navigation task with distracting moving objects. We show that our method extracts task-relevant state representations from highdimensional observations, even in the presence of task-irrelevant distractions. We also show that the state representations learned by our method greatly improve generalization in reinforcement learning."
    },
    {
      "paperId": "0db78a2047517227ca70b194fc02c9b12281dfce",
      "externalIds": {
        "MAG": "2147032798",
        "DBLP": "conf/icml/LevineK14",
        "CorpusId": 9719859
      },
      "title": "Learning Complex Neural Network Policies with Trajectory Optimization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "145231047",
          "name": "V. Koltun"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "6bdb186ec4726e00a8051119636d4df3b94043b5",
      "externalIds": {
        "MAG": "2950094539",
        "DBLP": "journals/corr/JiaSDKLGGD14",
        "ArXiv": "1408.5093",
        "DOI": "10.1145/2647868.2654889",
        "CorpusId": 1799558
      },
      "title": "Caffe: Convolutional Architecture for Fast Feature Embedding",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1408.5093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "39978391",
          "name": "Yangqing Jia"
        },
        {
          "authorId": "1782282",
          "name": "Evan Shelhamer"
        },
        {
          "authorId": "7408951",
          "name": "Jeff Donahue"
        },
        {
          "authorId": "3049736",
          "name": "Sergey Karayev"
        },
        {
          "authorId": "2117314646",
          "name": "Jonathan Long"
        },
        {
          "authorId": "2983898",
          "name": "Ross B. Girshick"
        },
        {
          "authorId": "1687120",
          "name": "S. Guadarrama"
        },
        {
          "authorId": "1753210",
          "name": "Trevor Darrell"
        }
      ],
      "abstract": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
    },
    {
      "paperId": "12ecc2d786080f638a01b9999518e9386baa157d",
      "externalIds": {
        "ArXiv": "1406.2984",
        "DBLP": "journals/corr/TompsonJLB14",
        "MAG": "2952422028",
        "CorpusId": 392527
      },
      "title": "Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1406.2984, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2704494",
          "name": "Jonathan Tompson"
        },
        {
          "authorId": "49147969",
          "name": "Arjun Jain"
        },
        {
          "authorId": "1688882",
          "name": "Yann LeCun"
        },
        {
          "authorId": "2428034",
          "name": "C. Bregler"
        }
      ],
      "abstract": "This paper proposes a new hybrid architecture that consists of a deep Convolu-tional Network and a Markov Random Field. We show how this architecture is successfully applied to the challenging problem of articulated human pose estimation in monocular images. The architecture can exploit structural domain constraints such as geometric relationships between body joint locations. We show that joint training of these two model paradigms improves performance and allows us to significantly outperform existing state-of-the-art techniques."
    },
    {
      "paperId": "72b71e08399226858dac8ef9ed2a90289b89f140",
      "externalIds": {
        "DBLP": "conf/icra/MohtaKD14",
        "MAG": "2155238998",
        "DOI": "10.1109/ICRA.2014.6907309",
        "CorpusId": 15051436
      },
      "title": "Vision-based control of a quadrotor for perching on lines",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICRA.2014.6907309?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICRA.2014.6907309, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1899477",
          "name": "K. Mohta"
        },
        {
          "authorId": "37956314",
          "name": "Vijay R. Kumar"
        },
        {
          "authorId": "1751586",
          "name": "Kostas Daniilidis"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "193edd20cae92c6759c18ce93eeea96afd9528eb",
      "externalIds": {
        "MAG": "2076063813",
        "ArXiv": "1404.7828",
        "DBLP": "journals/nn/Schmidhuber15",
        "DOI": "10.1016/j.neunet.2014.09.003",
        "CorpusId": 11715509,
        "PubMed": "25462637"
      },
      "title": "Deep learning in neural networks: An overview",
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1404.7828",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1404.7828, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145341374",
          "name": "J. Schmidhuber"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2319a491378867c7049b3da055c5df60e1671158",
      "externalIds": {
        "DBLP": "journals/corr/MnihKSGAWR13",
        "MAG": "1757796397",
        "ArXiv": "1312.5602",
        "CorpusId": 15238391
      },
      "title": "Playing Atari with Deep Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1312.5602, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3255983",
          "name": "Volodymyr Mnih"
        },
        {
          "authorId": "2645384",
          "name": "K. Kavukcuoglu"
        },
        {
          "authorId": "145824029",
          "name": "David Silver"
        },
        {
          "authorId": "1753223",
          "name": "Alex Graves"
        },
        {
          "authorId": "2460849",
          "name": "Ioannis Antonoglou"
        },
        {
          "authorId": "1688276",
          "name": "D. Wierstra"
        },
        {
          "authorId": "3137672",
          "name": "Martin A. Riedmiller"
        }
      ],
      "abstract": "We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them."
    },
    {
      "paperId": "ca7f25d5b139c684a8d477e954380138dcba3a73",
      "externalIds": {
        "MAG": "2098284983",
        "DBLP": "conf/nips/LevineK13",
        "CorpusId": 5941161
      },
      "title": "Variational Policy Search via Trajectory Optimization",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "145231047",
          "name": "V. Koltun"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
      "externalIds": {
        "DBLP": "journals/corr/GirshickDDM13",
        "MAG": "2951638509",
        "ArXiv": "1311.2524",
        "DOI": "10.1109/CVPR.2014.81",
        "CorpusId": 215827080
      },
      "title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/1311.2524",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1311.2524, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2983898",
          "name": "Ross B. Girshick"
        },
        {
          "authorId": "7408951",
          "name": "Jeff Donahue"
        },
        {
          "authorId": "1753210",
          "name": "Trevor Darrell"
        },
        {
          "authorId": "153652147",
          "name": "J. Malik"
        }
      ],
      "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
    },
    {
      "paperId": "65438e0ba226c1f97bd8a36333ebc3297b1a32fd",
      "externalIds": {
        "DBLP": "journals/ijrr/KoberBP13",
        "MAG": "1977655452",
        "DOI": "10.1177/0278364913495721",
        "CorpusId": 1932843
      },
      "title": "Reinforcement learning in robotics: A survey",
      "openAccessPdf": {
        "url": "https://figshare.com/articles/journal_contribution/Reinforcement_Learning_in_Robotics_A_Survey/6560648/1/files/12042932.pdf",
        "status": "GREEN",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1177/0278364913495721?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/0278364913495721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145739642",
          "name": "Jens Kober"
        },
        {
          "authorId": "1756566",
          "name": "J. Bagnell"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        }
      ],
      "abstract": "Reinforcement learning offers to robotics a framework and set of tools for the design of sophisticated and hard-to-engineer behaviors. Conversely, the challenges of robotic problems provide both inspiration, impact, and validation for developments in reinforcement learning. The relationship between disciplines has sufficient promise to be likened to that between physics and mathematics. In this article, we attempt to strengthen the links between the two research communities by providing a survey of work in reinforcement learning for behavior generation in robots. We highlight both key challenges in robot reinforcement learning as well as notable successes. We discuss how contributions tamed the complexity of the domain and study the role of algorithms, representations, and prior knowledge in achieving these successes. As a result, a particular focus of our paper lies on the choice between model-based and model-free as well as between value-function-based and policy-search methods. By analyzing a simple problem in some detail we demonstrate how reinforcement learning approaches may be profitably applied, and we note throughout open questions and the tremendous potential for future research."
    },
    {
      "paperId": "b6bfae6efa1110a57a4d8362721d152d78aae358",
      "externalIds": {
        "MAG": "2012587148",
        "DBLP": "journals/ftrob/DeisenrothNP13",
        "DOI": "10.1561/2300000021",
        "CorpusId": 17647665
      },
      "title": "A Survey on Policy Search for Robotics",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1561/2300000021?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1561/2300000021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261881",
          "name": "M. Deisenroth"
        },
        {
          "authorId": "26599977",
          "name": "G. Neumann"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        }
      ],
      "abstract": "Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning.Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems."
    },
    {
      "paperId": "f22b01deebcd471ccee8a3039a6f0fd09ff78b03",
      "externalIds": {
        "DBLP": "conf/ijcnn/LampeR13",
        "MAG": "2006273250",
        "DOI": "10.1109/IJCNN.2013.6707053",
        "CorpusId": 13932056
      },
      "title": "Acquiring visual servoing reaching and grasping skills using neural reinforcement learning",
      "openAccessPdf": {
        "url": "http://ml.informatik.uni-freiburg.de/_media/publications/lampe2013.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN.2013.6707053?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN.2013.6707053, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2066153554",
          "name": "Thomas Lampe"
        },
        {
          "authorId": "3137672",
          "name": "Martin A. Riedmiller"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5ff5e41617829b090c649eacb2ca3277a820dbd8",
      "externalIds": {
        "MAG": "2038794597",
        "DBLP": "conf/gecco/KoutnikCSG13",
        "DOI": "10.1145/2463372.2463509",
        "CorpusId": 3840949
      },
      "title": "Evolving large-scale neural networks for vision-based reinforcement learning",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/2463372.2463509?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/2463372.2463509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2865775",
          "name": "Jan Koutník"
        },
        {
          "authorId": "2004477",
          "name": "Giuseppe Cuccu"
        },
        {
          "authorId": "145341374",
          "name": "J. Schmidhuber"
        },
        {
          "authorId": "145842938",
          "name": "Faustino J. Gomez"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "244539f454800697ed663326b7cfba337ca0c2ec",
      "externalIds": {
        "DBLP": "conf/icml/LevineK13",
        "MAG": "2104733512",
        "CorpusId": 13971447
      },
      "title": "Guided Policy Search",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1736651",
          "name": "S. Levine"
        },
        {
          "authorId": "145231047",
          "name": "V. Koltun"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "cdfcac1e3291b10424b5858ac0ecc9daf702efc4",
      "externalIds": {
        "MAG": "2150695437",
        "ArXiv": "1306.3203",
        "DBLP": "journals/corr/WangB13b",
        "CorpusId": 9890331
      },
      "title": "Bregman Alternating Direction Method of Multipliers",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1306.3203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3909454",
          "name": "Huahua Wang"
        },
        {
          "authorId": "144205696",
          "name": "A. Banerjee"
        }
      ],
      "abstract": "The mirror descent algorithm (MDA) generalizes gradient descent by using a Bregman divergence to replace squared Euclidean distance. In this paper, we similarly generalize the alternating direction method of multipliers (ADMM) to Bregman ADMM (BADMM), which allows the choice of different Bregman divergences to exploit the structure of problems. BADMM provides a unified framework for ADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM. We establish the global convergence and the O(1/T) iteration complexity for BADMM. In some cases, BADMM can be faster than ADMM by a factor of O(n/ln n) where n is the dimensionality. In solving the linear program of mass transportation problem, BADMM leads to massive parallelism and can easily run on GPU. BADMM is several times faster than highly optimized commercial software Gurobi."
    },
    {
      "paperId": "9b223c8a31e0ea1d1f2c9787ffd8416dfc90c912",
      "externalIds": {
        "MAG": "2088049833",
        "DBLP": "journals/ijcv/UijlingsSGS13",
        "DOI": "10.1007/s11263-013-0620-5",
        "CorpusId": 216077384
      },
      "title": "Selective Search for Object Recognition",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/s11263-013-0620-5?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/s11263-013-0620-5, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2242508491",
          "name": "Jasper R. R. Uijlings"
        },
        {
          "authorId": "1756979",
          "name": "K. V. D. Sande"
        },
        {
          "authorId": "2257152645",
          "name": "Theo Gevers"
        },
        {
          "authorId": "144638781",
          "name": "A. Smeulders"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "260b98e772c4785fa06a5e8fe1c205eb05ec01e2",
      "externalIds": {
        "MAG": "1999156278",
        "DBLP": "conf/rss/LenzLS13",
        "ArXiv": "1301.3592",
        "DOI": "10.1177/0278364914549607",
        "CorpusId": 5240721
      },
      "title": "Deep learning for detecting robotic grasps",
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1301.3592",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1301.3592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1966934",
          "name": "Ian Lenz"
        },
        {
          "authorId": "1697141",
          "name": "Honglak Lee"
        },
        {
          "authorId": "1681995",
          "name": "Ashutosh Saxena"
        }
      ],
      "abstract": "We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects. In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features. This presents two main challenges. First, we need to evaluate a huge number of candidate grasps. In order to make detection fast and robust, we present a two-step cascaded system with two deep networks, where the top detections from the first are re-evaluated by the second. The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps. The second, with more features, is slower but has to run only on the top few detections. Second, we need to handle multimodal inputs effectively, for which we present a method that applies structured regularization on the weights based on multimodal group regularization. We show that our method improves performance on an RGBD robotic grasping dataset, and can be used to successfully execute grasps on two different robotic platforms."
    },
    {
      "paperId": "b354ee518bfc1ac0d8ac447eece9edb69e92eae1",
      "externalIds": {
        "DBLP": "conf/iros/TodorovET12",
        "MAG": "2158782408",
        "DOI": "10.1109/IROS.2012.6386109",
        "CorpusId": 5230692
      },
      "title": "MuJoCo: A physics engine for model-based control",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS.2012.6386109?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS.2012.6386109, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "144832491",
          "name": "E. Todorov"
        },
        {
          "authorId": "1968210",
          "name": "Tom Erez"
        },
        {
          "authorId": "2109481",
          "name": "Yuval Tassa"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
      "externalIds": {
        "DBLP": "conf/nips/KrizhevskySH12",
        "MAG": "2618530766",
        "DOI": "10.1145/3065386",
        "CorpusId": 195908774
      },
      "title": "ImageNet classification with deep convolutional neural networks",
      "openAccessPdf": {
        "url": "http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3065386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3065386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2064160",
          "name": "A. Krizhevsky"
        },
        {
          "authorId": "1701686",
          "name": "I. Sutskever"
        },
        {
          "authorId": "1695689",
          "name": "Geoffrey E. Hinton"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "84069287da0a6b488b8c933f3cb5be759cb6237e",
      "externalIds": {
        "ArXiv": "1211.5063",
        "DBLP": "conf/icml/PascanuMB13",
        "MAG": "2949190276",
        "CorpusId": 14650762
      },
      "title": "On the difficulty of training recurrent neural networks",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1211.5063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1996134",
          "name": "Razvan Pascanu"
        },
        {
          "authorId": "2047446108",
          "name": "Tomas Mikolov"
        },
        {
          "authorId": "1751762",
          "name": "Yoshua Bengio"
        }
      ],
      "abstract": "There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section."
    },
    {
      "paperId": "0f0d11429e5aaecbc9fce8445afaa3bad7a74888",
      "externalIds": {
        "MAG": "1980969546",
        "DBLP": "conf/icra/RossMSWDBH13",
        "ArXiv": "1211.1690",
        "DOI": "10.1109/ICRA.2013.6630809",
        "CorpusId": 479635
      },
      "title": "Learning monocular reactive UAV control in cluttered natural environments",
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1211.1690",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1211.1690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1700433",
          "name": "Stéphane Ross"
        },
        {
          "authorId": "1432683555",
          "name": "Narek Melik-Barkhudarov"
        },
        {
          "authorId": "2737958",
          "name": "Kumar Shaurya Shankar"
        },
        {
          "authorId": "35246669",
          "name": "Andreas Wendel"
        },
        {
          "authorId": "1780951",
          "name": "Debadeepta Dey"
        },
        {
          "authorId": "1756566",
          "name": "J. Bagnell"
        },
        {
          "authorId": "145670946",
          "name": "M. Hebert"
        }
      ],
      "abstract": "Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly straight-forward, as expensive sensors and monitoring devices can be employed. In contrast, obstacle avoidance remains a challenging task for Micro Aerial Vehicles (MAVs) which operate at low altitude in cluttered environments. Unlike large vehicles, MAVs can only carry very light sensors, such as cameras, making autonomous navigation through obstacles much more challenging. In this paper, we describe a system that navigates a small quadrotor helicopter autonomously at low altitude through natural forest environments. Using only a single cheap camera to perceive the environment, we are able to maintain a constant velocity of up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent state-of-the-art imitation learning techniques to train a controller that can avoid trees by adapting the MAVs heading. We demonstrate the performance of our system in a more controlled environment indoors, and in real natural forest environments outdoors."
    },
    {
      "paperId": "8bfda7a7f9c1483e2d51ed13ab9c21dc10392d95",
      "externalIds": {
        "ArXiv": "1206.4621",
        "DBLP": "journals/corr/abs-1206-4621",
        "MAG": "2098524868",
        "CorpusId": 15093996
      },
      "title": "Path Integral Policy Improvement with Covariance Matrix Adaptation",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1206.4621, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "50707365",
          "name": "F. Stulp"
        },
        {
          "authorId": "3211142",
          "name": "Olivier Sigaud"
        }
      ],
      "abstract": "There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies. PI2 is a recent example of this approach. It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory. In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function. We compare PI2 to other members of the same family - Cross-Entropy Methods and CMAES - at the conceptual level and in terms of performance. The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for \"Path Integral Policy Improvement with Covariance Matrix Adaptation\". PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically."
    },
    {
      "paperId": "b977da44048fca64acfc66f96bb31682b41bebbb",
      "externalIds": {
        "MAG": "1964201035",
        "DBLP": "conf/cvpr/PepikSGS12",
        "DOI": "10.1109/CVPR.2012.6248075",
        "CorpusId": 580391
      },
      "title": "Teaching 3D geometry to deformable part models",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2012.6248075?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2012.6248075, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3002099",
          "name": "Bojan Pepik"
        },
        {
          "authorId": "144421225",
          "name": "Michael Stark"
        },
        {
          "authorId": "2871555",
          "name": "Peter Gehler"
        },
        {
          "authorId": "48920094",
          "name": "B. Schiele"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "52449f97e09c7465adbc1d4f16e063802d392530",
      "externalIds": {
        "DBLP": "conf/ijcnn/LangeRV12",
        "MAG": "1968962398",
        "DOI": "10.1109/IJCNN.2012.6252823",
        "CorpusId": 17023340
      },
      "title": "Autonomous reinforcement learning on raw visual input data in a real world application",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IJCNN.2012.6252823?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IJCNN.2012.6252823, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "144202584",
          "name": "S. Lange"
        },
        {
          "authorId": "3137672",
          "name": "Martin A. Riedmiller"
        },
        {
          "authorId": "2122171",
          "name": "Arne Voigtländer"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "398c296d0cc7f9d180f84969f8937e6d3a413796",
      "externalIds": {
        "MAG": "2141125852",
        "DBLP": "conf/cvpr/CiresanMS12",
        "ArXiv": "1202.2745",
        "DOI": "10.1109/CVPR.2012.6248110",
        "CorpusId": 2161592
      },
      "title": "Multi-column deep neural networks for image classification",
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/1202.2745",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1202.2745, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1895356",
          "name": "D. Ciresan"
        },
        {
          "authorId": "2514691",
          "name": "U. Meier"
        },
        {
          "authorId": "145341374",
          "name": "J. Schmidhuber"
        }
      ],
      "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks."
    },
    {
      "paperId": "6658bbf68995731b2083195054ff45b4eca38b3a",
      "externalIds": {
        "MAG": "2147768505",
        "DBLP": "journals/taslp/DahlYDA12",
        "DOI": "10.1109/TASL.2011.2134090",
        "CorpusId": 14862572
      },
      "title": "Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition",
      "openAccessPdf": {
        "url": "http://www.cs.toronto.edu/%7Egdahl/papers/DRAFT_DBN4LVCSR-TransASLP.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TASL.2011.2134090?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TASL.2011.2134090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "35188630",
          "name": "George E. Dahl"
        },
        {
          "authorId": "144580027",
          "name": "Dong Yu"
        },
        {
          "authorId": "144718788",
          "name": "L. Deng"
        },
        {
          "authorId": "1723644",
          "name": "A. Acero"
        }
      ],
      "abstract": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively."
    },
    {
      "paperId": "aaf42313fb89aa731f538c0066f34e8cac4dd878",
      "externalIds": {
        "MAG": "3146846077",
        "DBLP": "conf/iros/KalakrishnanRPS11",
        "DOI": "10.1109/IROS.2011.6095096",
        "CorpusId": 7272500
      },
      "title": "Learning force control policies for compliant manipulation",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS.2011.6095096?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS.2011.6095096, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1729262",
          "name": "Mrinal Kalakrishnan"
        },
        {
          "authorId": "51204760",
          "name": "L. Righetti"
        },
        {
          "authorId": "143970835",
          "name": "P. Pastor"
        },
        {
          "authorId": "1745219",
          "name": "S. Schaal"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "8e2c9cddd9317ded47bce49e96bda2f9d55a70db",
      "externalIds": {
        "MAG": "2117494236",
        "DBLP": "journals/jair/JodogneP07",
        "ArXiv": "1110.2210",
        "DOI": "10.1613/jair.2110",
        "CorpusId": 989450
      },
      "title": "Closed-Loop Learning of Visual Control Policies",
      "openAccessPdf": {
        "url": "https://jair.org/index.php/jair/article/download/10491/25139",
        "status": "GOLD",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1110.2210, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [],
      "abstract": "In this paper we present a general, flexible framework for learning mappings from images to actions by interacting with the environment. The basic idea is to introduce a feature-based image classifier in front of a reinforcement learning algorithm. The classifier partitions the visual space according to the presence or absence of few highly informative local descriptors that are incrementally selected in a sequence of attempts to remove perceptual aliasing. We also address the problem of fighting overfitting in such a greedy algorithm. Finally, we show how high-level visual features can be generated when the power of local descriptors is insufficient for completely disambiguating the aliased states. This is done by building a hierarchy of composite features that consist of recursive spatial combinations of visual features. We demonstrate the efficacy of our algorithms by solving three visual navigation tasks and a visual version of the classical \"Car on the Hill\" control problem."
    },
    {
      "paperId": "60b7d47758a71978e74edff6dd8dea4d9c791d7a",
      "externalIds": {
        "DBLP": "conf/icml/DeisenrothR11",
        "MAG": "2140135625",
        "CorpusId": 14273320
      },
      "title": "PILCO: A Model-Based and Data-Efficient Approach to Policy Search",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "2261881",
          "name": "M. Deisenroth"
        },
        {
          "authorId": "3472959",
          "name": "C. Rasmussen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "4f185ec16ce9c4e2d01d4acb0f9b46fe91b1b1eb",
      "externalIds": {
        "MAG": "1520597402",
        "DBLP": "conf/rss/DeisenrothRF11",
        "DOI": "10.15607/RSS.2011.VII.008",
        "CorpusId": 16758749
      },
      "title": "Learning to Control a Low-Cost Manipulator using Data-Efficient Reinforcement Learning",
      "openAccessPdf": {
        "url": "https://doi.org/10.7551/mitpress/9481.003.0013",
        "status": "BRONZE",
        "license": "CCBY",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.15607/RSS.2011.VII.008?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.15607/RSS.2011.VII.008, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2261881",
          "name": "M. Deisenroth"
        },
        {
          "authorId": "3472959",
          "name": "C. Rasmussen"
        },
        {
          "authorId": "145197953",
          "name": "D. Fox"
        }
      ],
      "abstract": "Over the last years, there has been substantial progress in robust manipulation in unstructured environments. The long-term goal of our work is to get away from precise, but very expensive robotic systems and to develop affordable, potentially imprecise, self-adaptive manipulator systems that can interactively perform tasks such as playing with children. In this paper, we demonstrate how a low-cost off-the-shelf robotic system can learn closed-loop policies for a stacking task in only a handful of trials-from scratch. Our manipulator is inaccurate and provides no pose feedback. For learning a controller in the work space of a Kinect-style depth camera, we use a model-based reinforcement learning technique. Our learning method is data efficient, reduces model bias, and deals with several noise sources in a principled way during long-term planning. We present a way of incorporating state-space constraints into the learning process and analyze the learning gain by exploiting the sequential structure of the stacking task."
    },
    {
      "paperId": "85e4dbcff0b63773db298562ae3fff258eea195f",
      "externalIds": {
        "DBLP": "journals/ftml/BoydPCPE11",
        "MAG": "2164278908",
        "CorpusId": 51789432
      },
      "title": "Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [],
      "abstract": null
    },
    {
      "paperId": "79ab3c49903ec8cb339437ccf5cf998607fc313e",
      "externalIds": {
        "MAG": "2962957031",
        "ArXiv": "1011.0686",
        "DBLP": "journals/jmlr/RossGB11",
        "CorpusId": 103456
      },
      "title": "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1011.0686, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1700433",
          "name": "Stéphane Ross"
        },
        {
          "authorId": "21889436",
          "name": "Geoffrey J. Gordon"
        },
        {
          "authorId": "1756566",
          "name": "J. Bagnell"
        }
      ],
      "abstract": "Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem."
    },
    {
      "paperId": "f0640da2565bad7037d969e9f07276c10102083d",
      "externalIds": {
        "MAG": "1555385401",
        "DBLP": "conf/eccv/EndresH10",
        "DOI": "10.1007/978-3-642-15555-0_42",
        "CorpusId": 697224
      },
      "title": "Category Independent Object Proposals",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-642-15555-0_42?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-3-642-15555-0_42, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2831988",
          "name": "Ian Endres"
        },
        {
          "authorId": "2433269",
          "name": "Derek Hoiem"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "e0456b744af84f07cb5e750217463214f96c921e",
      "externalIds": {
        "DBLP": "conf/aaai/PetersMA10",
        "MAG": "1499669280",
        "DOI": "10.1609/aaai.v24i1.7727",
        "CorpusId": 2984847
      },
      "title": "Relative Entropy Policy Search",
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7727/7588",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v24i1.7727?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v24i1.7727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        },
        {
          "authorId": "1759719",
          "name": "Katharina Muelling"
        },
        {
          "authorId": "1783941",
          "name": "Y. Altun"
        }
      ],
      "abstract": "\n \n Policy search is a successful approach to reinforcement learning. However, policy improvements often result in the loss of information. Hence, it has been marred by premature convergence and implausible solutions. As first suggested in the context of covariant policy gradients, many of these problems may be addressed by constraining the information loss. In this paper, we continue this path of reasoning and suggest the Relative Entropy Policy Search (REPS) method. The resulting method differs significantly from previous policy gradient approaches and yields an exact update step. It can be shown to work well on typical reinforcement learning benchmark problems.\n \n"
    },
    {
      "paperId": "9b925d8c60f53e929f2651c2ea24713f820d9feb",
      "externalIds": {
        "DBLP": "conf/icra/MeeussenWGCMMMMEFHRMBKGB10",
        "MAG": "2016505519",
        "DOI": "10.1109/ROBOT.2010.5509556",
        "CorpusId": 14986228
      },
      "title": "Autonomous door opening and plugging in with a personal robot",
      "openAccessPdf": {
        "url": "http://www.willowgarage.com/sites/default/files/m2.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2010.5509556?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2010.5509556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2536582",
          "name": "W. Meeussen"
        },
        {
          "authorId": "35212077",
          "name": "M. Wise"
        },
        {
          "authorId": "2066064686",
          "name": "Stuart Glaser"
        },
        {
          "authorId": "1780110",
          "name": "Sachin Chitta"
        },
        {
          "authorId": "34903673",
          "name": "Conor McGann"
        },
        {
          "authorId": "2294116",
          "name": "P. Mihelich"
        },
        {
          "authorId": "1403822361",
          "name": "Eitan Marder-Eppstein"
        },
        {
          "authorId": "2658890",
          "name": "Marius Muja"
        },
        {
          "authorId": "1694382",
          "name": "V. Eruhimov"
        },
        {
          "authorId": "50056152",
          "name": "Tully Foote"
        },
        {
          "authorId": "32537578",
          "name": "John M. Hsu"
        },
        {
          "authorId": "2689311",
          "name": "R. Rusu"
        },
        {
          "authorId": "1711416",
          "name": "B. Marthi"
        },
        {
          "authorId": "1720184",
          "name": "Gary R. Bradski"
        },
        {
          "authorId": "70162540",
          "name": "K. Konolige"
        },
        {
          "authorId": "3087910",
          "name": "Brian P. Gerkey"
        },
        {
          "authorId": "2055515913",
          "name": "Eric Berger"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0f32846ca519e1966de3dbfecacf7188347b8c7b",
      "externalIds": {
        "MAG": "2042882799",
        "DBLP": "conf/icra/TheodorouBS10",
        "DOI": "10.1109/ROBOT.2010.5509336",
        "CorpusId": 15116370
      },
      "title": "Reinforcement learning of motor skills in high dimensions: A path integral approach",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2010.5509336?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2010.5509336, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1751063",
          "name": "Evangelos A. Theodorou"
        },
        {
          "authorId": "1741293",
          "name": "J. Buchli"
        },
        {
          "authorId": "1745219",
          "name": "S. Schaal"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1cacfca823b940c9741f244229fcf11741b0e278",
      "externalIds": {
        "DBLP": "conf/icra/Khansari-ZadehB10",
        "MAG": "2027457629",
        "DOI": "10.1109/ROBOT.2010.5510001",
        "CorpusId": 15930717
      },
      "title": "BM: An iterative algorithm to learn stable non-linear dynamical systems with Gaussian mixture models",
      "openAccessPdf": {
        "url": "http://infoscience.epfl.ch/record/143445/files/Khansari_Billard_ICRA10_FinalVersion_1.pdf?version=1",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2010.5510001?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2010.5510001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "69986851",
          "name": "S. M. Khansari-Zadeh"
        },
        {
          "authorId": "1807928",
          "name": "A. Billard"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2039f94c8d8342e1e5d36e9cb525cc1f172ba8ed",
      "externalIds": {
        "DBLP": "conf/icra/KoberMKLSP10",
        "MAG": "1982803779",
        "DOI": "10.1109/ROBOT.2010.5509672",
        "CorpusId": 14995854
      },
      "title": "Movement templates for learning of hitting and batting",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2010.5509672?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2010.5509672, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145739642",
          "name": "Jens Kober"
        },
        {
          "authorId": "1759719",
          "name": "Katharina Muelling"
        },
        {
          "authorId": "1785853",
          "name": "Oliver Kroemer"
        },
        {
          "authorId": "1787591",
          "name": "Christoph H. Lampert"
        },
        {
          "authorId": "1707625",
          "name": "B. Scholkopf"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
      "externalIds": {
        "DBLP": "conf/cvpr/DengDSLL009",
        "MAG": "2108598243",
        "DOI": "10.1109/CVPR.2009.5206848",
        "CorpusId": 57246310
      },
      "title": "ImageNet: A large-scale hierarchical image database",
      "openAccessPdf": {
        "url": "http://www.image-net.org/papers/imagenet_cvpr09.pdf",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CVPR.2009.5206848?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CVPR.2009.5206848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "153302678",
          "name": "Jia Deng"
        },
        {
          "authorId": "144847596",
          "name": "Wei Dong"
        },
        {
          "authorId": "2166511",
          "name": "R. Socher"
        },
        {
          "authorId": "2040091191",
          "name": "Li-Jia Li"
        },
        {
          "authorId": "94451829",
          "name": "K. Li"
        },
        {
          "authorId": "48004138",
          "name": "Li Fei-Fei"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "1e80f755bcbf10479afd2338cec05211fdbd325c",
      "externalIds": {
        "MAG": "2130325614",
        "DBLP": "conf/icml/LeeGRN09",
        "DOI": "10.1145/1553374.1553453",
        "CorpusId": 12008458
      },
      "title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1553374.1553453?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1553374.1553453, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1697141",
          "name": "Honglak Lee"
        },
        {
          "authorId": "1785346",
          "name": "R. Grosse"
        },
        {
          "authorId": "2615814",
          "name": "R. Ranganath"
        },
        {
          "authorId": "34699434",
          "name": "A. Ng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "220ad0189a4c9ee5b5c299f269a0e4bef290e8fd",
      "externalIds": {
        "MAG": "2161395589",
        "DBLP": "conf/icra/PastorHAS09",
        "DOI": "10.1109/ROBOT.2009.5152385",
        "CorpusId": 8011664
      },
      "title": "Learning and generalization of motor skills by learning from demonstration",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2009.5152385?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2009.5152385, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "143970835",
          "name": "P. Pastor"
        },
        {
          "authorId": "145983695",
          "name": "Heiko Hoffmann"
        },
        {
          "authorId": "1722677",
          "name": "T. Asfour"
        },
        {
          "authorId": "1745219",
          "name": "S. Schaal"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "0645c8792d5095f0de45e95c5fd9de5238f426e6",
      "externalIds": {
        "MAG": "2147162091",
        "DBLP": "conf/icra/KoberP09",
        "DOI": "10.1109/ROBOT.2009.5152577",
        "CorpusId": 15756501
      },
      "title": "Learning motor primitives for robotics",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2009.5152577?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2009.5152577, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145739642",
          "name": "Jens Kober"
        },
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a90998e0023db48b207cee3b39b0441b3935aaa7",
      "externalIds": {
        "MAG": "2024139303",
        "DOI": "10.1002/ROB.V26:2",
        "CorpusId": 116322669
      },
      "title": "Learning long-range vision for autonomous off-road driving",
      "openAccessPdf": {
        "url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20280",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1002/ROB.V26:2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1002/ROB.V26:2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2315504",
          "name": "R. Hadsell"
        },
        {
          "authorId": "3142556",
          "name": "P. Sermanet"
        },
        {
          "authorId": "144050629",
          "name": "J. Ben"
        },
        {
          "authorId": "1767093",
          "name": "A. Erkan"
        },
        {
          "authorId": "2085624",
          "name": "Marco Scoffier"
        },
        {
          "authorId": "2645384",
          "name": "K. Kavukcuoglu"
        },
        {
          "authorId": "145636949",
          "name": "Urs Muller"
        },
        {
          "authorId": "1688882",
          "name": "Yann LeCun"
        }
      ],
      "abstract": "Most vision-based approaches to mobile robotics suffer from the limitations imposed by stereo obstacle detection, which is short range and prone to failure. We present a self-supervised learning process for long-range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the learning process is due to the self-supervised training data that are generated on every frame: robust, visually consistent labels from a stereo module; normalized wide-context input windows; and a discriminative and concise feature representation. A deep hierarchical network is trained to extract informative and meaningful features from an input image, and the features are used to train a real-time classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to more than 100 m, far beyond the maximum stereo range of 12 m, and adapts very quickly to new environments. The process was developed and tested on the LAGR (Learning Applied to Ground Robots) mobile robot. Results from a ground truth data set, as well as field test results, are given. © 2009 Wiley Periodicals, Inc."
    },
    {
      "paperId": "3e4a739b9792b186342f6dd1c5e5156e1beb3dd9",
      "externalIds": {
        "MAG": "2121196653",
        "DBLP": "conf/icra/WyrobekBLS08",
        "DOI": "10.1109/ROBOT.2008.4543527",
        "CorpusId": 17282001
      },
      "title": "Towards a personal robotics development platform: Rationale and design of an intrinsically safe personal robot",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2008.4543527?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2008.4543527, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2237690088",
          "name": "Keenan A. Wyrobek"
        },
        {
          "authorId": "2237691643",
          "name": "Eric H. Berger"
        },
        {
          "authorId": "2255429377",
          "name": "H.F. Machiel Van der Loos"
        },
        {
          "authorId": "2237691347",
          "name": "J. K. Salisbury"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ffced5b53ad956474a12d73b5cbfd38355dfb70a",
      "externalIds": {
        "MAG": "2125612430",
        "DBLP": "journals/nn/PetersS08",
        "DOI": "10.1016/j.neunet.2008.02.003",
        "CorpusId": 15454626,
        "PubMed": "18482830"
      },
      "title": "Reinforcement learning of motor skills with policy gradients",
      "openAccessPdf": {
        "url": "http://www-clmc.usc.edu/publications/P/peters-NN2008.pdf",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/j.neunet.2008.02.003?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/j.neunet.2008.02.003, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        },
        {
          "authorId": "1745219",
          "name": "S. Schaal"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "a96800c7b8e693f82a925298b9c177fa9b05e1fc",
      "externalIds": {
        "MAG": "2051620263",
        "DBLP": "journals/ijrr/EndoMMNC08",
        "DOI": "10.1177/0278364907084980",
        "CorpusId": 52862711
      },
      "title": "Learning CPG-based Biped Locomotion with a Policy Gradient Method: Application to a Humanoid Robot",
      "openAccessPdf": {
        "url": "https://mediatum.ub.tum.de/doc/1545400/document.pdf",
        "status": "GREEN",
        "license": "other-oa",
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1177/0278364907084980?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1177/0278364907084980, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2266899",
          "name": "G. Endo"
        },
        {
          "authorId": "50414121",
          "name": "J. Morimoto"
        },
        {
          "authorId": "3248224",
          "name": "Takamitsu Matsubara"
        },
        {
          "authorId": "1692765",
          "name": "J. Nakanishi"
        },
        {
          "authorId": "1704474",
          "name": "G. Cheng"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5540bfd2b60fabcf767d08f2434d45a707ef917d",
      "externalIds": {
        "DBLP": "conf/iccv/SavareseF07",
        "MAG": "2123456673",
        "DOI": "10.1109/ICCV.2007.4408987",
        "CorpusId": 16934204
      },
      "title": "3D generic object categorization, localization and pose estimation",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICCV.2007.4408987?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICCV.2007.4408987, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1702137",
          "name": "S. Savarese"
        },
        {
          "authorId": "48004138",
          "name": "Li Fei-Fei"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "56632f37604eb35586d1f72a75b95e940eec0354",
      "externalIds": {
        "MAG": "1533535072",
        "DBLP": "conf/esann/PetersS07",
        "CorpusId": 14044676
      },
      "title": "Applying the Episodic Natural Actor-Critic Architecture to Motor Primitive Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "145197867",
          "name": "Jan Peters"
        },
        {
          "authorId": "1745219",
          "name": "S. Schaal"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "46f5b539cb51319a9f1dc3c59350e8f456877763",
      "externalIds": {
        "MAG": "1964502429",
        "DBLP": "journals/ar/MayerGWNKS08",
        "DOI": "10.1163/156855308X360604",
        "CorpusId": 12284900
      },
      "title": "A System for Robotic Heart Surgery that Learns to Tie Knots Using Recurrent Neural Networks",
      "openAccessPdf": {
        "url": "http://mediatum.ub.tum.de/doc/1292052/document.pdf",
        "status": "GREEN",
        "license": "other-oa",
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1163/156855308X360604?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1163/156855308X360604, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "34398722",
          "name": "H. Mayer"
        },
        {
          "authorId": "145842938",
          "name": "Faustino J. Gomez"
        },
        {
          "authorId": "1688276",
          "name": "D. Wierstra"
        },
        {
          "authorId": "49226534",
          "name": "I. Nagy"
        },
        {
          "authorId": "143873832",
          "name": "A. Knoll"
        },
        {
          "authorId": "145341374",
          "name": "J. Schmidhuber"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "ccd64fac85a1766c660c1d2e0eee0904b3d6139d",
      "externalIds": {
        "DBLP": "conf/nips/GengPW05",
        "MAG": "2156377376",
        "CorpusId": 1556618
      },
      "title": "Fast biped walking with a reflexive controller and real-time policy searching",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1977250",
          "name": "T. Geng"
        },
        {
          "authorId": "2728662",
          "name": "B. Porr"
        },
        {
          "authorId": "1714016",
          "name": "F. Wörgötter"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "92740d5a42268afec52f9d8a549cdb2559d68178",
      "externalIds": {
        "DBLP": "conf/nips/EngelSV05",
        "MAG": "2159008857",
        "CorpusId": 14334246
      },
      "title": "Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "2057050",
          "name": "Y. Engel"
        },
        {
          "authorId": "2073004840",
          "name": "P. Szabó"
        },
        {
          "authorId": "3076448",
          "name": "Dmitry Volkinshtein"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "57f25eb66bf97e15fdac0912a671a61e666ec6ee",
      "externalIds": {
        "MAG": "2109008048",
        "DBLP": "conf/iros/TedrakeZS04",
        "DOI": "10.1109/IROS.2004.1389841",
        "CorpusId": 2808397
      },
      "title": "Stochastic policy gradient reinforcement learning on a simple 3D biped",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS.2004.1389841?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS.2004.1389841, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1726802",
          "name": "Russ Tedrake"
        },
        {
          "authorId": "2110105347",
          "name": "T. Zhang"
        },
        {
          "authorId": "2875182",
          "name": "H. Seung"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "28271f5e25cabca2905d4c9113f0fef3f16751c8",
      "externalIds": {
        "MAG": "1502364872",
        "DOI": "10.1007/978-1-4757-4321-0",
        "CorpusId": 207929286
      },
      "title": "The Cross-Entropy Method",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-1-4757-4321-0?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-1-4757-4321-0, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "35064932",
          "name": "R. Rubinstein"
        },
        {
          "authorId": "1704242",
          "name": "Dirk P. Kroese"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "413b32dc8855f9db4c95657a3de5ca6c1d793da0",
      "externalIds": {
        "DBLP": "conf/icra/KohlS04",
        "MAG": "2139053308",
        "DOI": "10.1109/ROBOT.2004.1307456",
        "CorpusId": 7013049
      },
      "title": "Policy gradient reinforcement learning for fast quadrupedal locomotion",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.2004.1307456?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.2004.1307456, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2686687",
          "name": "Nate Kohl"
        },
        {
          "authorId": "144848112",
          "name": "P. Stone"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2b51d022d33c1d48a8ebddf38e68117740ba6dcd",
      "externalIds": {
        "MAG": "2077544344",
        "DOI": "10.1097/01.sla.0000103020.19595.7d",
        "CorpusId": 25675685,
        "PubMed": "14685095"
      },
      "title": "Robotic Surgery: A Current Perspective",
      "openAccessPdf": {
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1356187",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1097/01.sla.0000103020.19595.7d?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1097/01.sla.0000103020.19595.7d, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "5921364",
          "name": "A. Lanfranco"
        },
        {
          "authorId": "49783788",
          "name": "A. Castellanos"
        },
        {
          "authorId": "1772990",
          "name": "J. Desai"
        },
        {
          "authorId": "2748340",
          "name": "W. Meyers"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "72d8fe43c1ebeceb7dbb1dd6faf10c0234a9b6bf",
      "externalIds": {
        "MAG": "2123663688",
        "DBLP": "conf/iros/BakkerZGS03",
        "DOI": "10.1109/IROS.2003.1250667",
        "CorpusId": 18451256
      },
      "title": "A robot that reinforcement-learns to identify and memorize important previous observations",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/IROS.2003.1250667?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/IROS.2003.1250667, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3011589",
          "name": "B. Bakker"
        },
        {
          "authorId": "1830480",
          "name": "Viktor Zhumatiy"
        },
        {
          "authorId": "29773814",
          "name": "G. Gruener"
        },
        {
          "authorId": "145341374",
          "name": "J. Schmidhuber"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5d0184c044e13feea0d6539f4a6b8c31e49e0e90",
      "externalIds": {
        "MAG": "1564755532",
        "DOI": "10.1184/R1/6552458.V1",
        "CorpusId": 2103968
      },
      "title": "Covariant policy search",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1184/R1/6552458.V1?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1184/R1/6552458.V1, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "9168729",
          "name": "W. Liu"
        },
        {
          "authorId": "2116585623",
          "name": "Sanjiang Li"
        },
        {
          "authorId": "1680174",
          "name": "Jochen Renz"
        }
      ],
      "abstract": "We investigate the problem of non-covariant behavior of policy gradient reinforcement learning algorithms. The policy gradient approach is amenable to analysis by information geometric methods. This leads us to propose a natural metric on controller parameterization that results from considering the manifold of probability distributions over paths induced by a stochastic controller. Investigation of this approach leads to a covariant gradient ascent rule. Interesting properties of this rule are discussed, including its relation with actor-critic style reinforcement learning algorithms. The algorithms discussed here are computationally quite efficient and on some interesting problems lead to dramatic performance improvement over noncovariant rules."
    },
    {
      "paperId": "5562a56da3a96dae82add7de705e2bd841eb00fc",
      "externalIds": {
        "DBLP": "conf/icdar/SimardSP03",
        "MAG": "2156163116",
        "DOI": "10.1109/ICDAR.2003.1227801",
        "CorpusId": 4659176
      },
      "title": "Best practices for convolutional neural networks applied to visual document analysis",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDAR.2003.1227801?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDAR.2003.1227801, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2812486",
          "name": "Patrice Y. Simard"
        },
        {
          "authorId": "38767254",
          "name": "David Steinkraus"
        },
        {
          "authorId": "144189092",
          "name": "John C. Platt"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "647ff4a5bbfcae590db99be32fc635729a793ffc",
      "externalIds": {
        "MAG": "1517236425",
        "DOI": "10.1201/9781003062714",
        "CorpusId": 109101815
      },
      "title": "Neural Network Control Of Robot Manipulators And Non-Linear Systems",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1201/9781003062714?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1201/9781003062714, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "144441675",
          "name": "F. Lewis"
        },
        {
          "authorId": "97785304",
          "name": "A. Yesildirak"
        },
        {
          "authorId": "144576672",
          "name": "S. Jagannathan"
        }
      ],
      "abstract": "From the Publisher: \nThis graduate text provides an authoritative account of neural network (NN) controllers for robotics and nonlinear systems and gives the first textbook treatment of a general and streamlined design procedure for NN controllers. Stability proofs and performance guarantees are provided which illustrate the superior efficiency of the NN controllers over other design techniques when the system is unknown. New NN properties, such as robustness and passivity are introduced, and new weight tuning algorithms are presented. Neural Network Control of Robot Manipulators and Nonlinear Systems provides a welcome introduction to graduate students, and an invaluable reference to professional engineers and researchers in control systems."
    },
    {
      "paperId": "fe869cb220083fb36d1c953eb712989d9c178ba7",
      "externalIds": {
        "MAG": "2124904960",
        "DBLP": "conf/icra/JagersandFN97",
        "DOI": "10.1109/ROBOT.1997.606723",
        "CorpusId": 5072029
      },
      "title": "Experimental evaluation of uncalibrated visual servoing for precision manipulation",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.1997.606723?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.1997.606723, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3160299",
          "name": "Martin Jägersand"
        },
        {
          "authorId": "152401643",
          "name": "O. Fuentes"
        },
        {
          "authorId": "31614700",
          "name": "R. Nelson"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7eccec6d021c0f32b20020a4f1742161b14007b2",
      "externalIds": {
        "MAG": "2136492961",
        "DBLP": "journals/trob/WilsonHB96",
        "DOI": "10.1109/70.538974",
        "CorpusId": 23706304
      },
      "title": "Relative end-effector control using Cartesian position based visual servoing",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/70.538974?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/70.538974, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "145124148",
          "name": "W. Wilson"
        },
        {
          "authorId": "2786353",
          "name": "Carol C. W. Hulls"
        },
        {
          "authorId": "2067439133",
          "name": "G. Bell"
        }
      ],
      "abstract": "This paper presents a complete design methodology for Cartesian position based visual servo control for robots with a single camera mounted at the end-effector. Position based visual servo control requires the explicit calculation of the relative position and orientation (POSE) of the workpiece object with respect to the camera. This is accomplished using image plane measurements of a number of known feature points on the object, and then applying an extended Kalman filter to obtain a recursive solution of the photogrammetric equations, and to properly combine redundant measurements. The control is then designed by specifying the desired trajectories with respect to the object and forming the control error in the end-effector frame. The implementation using a distributed computer architecture is described. An experimental system has been built and used to evaluate the performance of the POSE estimation and the position based visual servo control. Several results for relative trajectory control and target tracking are presented. Results of the experiments showing the effect of loss of some of the redundant features are also presented."
    },
    {
      "paperId": "2f7e990b73d871b7dc046e19ebfa321dd3eef779",
      "externalIds": {
        "DBLP": "journals/ras/Gullapalli95",
        "MAG": "2028941029",
        "DOI": "10.1016/0921-8890(95)00006-2",
        "CorpusId": 12010001
      },
      "title": "Skillful control under uncertainty via direct reinforcement learning",
      "openAccessPdf": {
        "url": "https://doi.org/10.1016/0921-8890(95)00006-2",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/0921-8890(95)00006-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/0921-8890(95)00006-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2059884",
          "name": "V. Gullapalli"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c32bbf351bfc06ae8b70a31216f90003d9af1a70",
      "externalIds": {
        "DBLP": "conf/icra/YoshimiA94",
        "MAG": "2152164805",
        "DOI": "10.1109/ROBOT.1994.350995",
        "CorpusId": 11156749
      },
      "title": "Active, uncalibrated visual servoing",
      "openAccessPdf": {
        "url": "https://academiccommons.columbia.edu/doi/10.7916/D89W0QRB/download",
        "status": "GREEN",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ROBOT.1994.350995?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ROBOT.1994.350995, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1877763",
          "name": "B. Yoshimi"
        },
        {
          "authorId": "2193888",
          "name": "P. Allen"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "210225cacf1460daba8fa8c604b44f27a825da9c",
      "externalIds": {
        "MAG": "2390463742",
        "DBLP": "journals/automatica/HuntSZG92",
        "DOI": "10.1016/0005-1098(92)90053-I",
        "CorpusId": 207511431
      },
      "title": "Neural networks for control systems - A survey",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/0005-1098(92)90053-I?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/0005-1098(92)90053-I, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2111701647",
          "name": "K. Hunt"
        },
        {
          "authorId": "1390057008",
          "name": "D. Sbarbaro-Hofer"
        },
        {
          "authorId": "145455580",
          "name": "R. Zbikowski"
        },
        {
          "authorId": "1691847",
          "name": "P. Gawthrop"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "86778c6ae42381fd6620f1ba8c870e3b887285fb",
      "externalIds": {
        "DBLP": "conf/grpa/EspiauCR91",
        "MAG": "2161505441",
        "DOI": "10.1007/3-540-57132-9_8",
        "CorpusId": 6773377
      },
      "title": "A New Approach to Visual Servoing in Robotics",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/3-540-57132-9_8?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/3-540-57132-9_8, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "1792024",
          "name": "B. Espiau"
        },
        {
          "authorId": "1755969",
          "name": "F. Chaumette"
        },
        {
          "authorId": "144479211",
          "name": "P. Rives"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "69e68bfaadf2dccff800158749f5a50fe82d173b",
      "externalIds": {
        "MAG": "2101926813",
        "DOI": "10.1007/BF00344251",
        "CorpusId": 206775608,
        "PubMed": "7370364"
      },
      "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/BF00344251?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/BF00344251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "3160228",
          "name": "K. Fukushima"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "fc4420935ce04cbcc43ad5af5bde6f0e5f236529",
      "externalIds": {
        "MAG": "658381347",
        "DOI": "10.2307/3613752",
        "CorpusId": 59266418
      },
      "title": "Differential dynamic programming",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.2307/3613752?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2307/3613752, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "143756356",
          "name": "H. Rosenbrock"
        },
        {
          "authorId": "32133325",
          "name": "D. Jacobson"
        },
        {
          "authorId": "2317007",
          "name": "D. Mayne"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "633eaaaeca98f6796bde40e4cf5c5d5865e5b68b",
      "externalIds": {
        "MAG": "2341171179",
        "DOI": "10.1126/science.153.3731.34",
        "CorpusId": 271544899,
        "PubMed": "17730601"
      },
      "title": "Dynamic Programming",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1126/science.153.3731.34?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1126/science.153.3731.34, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2270322824",
          "name": "Richard Bellman"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb",
      "externalIds": {
        "CorpusId": 3074096
      },
      "title": "Deep Learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1688882",
          "name": "Yann LeCun"
        },
        {
          "authorId": "1751762",
          "name": "Yoshua Bengio"
        },
        {
          "authorId": "1695689",
          "name": "Geoffrey E. Hinton"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
      "externalIds": {
        "CorpusId": 18420840
      },
      "title": "ALVINN, an autonomous land vehicle in a neural network",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "48855558",
          "name": "D. Pomerleau"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "fc2ffb9daf9f0dd07e755d7ad5633907efb0a4b7",
      "externalIds": {
        "MAG": "2135362819",
        "DBLP": "journals/jmlr/BohmerGSMO13",
        "DOI": "10.5555/2567709.2567728",
        "CorpusId": 9438703
      },
      "title": "Construction of approximation spaces for reinforcement learning",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5555/2567709.2567728?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/2567709.2567728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "144285271",
          "name": "Wendelin Böhmer"
        },
        {
          "authorId": "1688482",
          "name": "S. Grünewälder"
        },
        {
          "authorId": "2117688020",
          "name": "Yun Shen"
        },
        {
          "authorId": "1748174",
          "name": "M. Musial"
        },
        {
          "authorId": "1743272",
          "name": "K. Obermayer"
        }
      ],
      "abstract": "Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modification to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modified SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance."
    },
    {
      "paperId": "48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b",
      "externalIds": {
        "MAG": "195033972",
        "DBLP": "conf/icinco/LiT04",
        "DOI": "10.5220/0001143902220229",
        "CorpusId": 19300
      },
      "title": "Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems",
      "openAccessPdf": {
        "url": "https://doi.org/10.5220/0001143902220229",
        "status": "HYBRID",
        "license": "CCBYNCND",
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.5220/0001143902220229?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5220/0001143902220229, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2108649594",
          "name": "Weiwei Li"
        },
        {
          "authorId": "144832491",
          "name": "E. Todorov"
        }
      ],
      "abstract": "This paper presents an Iterative Linear Quadratic Regulator (ILQR) method for locally-optimal feedback control of nonlinear dynamical systems. The method is applied to a musculo-skeletal arm model with 10 state dimensions and 6 controls, and is used to compute energy-optimal reaching movements. Numerical comparisons with three existing methods demonstrate that the new method converges substantially faster and finds slightly better solutions."
    },
    {
      "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
      "externalIds": {
        "DOI": "10.1023/A:1022672621406",
        "CorpusId": 2332513
      },
      "title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
      "openAccessPdf": {
        "url": "https://link.springer.com/content/pdf/10.1023/A:1022672621406.pdf",
        "status": "BRONZE",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1023/A:1022672621406?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1023/A:1022672621406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2116648700",
          "name": "Ronald J. Williams"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5e405b71a021238cb612bfc65b77baa09a245073",
      "externalIds": {
        "MAG": "1949804828",
        "DBLP": "conf/iser/NgCDGSTBL04",
        "DOI": "10.1007/11552246_35",
        "CorpusId": 263901753
      },
      "title": "Autonomous Inverted Helicopter Flight via Reinforcement Learning",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/11552246_35?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/11552246_35, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2285073523",
          "name": "Andrew Y. Ng"
        },
        {
          "authorId": "144638694",
          "name": "Adam Coates"
        },
        {
          "authorId": "1935095",
          "name": "Mark Diel"
        },
        {
          "authorId": "1981419",
          "name": "Varun Ganapathi"
        },
        {
          "authorId": "49084213",
          "name": "Jamie Schulte"
        },
        {
          "authorId": "2364039203",
          "name": "Ben Tse"
        },
        {
          "authorId": "2364040998",
          "name": "Eric Berger"
        },
        {
          "authorId": "2364040805",
          "name": "Eric Liang"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "2e5f2b57f4c476dd69dc22ccdf547e48f40a994c",
      "externalIds": {
        "MAG": "1525783482",
        "CorpusId": 17278462
      },
      "title": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "3308557",
          "name": "Sepp Hochreiter"
        },
        {
          "authorId": "1751762",
          "name": "Yoshua Bengio"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "24919c3d4de529d9ec4df381554d514c7b087760",
      "externalIds": {
        "DBLP": "journals/ras/BenbrahimF97",
        "MAG": "1994923984",
        "DOI": "10.1016/S0921-8890(97)00043-2",
        "CorpusId": 206093928
      },
      "title": "Biped dynamic walking using reinforcement learning",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/S0921-8890(97)00043-2?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/S0921-8890(97)00043-2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "30423929",
          "name": "H. Benbrahim"
        },
        {
          "authorId": "3085900",
          "name": "J. Franklin"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "7cba0547426a675ec06cfa0a3a82a8d4c8348a0a",
      "externalIds": {
        "MAG": "1599900761",
        "DOI": "10.1007/978-1-4615-3180-7",
        "CorpusId": 60599234
      },
      "title": "Neural Networks in Robotics",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-1-4615-3180-7?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1007/978-1-4615-3180-7, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "6778024",
          "name": "G. Bekey"
        },
        {
          "authorId": "144344283",
          "name": "Ken Goldberg"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "5e9dc8d71572719cec58ec815bbd331fbd07fa15",
      "externalIds": {
        "MAG": "2080759927",
        "DBLP": "journals/nn/Gullapalli90",
        "DOI": "10.1016/0893-6080(90)90056-Q",
        "CorpusId": 21486916
      },
      "title": "A stochastic reinforcement learning algorithm for learning real-valued functions",
      "openAccessPdf": {
        "url": "",
        "status": "CLOSED",
        "license": null,
        "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1016/0893-6080(90)90056-Q?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/0893-6080(90)90056-Q, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
      },
      "authors": [
        {
          "authorId": "2059884",
          "name": "V. Gullapalli"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
      "externalIds": {
        "DBLP": "conf/nips/CunBDHHHJ89",
        "MAG": "2154579312",
        "CorpusId": 2542741
      },
      "title": "Handwritten Digit Recognition with a Back-Propagation Network",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [
        {
          "authorId": "1688882",
          "name": "Yann LeCun"
        },
        {
          "authorId": "2219581",
          "name": "B. Boser"
        },
        {
          "authorId": "1747317",
          "name": "J. Denker"
        },
        {
          "authorId": "37274089",
          "name": "D. Henderson"
        },
        {
          "authorId": "2799635",
          "name": "R. Howard"
        },
        {
          "authorId": "34859193",
          "name": "W. Hubbard"
        },
        {
          "authorId": "2041866",
          "name": "L. Jackel"
        }
      ],
      "abstract": null
    },
    {
      "paperId": "c5a086109367fcaef613fa5573bd4035171f0e8e",
      "externalIds": {
        "CorpusId": 6531733
      },
      "title": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence Reinforcement Learning to Adjust Robot Movements to New Situations",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [],
      "abstract": null
    },
    {
      "paperId": "5a47ba057a858f8c024d2518cc3731fc7eb40de1",
      "externalIds": {
        "CorpusId": 904144
      },
      "title": "Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence Flexible, High Performance Convolutional Neural Networks for Image Classification",
      "openAccessPdf": {
        "url": "",
        "status": null,
        "license": null
      },
      "authors": [],
      "abstract": null
    }
  ]
}