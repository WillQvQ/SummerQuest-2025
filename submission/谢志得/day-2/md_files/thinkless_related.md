## Thinkless: LLM Learns When to Think 相关

### Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task
- ArXiv链接 : https://arxiv.org/abs/2506.11986
- 关键特点 : 针对Text-to-SQL中的schema linking问题，提出了基于强化学习的Schema-R1方法，通过高质量推理样本和规则驱动的RL训练提升模型推理能力。
- 相关技术 : Schema Linking, Reinforcement Learning

### Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models
- ArXiv链接 : https://arxiv.org/abs/2506.04210
- 关键特点 : 系统分析推理模型在测试时“多思考”是否真的有益，提出了parallel thinking等新方法，发现过度推理会导致性能下降。
- 相关技术 : Test-Time Scaling, Parallel Thinking, Overthinking

### VeriThinker: Learning to Verify Makes Reasoning Model Efficient
- ArXiv链接 : https://arxiv.org/abs/2505.17941
- 关键特点 : 通过辅助验证任务微调大模型，有效抑制过度思考，显著减少推理token数量并提升准确率。
- 相关技术 : Verification Task, CoT Compression

### Efficient Reasoning Models: A Survey
- ArXiv链接 : https://arxiv.org/abs/2504.10903
- 关键特点 : 综述高效推理模型的最新进展，涵盖推理链压缩、模型压缩和高效解码等方向。
- 相关技术 : CoT Compression, Model Compression, Efficient Decoding

### Understanding R1-Zero-Like Training: A Critical Perspective
- ArXiv链接 : https://arxiv.org/abs/2503.20783
- 关键特点 : 分析R1-Zero训练的核心组件，提出Dr. GRPO方法提升token效率，推动推理模型训练优化。
- 相关技术 : GRPO, Dr. GRPO, R1-Zero Training

### ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning
- ArXiv链接 : https://arxiv.org/abs/2503.19470
- 关键特点 : 通过强化学习训练LLM结合外部搜索进行推理，展现了反思和自我纠正能力。
- 相关技术 : Reinforcement Learning, Search Integration

### SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild
- ArXiv链接 : https://arxiv.org/abs/2503.18892
- 关键特点 : 研究零强化学习训练在不同基础模型上的表现，提出多种设计策略提升推理准确率和响应长度。
- 相关技术 : Zero RL Training, Rule-based Rewards

### Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching
- ArXiv链接 : https://arxiv.org/abs/2503.05179
- 关键特点 : 提出认知启发的推理范式，通过概念链、分块符号化和专家词典减少token使用。
- 相关技术 : Cognitive-Inspired Prompting, Conceptual Chaining

### L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning
- ArXiv链接 : https://arxiv.org/abs/2503.04697
- 关键特点 : 提出LCPO方法，基于PPO变体控制推理长度，实现推理效率与准确率的平衡。
- 相关技术 : LCPO (基于PPO), Length Control

### Chain of Draft: Thinking Faster by Writing Less
- ArXiv链接 : https://arxiv.org/abs/2502.18600
- 关键特点 : 受人类认知过程启发，生成简洁的中间推理输出，仅用7.6%的token即可匹配CoT性能。
- 相关技术 : Minimalistic Reasoning, Draft Generation
