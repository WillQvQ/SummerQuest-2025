# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
1024×1024  

#### 1.1.2
8×128×1024

#### 1.1.3
8×128×1024；把每个rank本地输出的Yconcat起来得到完整的Y  

### 1.2


#### 1.2.1
1024×1024

#### 1.2.2
8×128×1024

#### 1.2.3
8×128×1024；把所有rank的计算结果按元素相加得到完整的Z

## 2 通信分析

### 2.1

#### 2.1.1
需要通信；all-gather;总通信量：8×128×1024×4×12

#### 2.1.2
需要通信，因为反向传播时每个rank都只计算了自己那部分的梯度贡献，也就是partialL/partialYi,而整个输入张量的梯度partialL/partialX = sigma(partialL / partialYi)×W1i.T,所以需要all-reduce

### 2.2

#### 2.2.1
需要通信；all-reduce;总通信量：8×128×1024×4×4

#### 2.2.2
不需要。每个rank负责不同部分的梯度，不需要拼接

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
原来只有第一层需要all-gather,现在两层前向传播都需要allgather，通信开销增大;  
通信数据量大，实现起来复杂
