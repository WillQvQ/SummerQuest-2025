# conda environments:
#
base                     /root/miniconda
jupyterlab               /root/miniconda/envs/jupyterlab
ljx                   *  /root/miniconda/envs/ljx

/root/miniconda/bin/python
=== 环境信息检查 ===

INFO 07-07 21:34:27 [__init__.py:244] Automatically detected platform cuda.
vLLM 版本: 0.9.1

=== vLLM 引擎初始化 ===
正在初始化 vLLM 引擎...
注意: vLLM 初始化可能需要几分钟时间
正在加载分词器...
vLLM 引擎和分词器初始化完成！

INFO 07-07 21:34:42 [config.py:823] This model supports multiple tasks: {'classify', 'score', 'embed', 'reward', 'generate'}. Defaulting to 'generate'.
INFO 07-07 21:34:42 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 07-07 21:34:42 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 07-07 21:34:44 [core.py:455] Waiting for init message from front-end.
INFO 07-07 21:34:44 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/data-mnt/data-mnt/data-mnt/data-mnt/data/downloaded_ckpts/Qwen2.5-1.5B', speculative_config=None, tokenizer='/data-mnt/data-mnt/data-mnt/data-mnt/data/downloaded_ckpts/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/data-mnt/data-mnt/data-mnt/data-mnt/data/downloaded_ckpts/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}
WARNING 07-07 21:34:45 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7feee7b93e60>
INFO 07-07 21:34:45 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 07-07 21:34:45 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
INFO 07-07 21:34:45 [gpu_model_runner.py:1595] Starting to load model /data-mnt/data-mnt/data-mnt/data-mnt/data/downloaded_ckpts/Qwen2.5-1.5B...
INFO 07-07 21:34:45 [gpu_model_runner.py:1600] Loading model from scratch...
INFO 07-07 21:34:46 [cuda.py:252] Using Flash Attention backend on V1 engine.
INFO 07-07 21:35:02 [default_loader.py:272] Loading weights took 16.50 seconds
INFO 07-07 21:35:03 [gpu_model_runner.py:1624] Model loading took 2.9105 GiB and 16.722097 seconds
INFO 07-07 21:35:04 [gpu_worker.py:227] Available KV cache memory: 14.59 GiB
INFO 07-07 21:35:04 [kv_cache_utils.py:715] GPU KV cache size: 546,512 tokens
INFO 07-07 21:35:04 [kv_cache_utils.py:719] Maximum concurrency for 4,096 tokens per request: 133.43x
INFO 07-07 21:35:04 [core.py:171] init engine (profile, create kv cache, warmup model) took 1.73 seconds
=== 测试1: 普通对话模式 (vLLM) ===
输入文本: <|im_start|>system
你是一个会深思熟虑的AI助手。<|im_end|>
<|im_start|>user
你好，我是邱锡鹏老师的学生。<|im_end|>
<|im_start|>assistant


开始生成回复 (vLLM)...
AI回复（普通模式 - vLLM）:
============================================================
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，我是邱锡鹏老师的学生。 Geile
 Geileassistant
你好，
============================================================
推理耗时: 21.26 秒

==================================================

=== 测试2: 思维链推理模式 (vLLM) ===
输入文本: <|im_start|>system
你是一个会深思熟虑的AI助手。<|im_end|>
<|im_start|>user
你好，我是邱锡鹏老师的学生。<|im_end|>
<|im_start|>assistant


开始生成回复（思维链模式 - vLLM）...
AI回复（思维链模式 - vLLM）:
============================================================
你好，很高兴见到你。虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒虒
============================================================
推理耗时: 19.25 秒

=== 测试完成 ===