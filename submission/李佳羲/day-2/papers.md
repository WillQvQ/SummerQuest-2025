## 面向VQA的多模态推理

该方向聚焦于提升视觉-语言模型（VLM）在视觉问答（VQA）任务中的深度推理与可解释性。核心挑战在于，模型需要洞察并推理出图像内容与问题之间的隐性因果关系。研究重点包括：设计更具挑战性的因果推理型VQA任务、在模型对于开放性问题的回答中评估多步推理链的正确性、探索声音等额外模态如何辅助视觉因果解释，以及利用强化学习等技术，提升模型因果推理的鲁棒性与准确性。

### 代表论文

#### 1. CausalVQA: A Physically Grounded Causal
- [ArXiv链接](https://arxiv.org/abs/2506.09943)
- 贡献：  
1. 提出了一个基于物理因果推理的视频问答基准，旨在评估和推动视频模型对因果关系的理解能力。该工作强调模型不仅需要识别表面视觉信息，更需理解物理事件间的因果联系，从而提升视频理解和推理的深度与准确性。  
2. 通过设计具有明确因果结构的视频问答任务，推动多模态模型从关联学习转向因果推理，促进模型具备更强的泛化能力和解释能力。

#### 2. Multi-Modal Answer Validation for Knowledge-Based VQA
- [ArXiv链接](https://arxiv.org/abs/2103.12248)  
- 贡献：  
1. 提出了“答案验证”这一新范式，通过利用候选答案指导知识检索，显著提高了知识检索的相关性和验证准确性。  
2. 将外部视觉信息引入知识型VQA，为传统仅使用文本知识的方法提供了重要补充。  
3. 多粒度知识嵌入与注意力聚合机制有效捕捉问题、答案及其相关知识之间的细粒度关系，从而提升验证精度。  

#### 3. From Representation to Reasoning: Towards both Evidence and Commonsense Reasoning for Video Question-Answering
- [ArXiv链接](https://arxiv.org/abs/2205.14895)  
- 贡献：
1. 提出一个模块化的 ECR 框架，分离处理证据定位与常识推理，提升视频问答的因果与多步推理能力。
2. 构建增强版 STAR++ 数据集，提供细粒度推理标注与证据路径，便于模型训练和可解释性评估。
3. 实验证明解耦推理模块显著提高推理准确性，尤其在跨时间、因果型问题上效果突出。

#### 4. NExT-QA:Next Phase of Question-Answering to Explaining Temporal Actions
- [ArXiv链接]https://arxiv.org/abs/2105.08276()  
- 贡献：  
1. 引入了 NExT-QA 数据集，设计了强调时序、因果和逆因果推理的视频问答任务，超越了传统描述性VQA的范式。
2. 提出了一种视频–文本联合推理模型 (STAGE)，结合动作识别、语言理解与时序推理能力，提升复杂推理任务的表现。
3. 数据集和任务共同推动多模态模型从“看见并回答”走向“理解并解释”，促进具备人类式因果推理能力的 AI 发展。
