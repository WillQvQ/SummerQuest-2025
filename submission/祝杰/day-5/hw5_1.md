# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1
(1024 x 1024)

#### 1.1.2
(8 x 128 x 1024)

#### 1.1.3
(8 x 128 x 1024);由于没有使用 column parallel 时的本地输出是 (8 x 128 x 4096),因此需要将每个 Yi 按照最后一个维度拼接成 Y

### 1.2


#### 1.2.1
(1024 x 1024)

#### 1.2.2
(8 x 128 x 1024)

#### 1.2.3
(8 x 128 x 1024);每个 Zi 相加得到完整的 Z

## 2 通信分析

### 2.1

#### 2.1.1
需要使用; all-gather 通信把 Yi 收集并拼接; 3 * 8 * 128 * 1024

#### 2.1.2
需要使用;
因为完整的 L 对 X 的偏导(B,S,D) 是所有rank的贡献之和。
但是每个 rank 只能拿到自己的 W1i 和 L 对 Yi 的偏导(也就是只能算出一部分的反向梯度),因此需要合成完整梯度需要将每个rank的反向梯度使用all-reduce操作相加。

### 2.2

#### 2.2.1
需要通信;all-reduce;3 * 8 * 128 * 1024

#### 2.2.2
不需要通信;因为 linear2 只是按照输入进行切分,因此每个 rank 只需要算出 Yi 输入的梯度即可。


# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？
在 linear1 和 linear2 之间,由于 linear1 的输出Y(8 x 128 x 4096)是Yi相加操作得到的,但是 linear2 需要的输入是 (8 x 128 x 1024)，因此还需要 scatter 操作把完整的 Y 切出来分配给 linear2 的各个 rank;

如果都使用column parallel时,那么 linear2 的输入是需要完整的,也需要额外的操作———需要先把 linear1 的输出进行 all-gather 再输入到 linear2