# hw5_1 吴奇峰
## 1 并行策略与张量 shape

### 1.1 Linear1 使用 Column Parallel：
#### 1.1.1 

每个 rank 上的权重矩阵 $W_1^{(i)} $ shape 是$[1024,1024]$

#### 1.1.2 

每个 rank 输入的张量 $X$ 的 shape 是 $[8,128,1024]$

#### 1.1.3 

每个rank本地输出的 $Y_i$ 的 shape 是 $[8, 128, 1024]$ ，最后通过 $AllGather$ 进行拼接得到完整的 $Y$

### 1.2 Linear2 使用 Row Parallel：
#### 1.2.1 
每个 rank 上的权重矩阵 $W_2^{(i)} $ shape 是 $[1024,1024]$

#### 1.2.2 
每个 rank 接受输入的张量的 shape 是 $[8,128,1024]$

#### 1.2.3 
每个 rank 本地输出 $Z_i$ 的 shape 是：$[8, 128, 1024]$，最后通过 $AllReduce$ 操作，对所有 rank 的输出求和得到完整的 $Z$

## 2 通信分析
### 2.1 对于 Linear1：
#### 2.1.1 

Linear1的前向传播不需要通信，因为每个 rank 计算所需的输入都是原始的 $X$，而输出时，可以直接把每个 rank 的输出，各自用激活函数处理，直接交给 Linear2对应的 rank


#### 2.1.2 

反向传播时，计算 $\partial L / \partial X $ 需要通信，因为前向传播的输出是由复制得到的多份 $X$ 与参数相乘后拼接得到的，反向传播时，需要用 $AllReduce$ ，对各个 rank 上对输入的梯度求和


### 2.2 对于 Linear2：
#### 2.2.1 
前向传播时需要通信

每个 rank 本地计算 $Z_i\in\mathbb{R}^{8\times128\times1024}$，再通过 $All‑Reduce$ 求和，通信量为 $2 \times 8 \times 128 \times 1024 \times 3$，即 24MB

#### 2.2.2

反向传播时不需要通信，因为可以直接在各个rank上计算，反向传回 Linear1 的相应 rank

## 3

如果两层都使用 Row Parallel，Linear1的输出必须进行一次AllReduce通信，而从Linear2必须重新进行一次Scatter。总通信量增加

如果都使用 Column Parallel，则 Linear2 不能直接利用 Linear1 计算所得的切片，Linear1 必须额外进行 AllGather, Linear2需要重新进行 Scatter