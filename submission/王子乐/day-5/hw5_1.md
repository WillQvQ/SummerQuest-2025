# 答题卡

## 1 并行策略与张量 shape

### 1.1

#### 1.1.1

$(d, \frac {4d} P) = (1024, 1024)$

#### 1.1.2

$(B, S, d) = (8, 128, 1024)$

#### 1.1.3

$(B, S, \frac {4d} P) = (8, 128, 1024)$  做一次 All-gather

### 1.2

#### 1.2.1

$(\frac {4d} P, d) = (1024, 1024)$

#### 1.2.2

$(B, S, \frac {4d} P) = (8, 128, 1024)$

#### 1.2.3

$(B, S, d) = (8, 128, 1024)$ 做一次All-reduce

## 2 通信分析

### 2.1

#### 2.1.1

前向传播需要一次Boardcast，通信量为 $(P - 1) \times B \times S \times d = 3,145,728$

#### 2.1.2

需要，$\displaystyle \frac {\partial L} {\partial X} = \sum \frac {\partial L} {\partial Y_i} (W_1^i)^T$要做一次All-reduce

### 2.2

#### 2.2.1

前向传播需要一次All-reduce，通信量为$2(P - 1) \times B \times S \times d = 6,291,456$

#### 2.2.2

需要，$\displaystyle \frac {\partial L} {\partial Y} = \mathbb{concat}[\frac {\partial L } {\partial Z} \times (W_2^i)^T]$，要做一次All-gather

# 3 如果两层都使用 Row Parallel，会产生哪些额外通信？两层都使用 Column Parallel 会带来什么问题？

1.在进入GeLU前要做一次All-reduce，在进入Linear2之前要做一次Scatter，反向传播也要添加对应的操作

2.在进入GeLU层之后要额外做一次Prodcast，而且两个Gpu上都要存一个完整的矩阵，显存占用变大了，反向传播也要添加对应的操作